{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ffd758b6-9ea2-4b5e-be76-95a25bdd8150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:35:10.199262Z",
     "iopub.status.busy": "2023-02-25T11:35:10.198256Z",
     "iopub.status.idle": "2023-02-25T11:35:11.559949Z",
     "shell.execute_reply": "2023-02-25T11:35:11.558900Z",
     "shell.execute_reply.started": "2023-02-25T11:35:10.199230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_classifier_v2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 200, 16)]    0           []                               \n",
      "                                                                                                  \n",
      " split_layer_234 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_235 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_236 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_237 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_238 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_239 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_240 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_241 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_242 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_243 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_244 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_245 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_246 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_247 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_248 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_249 (SplitLayer)   (None, 200)          0           ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_234 (Reshape)          (None, 200, 1)       0           ['split_layer_234[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_235 (Reshape)          (None, 200, 1)       0           ['split_layer_235[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_236 (Reshape)          (None, 200, 1)       0           ['split_layer_236[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_237 (Reshape)          (None, 200, 1)       0           ['split_layer_237[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_238 (Reshape)          (None, 200, 1)       0           ['split_layer_238[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_239 (Reshape)          (None, 200, 1)       0           ['split_layer_239[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_240 (Reshape)          (None, 200, 1)       0           ['split_layer_240[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_241 (Reshape)          (None, 200, 1)       0           ['split_layer_241[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_242 (Reshape)          (None, 200, 1)       0           ['split_layer_242[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_243 (Reshape)          (None, 200, 1)       0           ['split_layer_243[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_244 (Reshape)          (None, 200, 1)       0           ['split_layer_244[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_245 (Reshape)          (None, 200, 1)       0           ['split_layer_245[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_246 (Reshape)          (None, 200, 1)       0           ['split_layer_246[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_247 (Reshape)          (None, 200, 1)       0           ['split_layer_247[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_248 (Reshape)          (None, 200, 1)       0           ['split_layer_248[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_249 (Reshape)          (None, 200, 1)       0           ['split_layer_249[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_634 (Conv1D)            (None, 100, 100)     400         ['reshape_234[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_637 (Conv1D)            (None, 100, 100)     400         ['reshape_235[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_640 (Conv1D)            (None, 100, 100)     400         ['reshape_236[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_643 (Conv1D)            (None, 100, 100)     400         ['reshape_237[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_646 (Conv1D)            (None, 100, 100)     400         ['reshape_238[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_649 (Conv1D)            (None, 100, 100)     400         ['reshape_239[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_652 (Conv1D)            (None, 100, 100)     400         ['reshape_240[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_655 (Conv1D)            (None, 100, 100)     400         ['reshape_241[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_658 (Conv1D)            (None, 100, 100)     400         ['reshape_242[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_661 (Conv1D)            (None, 100, 100)     400         ['reshape_243[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_664 (Conv1D)            (None, 100, 100)     400         ['reshape_244[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_667 (Conv1D)            (None, 100, 100)     400         ['reshape_245[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_670 (Conv1D)            (None, 100, 100)     400         ['reshape_246[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_673 (Conv1D)            (None, 100, 100)     400         ['reshape_247[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_676 (Conv1D)            (None, 100, 100)     400         ['reshape_248[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_679 (Conv1D)            (None, 100, 100)     400         ['reshape_249[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_634 (MaxPooling1  (None, 50, 100)     0           ['conv1d_634[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_637 (MaxPooling1  (None, 50, 100)     0           ['conv1d_637[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_640 (MaxPooling1  (None, 50, 100)     0           ['conv1d_640[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_643 (MaxPooling1  (None, 50, 100)     0           ['conv1d_643[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_646 (MaxPooling1  (None, 50, 100)     0           ['conv1d_646[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_649 (MaxPooling1  (None, 50, 100)     0           ['conv1d_649[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_652 (MaxPooling1  (None, 50, 100)     0           ['conv1d_652[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_655 (MaxPooling1  (None, 50, 100)     0           ['conv1d_655[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_658 (MaxPooling1  (None, 50, 100)     0           ['conv1d_658[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_661 (MaxPooling1  (None, 50, 100)     0           ['conv1d_661[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_664 (MaxPooling1  (None, 50, 100)     0           ['conv1d_664[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_667 (MaxPooling1  (None, 50, 100)     0           ['conv1d_667[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_670 (MaxPooling1  (None, 50, 100)     0           ['conv1d_670[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_673 (MaxPooling1  (None, 50, 100)     0           ['conv1d_673[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_676 (MaxPooling1  (None, 50, 100)     0           ['conv1d_676[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_679 (MaxPooling1  (None, 50, 100)     0           ['conv1d_679[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_635 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_634[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_638 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_637[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_641 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_640[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_644 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_643[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_647 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_646[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_650 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_649[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_653 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_652[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_656 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_655[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_659 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_658[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_662 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_661[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_665 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_664[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_668 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_667[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_671 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_670[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_674 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_673[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_677 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_676[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_680 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_679[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling1d_635 (MaxPooling1  (None, 25, 100)     0           ['conv1d_635[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_638 (MaxPooling1  (None, 25, 100)     0           ['conv1d_638[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_641 (MaxPooling1  (None, 25, 100)     0           ['conv1d_641[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_644 (MaxPooling1  (None, 25, 100)     0           ['conv1d_644[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_647 (MaxPooling1  (None, 25, 100)     0           ['conv1d_647[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_650 (MaxPooling1  (None, 25, 100)     0           ['conv1d_650[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_653 (MaxPooling1  (None, 25, 100)     0           ['conv1d_653[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_656 (MaxPooling1  (None, 25, 100)     0           ['conv1d_656[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_659 (MaxPooling1  (None, 25, 100)     0           ['conv1d_659[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_662 (MaxPooling1  (None, 25, 100)     0           ['conv1d_662[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_665 (MaxPooling1  (None, 25, 100)     0           ['conv1d_665[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_668 (MaxPooling1  (None, 25, 100)     0           ['conv1d_668[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_671 (MaxPooling1  (None, 25, 100)     0           ['conv1d_671[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_674 (MaxPooling1  (None, 25, 100)     0           ['conv1d_674[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_677 (MaxPooling1  (None, 25, 100)     0           ['conv1d_677[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_680 (MaxPooling1  (None, 25, 100)     0           ['conv1d_680[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_636 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_635[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_639 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_638[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_642 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_641[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_645 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_644[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_648 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_647[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_651 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_650[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_654 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_653[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_657 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_656[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_660 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_659[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_663 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_662[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_666 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_665[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_669 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_668[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_672 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_671[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_675 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_674[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_678 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_677[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_681 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_680[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling1d_636 (MaxPooling1  (None, 13, 100)     0           ['conv1d_636[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_639 (MaxPooling1  (None, 13, 100)     0           ['conv1d_639[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_642 (MaxPooling1  (None, 13, 100)     0           ['conv1d_642[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_645 (MaxPooling1  (None, 13, 100)     0           ['conv1d_645[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_648 (MaxPooling1  (None, 13, 100)     0           ['conv1d_648[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_651 (MaxPooling1  (None, 13, 100)     0           ['conv1d_651[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_654 (MaxPooling1  (None, 13, 100)     0           ['conv1d_654[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_657 (MaxPooling1  (None, 13, 100)     0           ['conv1d_657[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_660 (MaxPooling1  (None, 13, 100)     0           ['conv1d_660[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_663 (MaxPooling1  (None, 13, 100)     0           ['conv1d_663[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_666 (MaxPooling1  (None, 13, 100)     0           ['conv1d_666[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_669 (MaxPooling1  (None, 13, 100)     0           ['conv1d_669[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_672 (MaxPooling1  (None, 13, 100)     0           ['conv1d_672[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_675 (MaxPooling1  (None, 13, 100)     0           ['conv1d_675[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_678 (MaxPooling1  (None, 13, 100)     0           ['conv1d_678[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_681 (MaxPooling1  (None, 13, 100)     0           ['conv1d_681[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 13, 1600)     0           ['max_pooling1d_636[0][0]',      \n",
      "                                                                  'max_pooling1d_639[0][0]',      \n",
      "                                                                  'max_pooling1d_642[0][0]',      \n",
      "                                                                  'max_pooling1d_645[0][0]',      \n",
      "                                                                  'max_pooling1d_648[0][0]',      \n",
      "                                                                  'max_pooling1d_651[0][0]',      \n",
      "                                                                  'max_pooling1d_654[0][0]',      \n",
      "                                                                  'max_pooling1d_657[0][0]',      \n",
      "                                                                  'max_pooling1d_660[0][0]',      \n",
      "                                                                  'max_pooling1d_663[0][0]',      \n",
      "                                                                  'max_pooling1d_666[0][0]',      \n",
      "                                                                  'max_pooling1d_669[0][0]',      \n",
      "                                                                  'max_pooling1d_672[0][0]',      \n",
      "                                                                  'max_pooling1d_675[0][0]',      \n",
      "                                                                  'max_pooling1d_678[0][0]',      \n",
      "                                                                  'max_pooling1d_681[0][0]']      \n",
      "                                                                                                  \n",
      " lstm_24 (LSTM)                 (None, 10)           64440       ['concatenate_24[0][0]']         \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 25)           275         ['lstm_24[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 25)           0           ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " dense_77 (Dense)               (None, 10)           260         ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 10)           0           ['dense_77[0][0]']               \n",
      "                                                                                                  \n",
      " dense_78 (Dense)               (None, 1)            11          ['dropout_53[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,034,586\n",
      "Trainable params: 1,034,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, ConvLSTM1D, Flatten, LSTM, Permute, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Reshape, SpatialDropout1D, Dropout, TimeDistributed\n",
    "\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from featurize import featurize\n",
    "\n",
    "from scaler import CustomScaler\n",
    "from VAE import get_auth_model\n",
    "\n",
    "def get_eer(scores_legit, scores_adv):\n",
    "\tscores_legit = sorted(scores_legit)\n",
    "\tscores_adv = sorted(scores_adv)\n",
    "\t\n",
    "\t#treat each legitimate sample distance as a possible threshold, determine the point where FRR crosses FAR\n",
    "\tfor c, threshold in enumerate(scores_legit):\n",
    "\t\tfrr = c * 1.0 / len(scores_legit)\n",
    "\t\tadv_index = next((x[0] for x in enumerate(scores_adv) if x[1] > threshold), len(scores_adv))\n",
    "\t\tfar = 1 - (adv_index * 1.0 / len(scores_adv))\n",
    "\t\tif frr >= far:\n",
    "\t\t\treturn threshold, far\n",
    "\tprint(\"Failure\")\n",
    "\n",
    "class SplitLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super(SplitLayer, self).__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(inputs,indices=self.layers,axis=-1)\n",
    "\n",
    "\n",
    "def get_new_auth_model(input_dim=(200,16)):\n",
    "    #input_dim = (200,16)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_dim)\n",
    "    x = inputs\n",
    "    \n",
    "    xs = []\n",
    "    \n",
    "    for i in range(input_dim[1]):\n",
    "        \n",
    "        x = SplitLayer(i)(inputs)\n",
    "        reshaped = Reshape((200, 1))(x)\n",
    "        \n",
    "        x = Conv1D(100, 3, strides=2, padding=\"same\")(reshaped)   #, kernel_regularizer=l2(1e-5)\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "        \n",
    "        x = Conv1D(100, 3, strides=1,  padding=\"same\")(x)     #   kernel_regularizer=l2(1e-5),\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "        \n",
    "        x = Conv1D(100, 3, strides=1,  padding=\"same\")(x)     #   kernel_regularizer=l2(1e-5),\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "\n",
    "        \n",
    "        xs.append(x)\n",
    "    \n",
    "    x = layers.Concatenate()(xs)\n",
    "    x = LSTM(10)(x)\n",
    "\n",
    "    x = Dense(25, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(10, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    #x = layers.Lambda(lambda x: x / 2)\n",
    "    out = x\n",
    "\n",
    "    model = keras.Model(inputs, out, name=\"LSTM_classifier_v2\")\n",
    "    \n",
    "    return model\n",
    "\"\"\"\n",
    "def get_new_auth_model2(input_dim=(200,16)):\n",
    "    #input_dim = (200,16)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_dim)\n",
    "    x = inputs\n",
    "    \n",
    "    \n",
    "    layer_list = [Conv1D(100, 3, strides=1, padding=\"same\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "        \n",
    "        Conv1D(100, 3, strides=1,  padding=\"same\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "\n",
    "        Conv1D(100, 3, strides=1, padding=\"same\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "        \n",
    "        Conv1D(100, 3, strides=1, padding=\"same\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "        \n",
    "        Conv1D(100, 3, strides=1, padding=\"same\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "        \n",
    "        Conv1D(100, 7, strides=1, padding=\"valid\"),\n",
    "        MaxPooling1D(pool_size=2, strides=None, padding=\"same\"),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(25, activation=\"relu\")]\n",
    "    \n",
    "    xs = []\n",
    "    \n",
    "    for i in range(input_dim[1]):\n",
    "        \n",
    "        x = SplitLayer(i)(inputs)\n",
    "        x = Reshape((200, 1))(x)\n",
    "        \n",
    "        for layer in layer_list:\n",
    "            x = layer(x)\n",
    "        xs.append(x)\n",
    "    \n",
    "    x = layers.Concatenate()(xs)\n",
    "\n",
    "    x = Dense(25, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(10, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation=\"softmax\")(x)\n",
    "    #x = layers.Lambda(lambda x: x / 2)\n",
    "    out = x\n",
    "\n",
    "    model = keras.Model(inputs, out, name=\"LSTM_classifier_v2\")\n",
    "    \n",
    "    return model\n",
    "\"\"\"\n",
    "get_new_auth_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0080ba46-4227-4466-b8db-fa951e50f390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:35:11.561747Z",
     "iopub.status.busy": "2023-02-25T11:35:11.561486Z",
     "iopub.status.idle": "2023-02-25T11:35:12.702289Z",
     "shell.execute_reply": "2023-02-25T11:35:12.701403Z",
     "shell.execute_reply.started": "2023-02-25T11:35:11.561724Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"raw_with_maps\" # or offsets_2\n",
    "\n",
    "x_data = np.load(f\"data/processed/x_{file_name}_filtered.npy\")[:,:,[0,1,2,4,5,6]] # pre-filtered data is \"windowed_filtered\"\n",
    "feature_x_data = np.load(f\"data/processed/x_{file_name}_features.npy\")\n",
    "y_user = np.load(f\"data/processed/y_user_{file_name}.npy\")\n",
    "y_intent = np.load(f\"data/processed/y_intent_{file_name}.npy\")\n",
    "y_gesture = np.load(f\"data/processed/y_gesture_type_{file_name}.npy\")\n",
    "\n",
    "train_gesture_map = np.load(f\"data/processed/train_gesture_map_{file_name}.npy\")\n",
    "test_gesture_map = np.load(f\"data/processed/test_gesture_map_{file_name}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9393f3c3-c11c-4238-97db-b29759277d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:41:14.729414Z",
     "iopub.status.busy": "2023-02-25T11:41:14.728533Z",
     "iopub.status.idle": "2023-02-25T11:41:14.823181Z",
     "shell.execute_reply": "2023-02-25T11:41:14.822460Z",
     "shell.execute_reply.started": "2023-02-25T11:41:14.729376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2311,   88]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth_user = 8\n",
    "\n",
    "_map = (train_gesture_map == 1)\n",
    "_test_map = (test_gesture_map == 1)\n",
    "data_train = x_data[_map] #[train_gesture_map == 1]\n",
    "labels_train = (y_user.argmax(axis=1) == auth_user)[_map].astype(int) #\n",
    "\n",
    "\n",
    "shuffled_data_train, shuffled_labels_train = shuffle(data_train, labels_train, random_state=0)\n",
    "\n",
    "scaler = CustomScaler()\n",
    "scaler.CHANNELS = 6\n",
    "shuffled_data_train = scaler.fit_and_transform(shuffled_data_train)\n",
    "\n",
    "np.unique(shuffled_labels_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8f225f9b-aacd-4f50-9f3e-1b73e0eb464f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:41:15.074278Z",
     "iopub.status.busy": "2023-02-25T11:41:15.073412Z",
     "iopub.status.idle": "2023-02-25T11:43:43.689482Z",
     "shell.execute_reply": "2023-02-25T11:43:43.688851Z",
     "shell.execute_reply.started": "2023-02-25T11:41:15.074248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_classifier_v2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 200, 6)]     0           []                               \n",
      "                                                                                                  \n",
      " split_layer_256 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_257 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_258 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_259 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_260 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " split_layer_261 (SplitLayer)   (None, 200)          0           ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_256 (Reshape)          (None, 200, 1)       0           ['split_layer_256[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_257 (Reshape)          (None, 200, 1)       0           ['split_layer_257[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_258 (Reshape)          (None, 200, 1)       0           ['split_layer_258[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_259 (Reshape)          (None, 200, 1)       0           ['split_layer_259[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_260 (Reshape)          (None, 200, 1)       0           ['split_layer_260[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_261 (Reshape)          (None, 200, 1)       0           ['split_layer_261[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_700 (Conv1D)            (None, 100, 100)     400         ['reshape_256[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_703 (Conv1D)            (None, 100, 100)     400         ['reshape_257[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_706 (Conv1D)            (None, 100, 100)     400         ['reshape_258[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_709 (Conv1D)            (None, 100, 100)     400         ['reshape_259[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_712 (Conv1D)            (None, 100, 100)     400         ['reshape_260[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_715 (Conv1D)            (None, 100, 100)     400         ['reshape_261[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_700 (MaxPooling1  (None, 50, 100)     0           ['conv1d_700[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_703 (MaxPooling1  (None, 50, 100)     0           ['conv1d_703[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_706 (MaxPooling1  (None, 50, 100)     0           ['conv1d_706[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_709 (MaxPooling1  (None, 50, 100)     0           ['conv1d_709[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_712 (MaxPooling1  (None, 50, 100)     0           ['conv1d_712[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_715 (MaxPooling1  (None, 50, 100)     0           ['conv1d_715[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_701 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_700[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_704 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_703[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_707 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_706[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_710 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_709[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_713 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_712[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_716 (Conv1D)            (None, 50, 100)      30100       ['max_pooling1d_715[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling1d_701 (MaxPooling1  (None, 25, 100)     0           ['conv1d_701[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_704 (MaxPooling1  (None, 25, 100)     0           ['conv1d_704[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_707 (MaxPooling1  (None, 25, 100)     0           ['conv1d_707[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_710 (MaxPooling1  (None, 25, 100)     0           ['conv1d_710[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_713 (MaxPooling1  (None, 25, 100)     0           ['conv1d_713[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_716 (MaxPooling1  (None, 25, 100)     0           ['conv1d_716[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_702 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_701[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_705 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_704[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_708 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_707[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_711 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_710[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_714 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_713[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_717 (Conv1D)            (None, 25, 100)      30100       ['max_pooling1d_716[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling1d_702 (MaxPooling1  (None, 13, 100)     0           ['conv1d_702[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_705 (MaxPooling1  (None, 13, 100)     0           ['conv1d_705[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_708 (MaxPooling1  (None, 13, 100)     0           ['conv1d_708[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_711 (MaxPooling1  (None, 13, 100)     0           ['conv1d_711[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_714 (MaxPooling1  (None, 13, 100)     0           ['conv1d_714[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_717 (MaxPooling1  (None, 13, 100)     0           ['conv1d_717[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 13, 600)      0           ['max_pooling1d_702[0][0]',      \n",
      "                                                                  'max_pooling1d_705[0][0]',      \n",
      "                                                                  'max_pooling1d_708[0][0]',      \n",
      "                                                                  'max_pooling1d_711[0][0]',      \n",
      "                                                                  'max_pooling1d_714[0][0]',      \n",
      "                                                                  'max_pooling1d_717[0][0]']      \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)                 (None, 10)           24440       ['concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " dense_82 (Dense)               (None, 25)           275         ['lstm_26[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_56 (Dropout)           (None, 25)           0           ['dense_82[0][0]']               \n",
      "                                                                                                  \n",
      " dense_83 (Dense)               (None, 10)           260         ['dropout_56[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_57 (Dropout)           (None, 10)           0           ['dense_83[0][0]']               \n",
      "                                                                                                  \n",
      " dense_84 (Dense)               (None, 1)            11          ['dropout_57[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 388,586\n",
      "Trainable params: 388,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "4/4 [==============================] - 6s 280ms/step - loss: 0.5183 - val_loss: 0.6890\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5178 - val_loss: 0.6871\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.5166 - val_loss: 0.6850\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5151 - val_loss: 0.6826\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5152 - val_loss: 0.6802\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5142 - val_loss: 0.6775\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5131 - val_loss: 0.6751\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5106 - val_loss: 0.6726\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5118 - val_loss: 0.6700\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.5093 - val_loss: 0.6675\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.5099 - val_loss: 0.6651\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5087 - val_loss: 0.6628\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5070 - val_loss: 0.6602\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.5085 - val_loss: 0.6578\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.5077 - val_loss: 0.6554\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5058 - val_loss: 0.6531\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5051 - val_loss: 0.6509\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5037 - val_loss: 0.6485\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.5020 - val_loss: 0.6460\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5020 - val_loss: 0.6434\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.5009 - val_loss: 0.6408\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5014 - val_loss: 0.6383\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5012 - val_loss: 0.6357\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4981 - val_loss: 0.6334\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4994 - val_loss: 0.6311\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4968 - val_loss: 0.6288\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4940 - val_loss: 0.6265\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4954 - val_loss: 0.6240\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4954 - val_loss: 0.6216\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4942 - val_loss: 0.6193\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4933 - val_loss: 0.6172\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4953 - val_loss: 0.6152\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4916 - val_loss: 0.6132\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4912 - val_loss: 0.6112\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4892 - val_loss: 0.6091\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4895 - val_loss: 0.6067\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4903 - val_loss: 0.6042\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4889 - val_loss: 0.6022\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4898 - val_loss: 0.6002\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4885 - val_loss: 0.5985\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4863 - val_loss: 0.5965\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4859 - val_loss: 0.5947\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4834 - val_loss: 0.5929\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4841 - val_loss: 0.5910\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4844 - val_loss: 0.5893\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4856 - val_loss: 0.5876\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4816 - val_loss: 0.5859\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4820 - val_loss: 0.5841\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4798 - val_loss: 0.5825\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4796 - val_loss: 0.5805\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4795 - val_loss: 0.5787\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4815 - val_loss: 0.5771\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4780 - val_loss: 0.5754\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4793 - val_loss: 0.5739\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4807 - val_loss: 0.5724\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4727 - val_loss: 0.5706\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4773 - val_loss: 0.5691\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4791 - val_loss: 0.5675\n",
      "Epoch 59/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4753 - val_loss: 0.5662\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4737 - val_loss: 0.5648\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4741 - val_loss: 0.5633\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4738 - val_loss: 0.5618\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4721 - val_loss: 0.5602\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4667 - val_loss: 0.5587\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4758 - val_loss: 0.5575\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4686 - val_loss: 0.5564\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4672 - val_loss: 0.5550\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4669 - val_loss: 0.5535\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4657 - val_loss: 0.5518\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4740 - val_loss: 0.5503\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4703 - val_loss: 0.5488\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4667 - val_loss: 0.5476\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4604 - val_loss: 0.5464\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4663 - val_loss: 0.5452\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4651 - val_loss: 0.5438\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4697 - val_loss: 0.5429\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4672 - val_loss: 0.5418\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4635 - val_loss: 0.5407\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4655 - val_loss: 0.5398\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4641 - val_loss: 0.5390\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4695 - val_loss: 0.5380\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4642 - val_loss: 0.5373\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4657 - val_loss: 0.5364\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4587 - val_loss: 0.5356\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4542 - val_loss: 0.5344\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4578 - val_loss: 0.5330\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4508 - val_loss: 0.5316\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4563 - val_loss: 0.5302\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4596 - val_loss: 0.5288\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4622 - val_loss: 0.5272\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4439 - val_loss: 0.5258\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4491 - val_loss: 0.5241\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4530 - val_loss: 0.5227\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4575 - val_loss: 0.5218\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4559 - val_loss: 0.5207\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4507 - val_loss: 0.5195\n",
      "Epoch 97/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4505 - val_loss: 0.5187\n",
      "Epoch 98/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4515 - val_loss: 0.5178\n",
      "Epoch 99/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4605 - val_loss: 0.5170\n",
      "Epoch 100/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4549 - val_loss: 0.5164\n",
      "Epoch 101/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4515 - val_loss: 0.5156\n",
      "Epoch 102/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4560 - val_loss: 0.5149\n",
      "Epoch 103/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4571 - val_loss: 0.5141\n",
      "Epoch 104/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4503 - val_loss: 0.5136\n",
      "Epoch 105/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4550 - val_loss: 0.5133\n",
      "Epoch 106/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4503 - val_loss: 0.5127\n",
      "Epoch 107/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4483 - val_loss: 0.5116\n",
      "Epoch 108/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4447 - val_loss: 0.5105\n",
      "Epoch 109/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4556 - val_loss: 0.5098\n",
      "Epoch 110/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4482 - val_loss: 0.5092\n",
      "Epoch 111/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4489 - val_loss: 0.5087\n",
      "Epoch 112/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4489 - val_loss: 0.5075\n",
      "Epoch 113/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4493 - val_loss: 0.5061\n",
      "Epoch 114/1000\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.4481 - val_loss: 0.5049\n",
      "Epoch 115/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4447 - val_loss: 0.5038\n",
      "Epoch 116/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4351 - val_loss: 0.5029\n",
      "Epoch 117/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4489 - val_loss: 0.5026\n",
      "Epoch 118/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4371 - val_loss: 0.5018\n",
      "Epoch 119/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4426 - val_loss: 0.5015\n",
      "Epoch 120/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4384 - val_loss: 0.5009\n",
      "Epoch 121/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4463 - val_loss: 0.4999\n",
      "Epoch 122/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4360 - val_loss: 0.4987\n",
      "Epoch 123/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4447 - val_loss: 0.4977\n",
      "Epoch 124/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4366 - val_loss: 0.4964\n",
      "Epoch 125/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4360 - val_loss: 0.4951\n",
      "Epoch 126/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4410 - val_loss: 0.4936\n",
      "Epoch 127/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4330 - val_loss: 0.4919\n",
      "Epoch 128/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4398 - val_loss: 0.4905\n",
      "Epoch 129/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4382 - val_loss: 0.4890\n",
      "Epoch 130/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4481 - val_loss: 0.4880\n",
      "Epoch 131/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4418 - val_loss: 0.4874\n",
      "Epoch 132/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4382 - val_loss: 0.4865\n",
      "Epoch 133/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4374 - val_loss: 0.4856\n",
      "Epoch 134/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4337 - val_loss: 0.4846\n",
      "Epoch 135/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4289 - val_loss: 0.4838\n",
      "Epoch 136/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4409 - val_loss: 0.4830\n",
      "Epoch 137/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4345 - val_loss: 0.4824\n",
      "Epoch 138/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4318 - val_loss: 0.4817\n",
      "Epoch 139/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4323 - val_loss: 0.4808\n",
      "Epoch 140/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4280 - val_loss: 0.4800\n",
      "Epoch 141/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4340 - val_loss: 0.4791\n",
      "Epoch 142/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4341 - val_loss: 0.4781\n",
      "Epoch 143/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4323 - val_loss: 0.4768\n",
      "Epoch 144/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4334 - val_loss: 0.4754\n",
      "Epoch 145/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4306 - val_loss: 0.4739\n",
      "Epoch 146/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4341 - val_loss: 0.4731\n",
      "Epoch 147/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4308 - val_loss: 0.4723\n",
      "Epoch 148/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4186 - val_loss: 0.4713\n",
      "Epoch 149/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4298 - val_loss: 0.4702\n",
      "Epoch 150/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4278 - val_loss: 0.4696\n",
      "Epoch 151/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4225 - val_loss: 0.4687\n",
      "Epoch 152/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4316 - val_loss: 0.4677\n",
      "Epoch 153/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4289 - val_loss: 0.4670\n",
      "Epoch 154/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4238 - val_loss: 0.4664\n",
      "Epoch 155/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4198 - val_loss: 0.4655\n",
      "Epoch 156/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4228 - val_loss: 0.4637\n",
      "Epoch 157/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4290 - val_loss: 0.4621\n",
      "Epoch 158/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4231 - val_loss: 0.4610\n",
      "Epoch 159/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4203 - val_loss: 0.4598\n",
      "Epoch 160/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4254 - val_loss: 0.4590\n",
      "Epoch 161/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4226 - val_loss: 0.4577\n",
      "Epoch 162/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4226 - val_loss: 0.4567\n",
      "Epoch 163/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4154 - val_loss: 0.4554\n",
      "Epoch 164/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4156 - val_loss: 0.4546\n",
      "Epoch 165/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4161 - val_loss: 0.4540\n",
      "Epoch 166/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4187 - val_loss: 0.4538\n",
      "Epoch 167/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4243 - val_loss: 0.4531\n",
      "Epoch 168/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4147 - val_loss: 0.4524\n",
      "Epoch 169/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4138 - val_loss: 0.4514\n",
      "Epoch 170/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4176 - val_loss: 0.4501\n",
      "Epoch 171/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4235 - val_loss: 0.4492\n",
      "Epoch 172/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4094 - val_loss: 0.4482\n",
      "Epoch 173/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4151 - val_loss: 0.4471\n",
      "Epoch 174/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4139 - val_loss: 0.4464\n",
      "Epoch 175/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4072 - val_loss: 0.4452\n",
      "Epoch 176/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4127 - val_loss: 0.4440\n",
      "Epoch 177/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4126 - val_loss: 0.4425\n",
      "Epoch 178/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4076 - val_loss: 0.4415\n",
      "Epoch 179/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4117 - val_loss: 0.4407\n",
      "Epoch 180/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4152 - val_loss: 0.4402\n",
      "Epoch 181/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4076 - val_loss: 0.4390\n",
      "Epoch 182/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4153 - val_loss: 0.4380\n",
      "Epoch 183/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4067 - val_loss: 0.4366\n",
      "Epoch 184/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4137 - val_loss: 0.4353\n",
      "Epoch 185/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3998 - val_loss: 0.4339\n",
      "Epoch 186/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4128 - val_loss: 0.4327\n",
      "Epoch 187/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4147 - val_loss: 0.4321\n",
      "Epoch 188/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4126 - val_loss: 0.4311\n",
      "Epoch 189/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4098 - val_loss: 0.4307\n",
      "Epoch 190/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4107 - val_loss: 0.4306\n",
      "Epoch 191/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4004 - val_loss: 0.4298\n",
      "Epoch 192/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3989 - val_loss: 0.4285\n",
      "Epoch 193/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4034 - val_loss: 0.4273\n",
      "Epoch 194/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4089 - val_loss: 0.4256\n",
      "Epoch 195/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4043 - val_loss: 0.4237\n",
      "Epoch 196/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4006 - val_loss: 0.4235\n",
      "Epoch 197/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4010 - val_loss: 0.4229\n",
      "Epoch 198/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4068 - val_loss: 0.4223\n",
      "Epoch 199/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4030 - val_loss: 0.4214\n",
      "Epoch 200/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4036 - val_loss: 0.4208\n",
      "Epoch 201/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4023 - val_loss: 0.4199\n",
      "Epoch 202/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4038 - val_loss: 0.4189\n",
      "Epoch 203/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3986 - val_loss: 0.4178\n",
      "Epoch 204/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3940 - val_loss: 0.4167\n",
      "Epoch 205/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4086 - val_loss: 0.4162\n",
      "Epoch 206/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3965 - val_loss: 0.4157\n",
      "Epoch 207/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3967 - val_loss: 0.4147\n",
      "Epoch 208/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4005 - val_loss: 0.4135\n",
      "Epoch 209/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3977 - val_loss: 0.4123\n",
      "Epoch 210/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4021 - val_loss: 0.4113\n",
      "Epoch 211/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3932 - val_loss: 0.4105\n",
      "Epoch 212/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3883 - val_loss: 0.4093\n",
      "Epoch 213/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3844 - val_loss: 0.4079\n",
      "Epoch 214/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3925 - val_loss: 0.4069\n",
      "Epoch 215/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3948 - val_loss: 0.4066\n",
      "Epoch 216/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3769 - val_loss: 0.4063\n",
      "Epoch 217/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3967 - val_loss: 0.4053\n",
      "Epoch 218/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3889 - val_loss: 0.4047\n",
      "Epoch 219/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3969 - val_loss: 0.4048\n",
      "Epoch 220/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3963 - val_loss: 0.4044\n",
      "Epoch 221/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3993 - val_loss: 0.4038\n",
      "Epoch 222/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3902 - val_loss: 0.4039\n",
      "Epoch 223/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3861 - val_loss: 0.4030\n",
      "Epoch 224/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3867 - val_loss: 0.4024\n",
      "Epoch 225/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3790 - val_loss: 0.4013\n",
      "Epoch 226/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3874 - val_loss: 0.3996\n",
      "Epoch 227/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3837 - val_loss: 0.3985\n",
      "Epoch 228/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3863 - val_loss: 0.3977\n",
      "Epoch 229/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3743 - val_loss: 0.3965\n",
      "Epoch 230/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3793 - val_loss: 0.3953\n",
      "Epoch 231/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3860 - val_loss: 0.3948\n",
      "Epoch 232/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3882 - val_loss: 0.3944\n",
      "Epoch 233/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3808 - val_loss: 0.3933\n",
      "Epoch 234/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3760 - val_loss: 0.3923\n",
      "Epoch 235/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3774 - val_loss: 0.3915\n",
      "Epoch 236/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3876 - val_loss: 0.3907\n",
      "Epoch 237/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3881 - val_loss: 0.3898\n",
      "Epoch 238/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3747 - val_loss: 0.3894\n",
      "Epoch 239/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3892 - val_loss: 0.3894\n",
      "Epoch 240/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3761 - val_loss: 0.3896\n",
      "Epoch 241/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3804 - val_loss: 0.3897\n",
      "Epoch 242/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3813 - val_loss: 0.3889\n",
      "Epoch 243/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3749 - val_loss: 0.3875\n",
      "Epoch 244/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3794 - val_loss: 0.3864\n",
      "Epoch 245/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3761 - val_loss: 0.3846\n",
      "Epoch 246/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3801 - val_loss: 0.3831\n",
      "Epoch 247/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3767 - val_loss: 0.3819\n",
      "Epoch 248/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3818 - val_loss: 0.3818\n",
      "Epoch 249/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3852 - val_loss: 0.3817\n",
      "Epoch 250/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3781 - val_loss: 0.3816\n",
      "Epoch 251/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3795 - val_loss: 0.3814\n",
      "Epoch 252/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3820 - val_loss: 0.3802\n",
      "Epoch 253/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3765 - val_loss: 0.3790\n",
      "Epoch 254/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3719 - val_loss: 0.3777\n",
      "Epoch 255/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3780 - val_loss: 0.3766\n",
      "Epoch 256/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3821 - val_loss: 0.3753\n",
      "Epoch 257/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3805 - val_loss: 0.3751\n",
      "Epoch 258/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3829 - val_loss: 0.3746\n",
      "Epoch 259/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3709 - val_loss: 0.3737\n",
      "Epoch 260/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3646 - val_loss: 0.3726\n",
      "Epoch 261/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3825 - val_loss: 0.3717\n",
      "Epoch 262/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3768 - val_loss: 0.3713\n",
      "Epoch 263/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3701 - val_loss: 0.3708\n",
      "Epoch 264/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3733 - val_loss: 0.3711\n",
      "Epoch 265/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3690 - val_loss: 0.3700\n",
      "Epoch 266/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3678 - val_loss: 0.3692\n",
      "Epoch 267/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3784 - val_loss: 0.3682\n",
      "Epoch 268/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3695 - val_loss: 0.3676\n",
      "Epoch 269/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3808 - val_loss: 0.3673\n",
      "Epoch 270/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3769 - val_loss: 0.3664\n",
      "Epoch 271/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3739 - val_loss: 0.3655\n",
      "Epoch 272/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3721 - val_loss: 0.3645\n",
      "Epoch 273/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3767 - val_loss: 0.3637\n",
      "Epoch 274/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3709 - val_loss: 0.3637\n",
      "Epoch 275/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3662 - val_loss: 0.3634\n",
      "Epoch 276/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3581 - val_loss: 0.3634\n",
      "Epoch 277/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3575 - val_loss: 0.3627\n",
      "Epoch 278/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3804 - val_loss: 0.3619\n",
      "Epoch 279/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3806 - val_loss: 0.3614\n",
      "Epoch 280/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3704 - val_loss: 0.3608\n",
      "Epoch 281/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3738 - val_loss: 0.3595\n",
      "Epoch 282/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3717 - val_loss: 0.3586\n",
      "Epoch 283/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3489 - val_loss: 0.3574\n",
      "Epoch 284/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3659 - val_loss: 0.3562\n",
      "Epoch 285/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3618 - val_loss: 0.3552\n",
      "Epoch 286/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3709 - val_loss: 0.3546\n",
      "Epoch 287/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3635 - val_loss: 0.3551\n",
      "Epoch 288/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3589 - val_loss: 0.3550\n",
      "Epoch 289/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3675 - val_loss: 0.3545\n",
      "Epoch 290/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3671 - val_loss: 0.3543\n",
      "Epoch 291/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3665 - val_loss: 0.3535\n",
      "Epoch 292/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3670 - val_loss: 0.3527\n",
      "Epoch 293/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3663 - val_loss: 0.3523\n",
      "Epoch 294/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3571 - val_loss: 0.3517\n",
      "Epoch 295/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3689 - val_loss: 0.3515\n",
      "Epoch 296/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3655 - val_loss: 0.3511\n",
      "Epoch 297/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3575 - val_loss: 0.3513\n",
      "Epoch 298/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3502 - val_loss: 0.3510\n",
      "Epoch 299/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3560 - val_loss: 0.3503\n",
      "Epoch 300/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3544 - val_loss: 0.3491\n",
      "Epoch 301/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3624 - val_loss: 0.3479\n",
      "Epoch 302/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3727 - val_loss: 0.3470\n",
      "Epoch 303/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3562 - val_loss: 0.3467\n",
      "Epoch 304/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3452 - val_loss: 0.3463\n",
      "Epoch 305/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3602 - val_loss: 0.3457\n",
      "Epoch 306/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3552 - val_loss: 0.3452\n",
      "Epoch 307/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3580 - val_loss: 0.3460\n",
      "Epoch 308/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3592 - val_loss: 0.3463\n",
      "Epoch 309/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3543 - val_loss: 0.3462\n",
      "Epoch 310/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3704 - val_loss: 0.3461\n",
      "Epoch 311/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3451 - val_loss: 0.3451\n",
      "Epoch 312/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3632 - val_loss: 0.3441\n",
      "Epoch 313/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3485 - val_loss: 0.3432\n",
      "Epoch 314/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3590 - val_loss: 0.3423\n",
      "Epoch 315/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3418 - val_loss: 0.3418\n",
      "Epoch 316/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3543 - val_loss: 0.3414\n",
      "Epoch 317/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3534 - val_loss: 0.3406\n",
      "Epoch 318/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3425 - val_loss: 0.3398\n",
      "Epoch 319/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3431 - val_loss: 0.3386\n",
      "Epoch 320/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3421 - val_loss: 0.3373\n",
      "Epoch 321/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3505 - val_loss: 0.3369\n",
      "Epoch 322/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3517 - val_loss: 0.3363\n",
      "Epoch 323/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3577 - val_loss: 0.3360\n",
      "Epoch 324/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3512 - val_loss: 0.3357\n",
      "Epoch 325/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3527 - val_loss: 0.3358\n",
      "Epoch 326/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3442 - val_loss: 0.3361\n",
      "Epoch 327/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3467 - val_loss: 0.3356\n",
      "Epoch 328/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3601 - val_loss: 0.3350\n",
      "Epoch 329/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3515 - val_loss: 0.3340\n",
      "Epoch 330/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3535 - val_loss: 0.3330\n",
      "Epoch 331/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3515 - val_loss: 0.3325\n",
      "Epoch 332/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3377 - val_loss: 0.3316\n",
      "Epoch 333/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3509 - val_loss: 0.3312\n",
      "Epoch 334/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3518 - val_loss: 0.3312\n",
      "Epoch 335/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3551 - val_loss: 0.3311\n",
      "Epoch 336/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3598 - val_loss: 0.3307\n",
      "Epoch 337/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3426 - val_loss: 0.3304\n",
      "Epoch 338/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3391 - val_loss: 0.3299\n",
      "Epoch 339/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3381 - val_loss: 0.3296\n",
      "Epoch 340/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.3564 - val_loss: 0.3294\n",
      "Epoch 341/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3454 - val_loss: 0.3286\n",
      "Epoch 342/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3473 - val_loss: 0.3281\n",
      "Epoch 343/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3366 - val_loss: 0.3272\n",
      "Epoch 344/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3608 - val_loss: 0.3265\n",
      "Epoch 345/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3447 - val_loss: 0.3258\n",
      "Epoch 346/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3403 - val_loss: 0.3249\n",
      "Epoch 347/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3244 - val_loss: 0.3239\n",
      "Epoch 348/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3418 - val_loss: 0.3232\n",
      "Epoch 349/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3468 - val_loss: 0.3230\n",
      "Epoch 350/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3372 - val_loss: 0.3230\n",
      "Epoch 351/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3419 - val_loss: 0.3226\n",
      "Epoch 352/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3392 - val_loss: 0.3228\n",
      "Epoch 353/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3376 - val_loss: 0.3228\n",
      "Epoch 354/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3362 - val_loss: 0.3225\n",
      "Epoch 355/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3486 - val_loss: 0.3220\n",
      "Epoch 356/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3477 - val_loss: 0.3212\n",
      "Epoch 357/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3450 - val_loss: 0.3207\n",
      "Epoch 358/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3559 - val_loss: 0.3203\n",
      "Epoch 359/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3458 - val_loss: 0.3204\n",
      "Epoch 360/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3409 - val_loss: 0.3202\n",
      "Epoch 361/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3501 - val_loss: 0.3194\n",
      "Epoch 362/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3264 - val_loss: 0.3187\n",
      "Epoch 363/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3417 - val_loss: 0.3185\n",
      "Epoch 364/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3395 - val_loss: 0.3186\n",
      "Epoch 365/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3352 - val_loss: 0.3182\n",
      "Epoch 366/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3414 - val_loss: 0.3180\n",
      "Epoch 367/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3373 - val_loss: 0.3171\n",
      "Epoch 368/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3379 - val_loss: 0.3166\n",
      "Epoch 369/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3319 - val_loss: 0.3162\n",
      "Epoch 370/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3344 - val_loss: 0.3154\n",
      "Epoch 371/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.3198 - val_loss: 0.3144\n",
      "Epoch 372/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3232 - val_loss: 0.3136\n",
      "Epoch 373/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3207 - val_loss: 0.3131\n",
      "Epoch 374/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3414 - val_loss: 0.3133\n",
      "Epoch 375/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3473 - val_loss: 0.3136\n",
      "Epoch 376/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3363 - val_loss: 0.3133\n",
      "Epoch 377/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3232 - val_loss: 0.3127\n",
      "Epoch 378/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3412 - val_loss: 0.3123\n",
      "Epoch 379/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3323 - val_loss: 0.3109\n",
      "Epoch 380/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3393 - val_loss: 0.3093\n",
      "Epoch 381/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3345 - val_loss: 0.3082\n",
      "Epoch 382/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3245 - val_loss: 0.3073\n",
      "Epoch 383/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3361 - val_loss: 0.3067\n",
      "Epoch 384/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3303 - val_loss: 0.3066\n",
      "Epoch 385/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3297 - val_loss: 0.3074\n",
      "Epoch 386/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3367 - val_loss: 0.3082\n",
      "Epoch 387/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3382 - val_loss: 0.3082\n",
      "Epoch 388/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3394 - val_loss: 0.3078\n",
      "Epoch 389/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3297 - val_loss: 0.3075\n",
      "Epoch 390/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3402 - val_loss: 0.3068\n",
      "Epoch 391/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3208 - val_loss: 0.3061\n",
      "Epoch 392/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3203 - val_loss: 0.3051\n",
      "Epoch 393/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3400 - val_loss: 0.3043\n",
      "Epoch 394/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3236 - val_loss: 0.3035\n",
      "Epoch 395/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3339 - val_loss: 0.3038\n",
      "Epoch 396/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3216 - val_loss: 0.3038\n",
      "Epoch 397/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3336 - val_loss: 0.3036\n",
      "Epoch 398/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3289 - val_loss: 0.3029\n",
      "Epoch 399/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3287 - val_loss: 0.3023\n",
      "Epoch 400/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3125 - val_loss: 0.3018\n",
      "Epoch 401/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3275 - val_loss: 0.3016\n",
      "Epoch 402/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3331 - val_loss: 0.3012\n",
      "Epoch 403/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3030 - val_loss: 0.3008\n",
      "Epoch 404/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3392 - val_loss: 0.3001\n",
      "Epoch 405/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3477 - val_loss: 0.2998\n",
      "Epoch 406/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3257 - val_loss: 0.2997\n",
      "Epoch 407/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3261 - val_loss: 0.3002\n",
      "Epoch 408/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3243 - val_loss: 0.3002\n",
      "Epoch 409/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3296 - val_loss: 0.2999\n",
      "Epoch 410/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3222 - val_loss: 0.2996\n",
      "Epoch 411/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3321 - val_loss: 0.2987\n",
      "Epoch 412/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3131 - val_loss: 0.2975\n",
      "Epoch 413/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3309 - val_loss: 0.2967\n",
      "Epoch 414/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3237 - val_loss: 0.2960\n",
      "Epoch 415/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3377 - val_loss: 0.2964\n",
      "Epoch 416/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3276 - val_loss: 0.2964\n",
      "Epoch 417/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3233 - val_loss: 0.2961\n",
      "Epoch 418/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3142 - val_loss: 0.2955\n",
      "Epoch 419/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3322 - val_loss: 0.2952\n",
      "Epoch 420/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3322 - val_loss: 0.2955\n",
      "Epoch 421/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3410 - val_loss: 0.2959\n",
      "Epoch 422/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3295 - val_loss: 0.2959\n",
      "Epoch 423/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3254 - val_loss: 0.2952\n",
      "Epoch 424/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3278 - val_loss: 0.2943\n",
      "Epoch 425/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3238 - val_loss: 0.2930\n",
      "Epoch 426/1000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.3187 - val_loss: 0.2919\n",
      "Epoch 427/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3273 - val_loss: 0.2909\n",
      "Epoch 428/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3137 - val_loss: 0.2904\n",
      "Epoch 429/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3157 - val_loss: 0.2907\n",
      "Epoch 430/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3100 - val_loss: 0.2908\n",
      "Epoch 431/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3114 - val_loss: 0.2914\n",
      "Epoch 432/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3102 - val_loss: 0.2919\n",
      "Epoch 433/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3029 - val_loss: 0.2913\n",
      "Epoch 434/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3151 - val_loss: 0.2903\n",
      "Epoch 435/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3162 - val_loss: 0.2895\n",
      "Epoch 436/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3344 - val_loss: 0.2887\n",
      "Epoch 437/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3316 - val_loss: 0.2885\n",
      "Epoch 438/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3188 - val_loss: 0.2882\n",
      "Epoch 439/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3153 - val_loss: 0.2882\n",
      "Epoch 440/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3352 - val_loss: 0.2874\n",
      "Epoch 441/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3235 - val_loss: 0.2866\n",
      "Epoch 442/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3137 - val_loss: 0.2857\n",
      "Epoch 443/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3225 - val_loss: 0.2846\n",
      "Epoch 444/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3149 - val_loss: 0.2836\n",
      "Epoch 445/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3122 - val_loss: 0.2835\n",
      "Epoch 446/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3049 - val_loss: 0.2830\n",
      "Epoch 447/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3231 - val_loss: 0.2830\n",
      "Epoch 448/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3245 - val_loss: 0.2832\n",
      "Epoch 449/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3447 - val_loss: 0.2836\n",
      "Epoch 450/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3184 - val_loss: 0.2844\n",
      "Epoch 451/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3182 - val_loss: 0.2844\n",
      "Epoch 452/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2978 - val_loss: 0.2838\n",
      "Epoch 453/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3244 - val_loss: 0.2830\n",
      "Epoch 454/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3150 - val_loss: 0.2816\n",
      "Epoch 455/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3086 - val_loss: 0.2807\n",
      "Epoch 456/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3104 - val_loss: 0.2804\n",
      "Epoch 457/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3145 - val_loss: 0.2795\n",
      "Epoch 458/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3136 - val_loss: 0.2795\n",
      "Epoch 459/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3145 - val_loss: 0.2798\n",
      "Epoch 460/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3344 - val_loss: 0.2794\n",
      "Epoch 461/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3109 - val_loss: 0.2797\n",
      "Epoch 462/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3143 - val_loss: 0.2800\n",
      "Epoch 463/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3176 - val_loss: 0.2797\n",
      "Epoch 464/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3200 - val_loss: 0.2792\n",
      "Epoch 465/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3127 - val_loss: 0.2786\n",
      "Epoch 466/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3388 - val_loss: 0.2780\n",
      "Epoch 467/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3057 - val_loss: 0.2773\n",
      "Epoch 468/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3183 - val_loss: 0.2773\n",
      "Epoch 469/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3088 - val_loss: 0.2779\n",
      "Epoch 470/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3091 - val_loss: 0.2778\n",
      "Epoch 471/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3091 - val_loss: 0.2771\n",
      "Epoch 472/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3173 - val_loss: 0.2761\n",
      "Epoch 473/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3169 - val_loss: 0.2754\n",
      "Epoch 474/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3224 - val_loss: 0.2749\n",
      "Epoch 475/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3115 - val_loss: 0.2754\n",
      "Epoch 476/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3011 - val_loss: 0.2752\n",
      "Epoch 477/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3171 - val_loss: 0.2749\n",
      "Epoch 478/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3217 - val_loss: 0.2748\n",
      "Epoch 479/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2925 - val_loss: 0.2742\n",
      "Epoch 480/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3312 - val_loss: 0.2740\n",
      "Epoch 481/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3228 - val_loss: 0.2737\n",
      "Epoch 482/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3026 - val_loss: 0.2726\n",
      "Epoch 483/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3232 - val_loss: 0.2719\n",
      "Epoch 484/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3002 - val_loss: 0.2711\n",
      "Epoch 485/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3101 - val_loss: 0.2704\n",
      "Epoch 486/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3181 - val_loss: 0.2700\n",
      "Epoch 487/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3105 - val_loss: 0.2701\n",
      "Epoch 488/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3262 - val_loss: 0.2703\n",
      "Epoch 489/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3096 - val_loss: 0.2704\n",
      "Epoch 490/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3087 - val_loss: 0.2698\n",
      "Epoch 491/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2938 - val_loss: 0.2694\n",
      "Epoch 492/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3090 - val_loss: 0.2688\n",
      "Epoch 493/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3085 - val_loss: 0.2688\n",
      "Epoch 494/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3152 - val_loss: 0.2685\n",
      "Epoch 495/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3068 - val_loss: 0.2680\n",
      "Epoch 496/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3053 - val_loss: 0.2678\n",
      "Epoch 497/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3130 - val_loss: 0.2677\n",
      "Epoch 498/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3117 - val_loss: 0.2673\n",
      "Epoch 499/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3002 - val_loss: 0.2672\n",
      "Epoch 500/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3080 - val_loss: 0.2667\n",
      "Epoch 501/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2862 - val_loss: 0.2656\n",
      "Epoch 502/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3127 - val_loss: 0.2654\n",
      "Epoch 503/1000\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.2936 - val_loss: 0.2650\n",
      "Epoch 504/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.2919 - val_loss: 0.2647\n",
      "Epoch 505/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3023 - val_loss: 0.2636\n",
      "Epoch 506/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3235 - val_loss: 0.2629\n",
      "Epoch 507/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2871 - val_loss: 0.2621\n",
      "Epoch 508/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3141 - val_loss: 0.2618\n",
      "Epoch 509/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3212 - val_loss: 0.2622\n",
      "Epoch 510/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2998 - val_loss: 0.2626\n",
      "Epoch 511/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2894 - val_loss: 0.2624\n",
      "Epoch 512/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3044 - val_loss: 0.2619\n",
      "Epoch 513/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3088 - val_loss: 0.2617\n",
      "Epoch 514/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3015 - val_loss: 0.2612\n",
      "Epoch 515/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2995 - val_loss: 0.2607\n",
      "Epoch 516/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3090 - val_loss: 0.2603\n",
      "Epoch 517/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3023 - val_loss: 0.2603\n",
      "Epoch 518/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2927 - val_loss: 0.2596\n",
      "Epoch 519/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.2868 - val_loss: 0.2592\n",
      "Epoch 520/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.2979 - val_loss: 0.2590\n",
      "Epoch 521/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2995 - val_loss: 0.2591\n",
      "Epoch 522/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2975 - val_loss: 0.2588\n",
      "Epoch 523/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3220 - val_loss: 0.2585\n",
      "Epoch 524/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2878 - val_loss: 0.2579\n",
      "Epoch 525/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2905 - val_loss: 0.2574\n",
      "Epoch 526/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3069 - val_loss: 0.2578\n",
      "Epoch 527/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2963 - val_loss: 0.2576\n",
      "Epoch 528/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3136 - val_loss: 0.2580\n",
      "Epoch 529/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3153 - val_loss: 0.2587\n",
      "Epoch 530/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3006 - val_loss: 0.2585\n",
      "Epoch 531/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3063 - val_loss: 0.2584\n",
      "Epoch 532/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3078 - val_loss: 0.2578\n",
      "Epoch 533/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2993 - val_loss: 0.2566\n",
      "Epoch 534/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2929 - val_loss: 0.2558\n",
      "Epoch 535/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3054 - val_loss: 0.2549\n",
      "Epoch 536/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2992 - val_loss: 0.2541\n",
      "Epoch 537/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2818 - val_loss: 0.2536\n",
      "Epoch 538/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2873 - val_loss: 0.2532\n",
      "Epoch 539/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2791 - val_loss: 0.2537\n",
      "Epoch 540/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2885 - val_loss: 0.2543\n",
      "Epoch 541/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2974 - val_loss: 0.2547\n",
      "Epoch 542/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2809 - val_loss: 0.2544\n",
      "Epoch 543/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2971 - val_loss: 0.2539\n",
      "Epoch 544/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2889 - val_loss: 0.2530\n",
      "Epoch 545/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2737 - val_loss: 0.2518\n",
      "Epoch 546/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3001 - val_loss: 0.2509\n",
      "Epoch 547/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2903 - val_loss: 0.2506\n",
      "Epoch 548/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2931 - val_loss: 0.2503\n",
      "Epoch 549/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2970 - val_loss: 0.2500\n",
      "Epoch 550/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2940 - val_loss: 0.2503\n",
      "Epoch 551/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2965 - val_loss: 0.2507\n",
      "Epoch 552/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2971 - val_loss: 0.2513\n",
      "Epoch 553/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2951 - val_loss: 0.2516\n",
      "Epoch 554/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2870 - val_loss: 0.2512\n",
      "Epoch 555/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2970 - val_loss: 0.2504\n",
      "Epoch 556/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3073 - val_loss: 0.2493\n",
      "Epoch 557/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2856 - val_loss: 0.2490\n",
      "Epoch 558/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2985 - val_loss: 0.2485\n",
      "Epoch 559/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2850 - val_loss: 0.2484\n",
      "Epoch 560/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2904 - val_loss: 0.2479\n",
      "Epoch 561/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2945 - val_loss: 0.2475\n",
      "Epoch 562/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3043 - val_loss: 0.2472\n",
      "Epoch 563/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2899 - val_loss: 0.2470\n",
      "Epoch 564/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2831 - val_loss: 0.2465\n",
      "Epoch 565/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2954 - val_loss: 0.2455\n",
      "Epoch 566/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2653 - val_loss: 0.2441\n",
      "Epoch 567/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2937 - val_loss: 0.2435\n",
      "Epoch 568/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3066 - val_loss: 0.2434\n",
      "Epoch 569/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2872 - val_loss: 0.2435\n",
      "Epoch 570/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2882 - val_loss: 0.2436\n",
      "Epoch 571/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2768 - val_loss: 0.2438\n",
      "Epoch 572/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3086 - val_loss: 0.2439\n",
      "Epoch 573/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2847 - val_loss: 0.2439\n",
      "Epoch 574/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2922 - val_loss: 0.2440\n",
      "Epoch 575/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2726 - val_loss: 0.2438\n",
      "Epoch 576/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2812 - val_loss: 0.2436\n",
      "Epoch 577/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2991 - val_loss: 0.2437\n",
      "Epoch 578/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2960 - val_loss: 0.2439\n",
      "Epoch 579/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2647 - val_loss: 0.2438\n",
      "Epoch 580/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2816 - val_loss: 0.2433\n",
      "Epoch 581/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2892 - val_loss: 0.2427\n",
      "Epoch 582/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2743 - val_loss: 0.2418\n",
      "Epoch 583/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2738 - val_loss: 0.2413\n",
      "Epoch 584/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2822 - val_loss: 0.2409\n",
      "Epoch 585/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2891 - val_loss: 0.2404\n",
      "Epoch 586/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2841 - val_loss: 0.2399\n",
      "Epoch 587/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2985 - val_loss: 0.2390\n",
      "Epoch 588/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3074 - val_loss: 0.2375\n",
      "Epoch 589/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2722 - val_loss: 0.2367\n",
      "Epoch 590/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2828 - val_loss: 0.2359\n",
      "Epoch 591/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.2599 - val_loss: 0.2359\n",
      "Epoch 592/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2882 - val_loss: 0.2369\n",
      "Epoch 593/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2705 - val_loss: 0.2379\n",
      "Epoch 594/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2851 - val_loss: 0.2383\n",
      "Epoch 595/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2756 - val_loss: 0.2382\n",
      "Epoch 596/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2599 - val_loss: 0.2372\n",
      "Epoch 597/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2785 - val_loss: 0.2367\n",
      "Epoch 598/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2858 - val_loss: 0.2362\n",
      "Epoch 599/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2808 - val_loss: 0.2357\n",
      "Epoch 600/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2872 - val_loss: 0.2355\n",
      "Epoch 601/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2945 - val_loss: 0.2356\n",
      "Epoch 602/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2731 - val_loss: 0.2356\n",
      "Epoch 603/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2833 - val_loss: 0.2353\n",
      "Epoch 604/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2701 - val_loss: 0.2348\n",
      "Epoch 605/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2992 - val_loss: 0.2349\n",
      "Epoch 606/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2694 - val_loss: 0.2349\n",
      "Epoch 607/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2839 - val_loss: 0.2351\n",
      "Epoch 608/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2821 - val_loss: 0.2349\n",
      "Epoch 609/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2588 - val_loss: 0.2343\n",
      "Epoch 610/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2787 - val_loss: 0.2336\n",
      "Epoch 611/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2790 - val_loss: 0.2335\n",
      "Epoch 612/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2835 - val_loss: 0.2338\n",
      "Epoch 613/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2679 - val_loss: 0.2336\n",
      "Epoch 614/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2635 - val_loss: 0.2327\n",
      "Epoch 615/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2700 - val_loss: 0.2320\n",
      "Epoch 616/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2962 - val_loss: 0.2313\n",
      "Epoch 617/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2887 - val_loss: 0.2312\n",
      "Epoch 618/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2788 - val_loss: 0.2304\n",
      "Epoch 619/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2873 - val_loss: 0.2299\n",
      "Epoch 620/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2831 - val_loss: 0.2301\n",
      "Epoch 621/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2842 - val_loss: 0.2302\n",
      "Epoch 622/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2958 - val_loss: 0.2304\n",
      "Epoch 623/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2956 - val_loss: 0.2299\n",
      "Epoch 624/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2632 - val_loss: 0.2294\n",
      "Epoch 625/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2674 - val_loss: 0.2291\n",
      "Epoch 626/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3004 - val_loss: 0.2290\n",
      "Epoch 627/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2705 - val_loss: 0.2294\n",
      "Epoch 628/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2742 - val_loss: 0.2294\n",
      "Epoch 629/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2773 - val_loss: 0.2292\n",
      "Epoch 630/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2622 - val_loss: 0.2288\n",
      "Epoch 631/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2798 - val_loss: 0.2283\n",
      "Epoch 632/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2440 - val_loss: 0.2277\n",
      "Epoch 633/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2779 - val_loss: 0.2271\n",
      "Epoch 634/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2790 - val_loss: 0.2265\n",
      "Epoch 635/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2763 - val_loss: 0.2262\n",
      "Epoch 636/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2816 - val_loss: 0.2261\n",
      "Epoch 637/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2536 - val_loss: 0.2258\n",
      "Epoch 638/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2841 - val_loss: 0.2263\n",
      "Epoch 639/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2808 - val_loss: 0.2264\n",
      "Epoch 640/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2705 - val_loss: 0.2264\n",
      "Epoch 641/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2721 - val_loss: 0.2262\n",
      "Epoch 642/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2762 - val_loss: 0.2255\n",
      "Epoch 643/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2778 - val_loss: 0.2248\n",
      "Epoch 644/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2758 - val_loss: 0.2242\n",
      "Epoch 645/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2741 - val_loss: 0.2237\n",
      "Epoch 646/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2686 - val_loss: 0.2236\n",
      "Epoch 647/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2880 - val_loss: 0.2237\n",
      "Epoch 648/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2673 - val_loss: 0.2237\n",
      "Epoch 649/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2580 - val_loss: 0.2234\n",
      "Epoch 650/1000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.2559 - val_loss: 0.2227\n",
      "Epoch 651/1000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.2791 - val_loss: 0.2219\n",
      "Epoch 652/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2706 - val_loss: 0.2212\n",
      "Epoch 653/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2757 - val_loss: 0.2215\n",
      "Epoch 654/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2722 - val_loss: 0.2219\n",
      "Epoch 655/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2724 - val_loss: 0.2217\n",
      "Epoch 656/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2922 - val_loss: 0.2214\n",
      "Epoch 657/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2669 - val_loss: 0.2214\n",
      "Epoch 658/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2742 - val_loss: 0.2218\n",
      "Epoch 659/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2779 - val_loss: 0.2224\n",
      "Epoch 660/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2717 - val_loss: 0.2223\n",
      "Epoch 661/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2768 - val_loss: 0.2221\n",
      "Epoch 662/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2884 - val_loss: 0.2219\n",
      "Epoch 663/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2772 - val_loss: 0.2215\n",
      "Epoch 664/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2643 - val_loss: 0.2210\n",
      "Epoch 665/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2667 - val_loss: 0.2204\n",
      "Epoch 666/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2606 - val_loss: 0.2197\n",
      "Epoch 667/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2710 - val_loss: 0.2197\n",
      "Epoch 668/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2655 - val_loss: 0.2194\n",
      "Epoch 669/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2542 - val_loss: 0.2189\n",
      "Epoch 670/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2773 - val_loss: 0.2183\n",
      "Epoch 671/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2567 - val_loss: 0.2174\n",
      "Epoch 672/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2693 - val_loss: 0.2172\n",
      "Epoch 673/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2551 - val_loss: 0.2166\n",
      "Epoch 674/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2660 - val_loss: 0.2168\n",
      "Epoch 675/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2622 - val_loss: 0.2171\n",
      "Epoch 676/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2688 - val_loss: 0.2171\n",
      "Epoch 677/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2537 - val_loss: 0.2168\n",
      "Epoch 678/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2715 - val_loss: 0.2165\n",
      "Epoch 679/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2634 - val_loss: 0.2168\n",
      "Epoch 680/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2514 - val_loss: 0.2168\n",
      "Epoch 681/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2812 - val_loss: 0.2167\n",
      "Epoch 682/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2779 - val_loss: 0.2165\n",
      "Epoch 683/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2600 - val_loss: 0.2163\n",
      "Epoch 684/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2748 - val_loss: 0.2162\n",
      "Epoch 685/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2613 - val_loss: 0.2157\n",
      "Epoch 686/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2829 - val_loss: 0.2154\n",
      "Epoch 687/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2635 - val_loss: 0.2149\n",
      "Epoch 688/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2764 - val_loss: 0.2153\n",
      "Epoch 689/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2586 - val_loss: 0.2153\n",
      "Epoch 690/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2464 - val_loss: 0.2150\n",
      "Epoch 691/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2633 - val_loss: 0.2147\n",
      "Epoch 692/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2812 - val_loss: 0.2146\n",
      "Epoch 693/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2666 - val_loss: 0.2143\n",
      "Epoch 694/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2577 - val_loss: 0.2139\n",
      "Epoch 695/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2565 - val_loss: 0.2135\n",
      "Epoch 696/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2568 - val_loss: 0.2130\n",
      "Epoch 697/1000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.2743 - val_loss: 0.2125\n",
      "Epoch 698/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2642 - val_loss: 0.2118\n",
      "Epoch 699/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2593 - val_loss: 0.2114\n",
      "Epoch 700/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2697 - val_loss: 0.2115\n",
      "Epoch 701/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2683 - val_loss: 0.2114\n",
      "Epoch 702/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2720 - val_loss: 0.2113\n",
      "Epoch 703/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2688 - val_loss: 0.2112\n",
      "Epoch 704/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2523 - val_loss: 0.2112\n",
      "Epoch 705/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2659 - val_loss: 0.2111\n",
      "Epoch 706/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2738 - val_loss: 0.2107\n",
      "Epoch 707/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2777 - val_loss: 0.2102\n",
      "Epoch 708/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2617 - val_loss: 0.2096\n",
      "Epoch 709/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2673 - val_loss: 0.2087\n",
      "Epoch 710/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2622 - val_loss: 0.2092\n",
      "Epoch 711/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2660 - val_loss: 0.2099\n",
      "Epoch 712/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2720 - val_loss: 0.2098\n",
      "Epoch 713/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2781 - val_loss: 0.2096\n",
      "Epoch 714/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2685 - val_loss: 0.2094\n",
      "Epoch 715/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2595 - val_loss: 0.2091\n",
      "Epoch 716/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2619 - val_loss: 0.2085\n",
      "Epoch 717/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2637 - val_loss: 0.2080\n",
      "Epoch 718/1000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.2663 - val_loss: 0.2073\n",
      "Epoch 719/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2487 - val_loss: 0.2070\n",
      "Epoch 720/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2750 - val_loss: 0.2069\n",
      "Epoch 721/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2565 - val_loss: 0.2070\n",
      "Epoch 722/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2541 - val_loss: 0.2072\n",
      "Epoch 723/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2787 - val_loss: 0.2073\n",
      "Epoch 724/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2516 - val_loss: 0.2073\n",
      "Epoch 725/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2668 - val_loss: 0.2072\n",
      "Epoch 726/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2606 - val_loss: 0.2068\n",
      "Epoch 727/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2634 - val_loss: 0.2063\n",
      "Epoch 728/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2658 - val_loss: 0.2055\n",
      "Epoch 729/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2575 - val_loss: 0.2051\n",
      "Epoch 730/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2452 - val_loss: 0.2051\n",
      "Epoch 731/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2660 - val_loss: 0.2048\n",
      "Epoch 732/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2864 - val_loss: 0.2053\n",
      "Epoch 733/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2547 - val_loss: 0.2059\n",
      "Epoch 734/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2315 - val_loss: 0.2059\n",
      "Epoch 735/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2723 - val_loss: 0.2057\n",
      "Epoch 736/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2422 - val_loss: 0.2054\n",
      "Epoch 737/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2671 - val_loss: 0.2049\n",
      "Epoch 738/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2557 - val_loss: 0.2042\n",
      "Epoch 739/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2694 - val_loss: 0.2036\n",
      "Epoch 740/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2370 - val_loss: 0.2035\n",
      "Epoch 741/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2542 - val_loss: 0.2038\n",
      "Epoch 742/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2671 - val_loss: 0.2036\n",
      "Epoch 743/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2428 - val_loss: 0.2037\n",
      "Epoch 744/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2571 - val_loss: 0.2037\n",
      "Epoch 745/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2576 - val_loss: 0.2034\n",
      "Epoch 746/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2738 - val_loss: 0.2033\n",
      "Epoch 747/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2418 - val_loss: 0.2030\n",
      "Epoch 748/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2409 - val_loss: 0.2025\n",
      "Epoch 749/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2465 - val_loss: 0.2020\n",
      "Epoch 750/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2537 - val_loss: 0.2021\n",
      "Epoch 751/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2563 - val_loss: 0.2017\n",
      "Epoch 752/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2559 - val_loss: 0.2013\n",
      "Epoch 753/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2515 - val_loss: 0.2011\n",
      "Epoch 754/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2382 - val_loss: 0.2006\n",
      "Epoch 755/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2592 - val_loss: 0.2005\n",
      "Epoch 756/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2465 - val_loss: 0.2003\n",
      "Epoch 757/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2604 - val_loss: 0.2007\n",
      "Epoch 758/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2571 - val_loss: 0.2010\n",
      "Epoch 759/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2728 - val_loss: 0.2012\n",
      "Epoch 760/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2603 - val_loss: 0.2010\n",
      "Epoch 761/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2580 - val_loss: 0.2010\n",
      "Epoch 762/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2643 - val_loss: 0.2009\n",
      "Epoch 763/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2686 - val_loss: 0.2012\n",
      "Epoch 764/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2502 - val_loss: 0.2015\n",
      "Epoch 765/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2546 - val_loss: 0.2013\n",
      "Epoch 766/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2662 - val_loss: 0.2007\n",
      "Epoch 767/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2650 - val_loss: 0.2000\n",
      "Epoch 768/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2521 - val_loss: 0.2000\n",
      "Epoch 769/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2296 - val_loss: 0.1998\n",
      "Epoch 770/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2436 - val_loss: 0.1994\n",
      "Epoch 771/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2643 - val_loss: 0.1993\n",
      "Epoch 772/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2469 - val_loss: 0.1993\n",
      "Epoch 773/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2616 - val_loss: 0.1999\n",
      "Epoch 774/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2631 - val_loss: 0.1995\n",
      "Epoch 775/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2459 - val_loss: 0.1990\n",
      "Epoch 776/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2374 - val_loss: 0.1986\n",
      "Epoch 777/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2337 - val_loss: 0.1983\n",
      "Epoch 778/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2551 - val_loss: 0.1977\n",
      "Epoch 779/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2592 - val_loss: 0.1975\n",
      "Epoch 780/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2342 - val_loss: 0.1968\n",
      "Epoch 781/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2434 - val_loss: 0.1962\n",
      "Epoch 782/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2459 - val_loss: 0.1965\n",
      "Epoch 783/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2500 - val_loss: 0.1964\n",
      "Epoch 784/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2472 - val_loss: 0.1962\n",
      "Epoch 785/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2527 - val_loss: 0.1963\n",
      "Epoch 786/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2451 - val_loss: 0.1961\n",
      "Epoch 787/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2418 - val_loss: 0.1961\n",
      "Epoch 788/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2497 - val_loss: 0.1957\n",
      "Epoch 789/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2622 - val_loss: 0.1953\n",
      "Epoch 790/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2396 - val_loss: 0.1951\n",
      "Epoch 791/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2421 - val_loss: 0.1948\n",
      "Epoch 792/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2351 - val_loss: 0.1949\n",
      "Epoch 793/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2540 - val_loss: 0.1949\n",
      "Epoch 794/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2579 - val_loss: 0.1948\n",
      "Epoch 795/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2422 - val_loss: 0.1949\n",
      "Epoch 796/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2454 - val_loss: 0.1953\n",
      "Epoch 797/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2441 - val_loss: 0.1956\n",
      "Epoch 798/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2521 - val_loss: 0.1953\n",
      "Epoch 799/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2357 - val_loss: 0.1952\n",
      "Epoch 800/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2426 - val_loss: 0.1947\n",
      "Epoch 801/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2542 - val_loss: 0.1943\n",
      "Epoch 802/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2392 - val_loss: 0.1939\n",
      "Epoch 803/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2424 - val_loss: 0.1931\n",
      "Epoch 804/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2717 - val_loss: 0.1926\n",
      "Epoch 805/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2374 - val_loss: 0.1925\n",
      "Epoch 806/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2496 - val_loss: 0.1922\n",
      "Epoch 807/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2406 - val_loss: 0.1924\n",
      "Epoch 808/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2545 - val_loss: 0.1926\n",
      "Epoch 809/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2535 - val_loss: 0.1925\n",
      "Epoch 810/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2656 - val_loss: 0.1922\n",
      "Epoch 811/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2264 - val_loss: 0.1920\n",
      "Epoch 812/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2278 - val_loss: 0.1920\n",
      "Epoch 813/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2174 - val_loss: 0.1916\n",
      "Epoch 814/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2481 - val_loss: 0.1905\n",
      "Epoch 815/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2218 - val_loss: 0.1895\n",
      "Epoch 816/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2481 - val_loss: 0.1892\n",
      "Epoch 817/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2568 - val_loss: 0.1887\n",
      "Epoch 818/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2419 - val_loss: 0.1883\n",
      "Epoch 819/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2236 - val_loss: 0.1881\n",
      "Epoch 820/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2619 - val_loss: 0.1885\n",
      "Epoch 821/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2373 - val_loss: 0.1891\n",
      "Epoch 822/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2492 - val_loss: 0.1895\n",
      "Epoch 823/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2194 - val_loss: 0.1900\n",
      "Epoch 824/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2567 - val_loss: 0.1899\n",
      "Epoch 825/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2341 - val_loss: 0.1895\n",
      "Epoch 826/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2462 - val_loss: 0.1892\n",
      "Epoch 827/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2459 - val_loss: 0.1889\n",
      "Epoch 828/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2218 - val_loss: 0.1884\n",
      "Epoch 829/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2216 - val_loss: 0.1878\n",
      "Epoch 830/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2499 - val_loss: 0.1871\n",
      "Epoch 831/1000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2308 - val_loss: 0.1868\n",
      "Epoch 832/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2443 - val_loss: 0.1870\n",
      "Epoch 833/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2348 - val_loss: 0.1870\n",
      "Epoch 834/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2454 - val_loss: 0.1869\n",
      "Epoch 835/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2367 - val_loss: 0.1868\n",
      "Epoch 836/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2451 - val_loss: 0.1867\n",
      "Epoch 837/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2403 - val_loss: 0.1863\n",
      "Epoch 838/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2373 - val_loss: 0.1862\n",
      "Epoch 839/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2502 - val_loss: 0.1862\n",
      "Epoch 840/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2456 - val_loss: 0.1857\n",
      "Epoch 841/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2318 - val_loss: 0.1850\n",
      "Epoch 842/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2575 - val_loss: 0.1842\n",
      "Epoch 843/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2460 - val_loss: 0.1837\n",
      "Epoch 844/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2311 - val_loss: 0.1838\n",
      "Epoch 845/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2396 - val_loss: 0.1843\n",
      "Epoch 846/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2344 - val_loss: 0.1845\n",
      "Epoch 847/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2336 - val_loss: 0.1851\n",
      "Epoch 848/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2300 - val_loss: 0.1847\n",
      "Epoch 849/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2599 - val_loss: 0.1844\n",
      "Epoch 850/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2570 - val_loss: 0.1839\n",
      "Epoch 851/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2500 - val_loss: 0.1833\n",
      "Epoch 852/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2212 - val_loss: 0.1832\n",
      "Epoch 853/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2487 - val_loss: 0.1831\n",
      "Epoch 854/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2316 - val_loss: 0.1828\n",
      "Epoch 855/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2532 - val_loss: 0.1828\n",
      "Epoch 856/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2411 - val_loss: 0.1828\n",
      "Epoch 857/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2287 - val_loss: 0.1827\n",
      "Epoch 858/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2290 - val_loss: 0.1828\n",
      "Epoch 859/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2591 - val_loss: 0.1825\n",
      "Epoch 860/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2358 - val_loss: 0.1824\n",
      "Epoch 861/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2336 - val_loss: 0.1820\n",
      "Epoch 862/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2458 - val_loss: 0.1813\n",
      "Epoch 863/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2752 - val_loss: 0.1807\n",
      "Epoch 864/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2271 - val_loss: 0.1805\n",
      "Epoch 865/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2580 - val_loss: 0.1814\n",
      "Epoch 866/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2174 - val_loss: 0.1821\n",
      "Epoch 867/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2348 - val_loss: 0.1820\n",
      "Epoch 868/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2449 - val_loss: 0.1817\n",
      "Epoch 869/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2284 - val_loss: 0.1812\n",
      "Epoch 870/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2390 - val_loss: 0.1803\n",
      "Epoch 871/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2286 - val_loss: 0.1801\n",
      "Epoch 872/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2334 - val_loss: 0.1799\n",
      "Epoch 873/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2441 - val_loss: 0.1796\n",
      "Epoch 874/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2304 - val_loss: 0.1802\n",
      "Epoch 875/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2318 - val_loss: 0.1802\n",
      "Epoch 876/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2545 - val_loss: 0.1809\n",
      "Epoch 877/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2119 - val_loss: 0.1808\n",
      "Epoch 878/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2357 - val_loss: 0.1801\n",
      "Epoch 879/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2165 - val_loss: 0.1797\n",
      "Epoch 880/1000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2157 - val_loss: 0.1791\n",
      "Epoch 881/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2516 - val_loss: 0.1786\n",
      "Epoch 882/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2230 - val_loss: 0.1781\n",
      "Epoch 883/1000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2268 - val_loss: 0.1777\n",
      "Epoch 884/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2342 - val_loss: 0.1774\n",
      "Epoch 885/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2315 - val_loss: 0.1771\n",
      "Epoch 886/1000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2251 - val_loss: 0.1771\n",
      "Epoch 887/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2278 - val_loss: 0.1770\n",
      "Epoch 888/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2363 - val_loss: 0.1771\n",
      "Epoch 889/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2269 - val_loss: 0.1771\n",
      "Epoch 890/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2559 - val_loss: 0.1772\n",
      "Epoch 891/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2194 - val_loss: 0.1772\n",
      "Epoch 892/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2286 - val_loss: 0.1771\n",
      "Epoch 893/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2140 - val_loss: 0.1768\n",
      "Epoch 894/1000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2207 - val_loss: 0.1761\n",
      "Epoch 895/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2415 - val_loss: 0.1755\n",
      "Epoch 896/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2445 - val_loss: 0.1752\n",
      "Epoch 897/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2333 - val_loss: 0.1755\n",
      "Epoch 898/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2488 - val_loss: 0.1759\n",
      "Epoch 899/1000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2146 - val_loss: 0.1760\n",
      "Epoch 900/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.1972 - val_loss: 0.1757\n",
      "Epoch 901/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2506 - val_loss: 0.1754\n",
      "Epoch 902/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2004 - val_loss: 0.1751\n",
      "Epoch 903/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2410 - val_loss: 0.1749\n",
      "Epoch 904/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2095 - val_loss: 0.1747\n",
      "Epoch 905/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2140 - val_loss: 0.1744\n",
      "Epoch 906/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2334 - val_loss: 0.1745\n",
      "Epoch 907/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2231 - val_loss: 0.1749\n",
      "Epoch 908/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2111 - val_loss: 0.1751\n",
      "Epoch 909/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2176 - val_loss: 0.1747\n",
      "Epoch 910/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2438 - val_loss: 0.1745\n",
      "Epoch 911/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2177 - val_loss: 0.1737\n",
      "Epoch 912/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2262 - val_loss: 0.1733\n",
      "Epoch 913/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2497 - val_loss: 0.1729\n",
      "Epoch 914/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2314 - val_loss: 0.1732\n",
      "Epoch 915/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2326 - val_loss: 0.1735\n",
      "Epoch 916/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2251 - val_loss: 0.1737\n",
      "Epoch 917/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2250 - val_loss: 0.1739\n",
      "Epoch 918/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2429 - val_loss: 0.1735\n",
      "Epoch 919/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2250 - val_loss: 0.1738\n",
      "Epoch 920/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2310 - val_loss: 0.1737\n",
      "Epoch 921/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2157 - val_loss: 0.1735\n",
      "Epoch 922/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2223 - val_loss: 0.1730\n",
      "Epoch 923/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2181 - val_loss: 0.1725\n",
      "Epoch 924/1000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2237 - val_loss: 0.1723\n",
      "Epoch 925/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2214 - val_loss: 0.1724\n",
      "Epoch 926/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2586 - val_loss: 0.1722\n",
      "Epoch 927/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2044 - val_loss: 0.1717\n",
      "Epoch 928/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.1954 - val_loss: 0.1711\n",
      "Epoch 929/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2266 - val_loss: 0.1710\n",
      "Epoch 930/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2170 - val_loss: 0.1712\n",
      "Epoch 931/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2174 - val_loss: 0.1717\n",
      "Epoch 932/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2261 - val_loss: 0.1718\n",
      "Epoch 933/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2393 - val_loss: 0.1722\n",
      "Epoch 934/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2427 - val_loss: 0.1724\n",
      "Epoch 935/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.1848 - val_loss: 0.1720\n",
      "Epoch 936/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2063 - val_loss: 0.1715\n",
      "Epoch 937/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2369 - val_loss: 0.1712\n",
      "Epoch 938/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2461 - val_loss: 0.1708\n",
      "Epoch 939/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2346 - val_loss: 0.1708\n",
      "Epoch 940/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2178 - val_loss: 0.1704\n",
      "Epoch 941/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2305 - val_loss: 0.1702\n",
      "Epoch 942/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2450 - val_loss: 0.1703\n",
      "Epoch 943/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2192 - val_loss: 0.1700\n",
      "Epoch 944/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2332 - val_loss: 0.1696\n",
      "Epoch 945/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2134 - val_loss: 0.1694\n",
      "Epoch 946/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2077 - val_loss: 0.1696\n",
      "Epoch 947/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2104 - val_loss: 0.1693\n",
      "Epoch 948/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2028 - val_loss: 0.1692\n",
      "Epoch 949/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2025 - val_loss: 0.1690\n",
      "Epoch 950/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2371 - val_loss: 0.1687\n",
      "Epoch 951/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2110 - val_loss: 0.1687\n",
      "Epoch 952/1000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2006 - val_loss: 0.1685\n",
      "Epoch 953/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2286 - val_loss: 0.1683\n",
      "Epoch 954/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2185 - val_loss: 0.1679\n",
      "Epoch 955/1000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2335 - val_loss: 0.1681\n",
      "Epoch 956/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2039 - val_loss: 0.1681\n",
      "Epoch 957/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2228 - val_loss: 0.1680\n",
      "Epoch 958/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2015 - val_loss: 0.1679\n",
      "Epoch 959/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2248 - val_loss: 0.1676\n",
      "Epoch 960/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2060 - val_loss: 0.1675\n",
      "Epoch 961/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2500 - val_loss: 0.1674\n",
      "Epoch 962/1000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2259 - val_loss: 0.1674\n",
      "Epoch 963/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2132 - val_loss: 0.1677\n",
      "Epoch 964/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2141 - val_loss: 0.1678\n",
      "Epoch 965/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2238 - val_loss: 0.1686\n",
      "Epoch 966/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2316 - val_loss: 0.1695\n",
      "Epoch 967/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2009 - val_loss: 0.1698\n",
      "Epoch 968/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2137 - val_loss: 0.1698\n",
      "Epoch 969/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2204 - val_loss: 0.1701\n",
      "Epoch 970/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2128 - val_loss: 0.1704\n",
      "Epoch 971/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2315 - val_loss: 0.1705\n",
      "Epoch 972/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2051 - val_loss: 0.1708\n",
      "Epoch 973/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2059 - val_loss: 0.1712\n",
      "Epoch 974/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2028 - val_loss: 0.1719\n",
      "Epoch 975/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2054 - val_loss: 0.1723\n",
      "Epoch 976/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2374 - val_loss: 0.1732\n",
      "Epoch 977/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2165 - val_loss: 0.1736\n",
      "Epoch 978/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2384 - val_loss: 0.1746\n",
      "Epoch 979/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1910 - val_loss: 0.1751\n",
      "Epoch 980/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2121 - val_loss: 0.1742\n",
      "Epoch 981/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2284 - val_loss: 0.1737\n",
      "Epoch 982/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2189 - val_loss: 0.1729\n",
      "Epoch 983/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1950 - val_loss: 0.1724\n",
      "Epoch 984/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2208 - val_loss: 0.1723\n",
      "Epoch 985/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2138 - val_loss: 0.1714\n",
      "Epoch 986/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2152 - val_loss: 0.1716\n",
      "Epoch 987/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2009 - val_loss: 0.1717\n",
      "Epoch 988/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2176 - val_loss: 0.1717\n",
      "Epoch 989/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2074 - val_loss: 0.1716\n",
      "Epoch 990/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.1969 - val_loss: 0.1717\n",
      "Epoch 991/1000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2194 - val_loss: 0.1715\n",
      "Epoch 992/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1909 - val_loss: 0.1710\n",
      "Epoch 993/1000\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1819 - val_loss: 0.1701\n",
      "Epoch 994/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2156 - val_loss: 0.1695\n",
      "Epoch 995/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.1917 - val_loss: 0.1689\n",
      "Epoch 996/1000\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1794 - val_loss: 0.1684\n",
      "Epoch 997/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.1905 - val_loss: 0.1679\n",
      "Epoch 998/1000\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2157 - val_loss: 0.1675\n",
      "Epoch 999/1000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2156 - val_loss: 0.1675\n",
      "Epoch 1000/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2045 - val_loss: 0.1680\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_new_auth_model((200,6)) # new_\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
    "\n",
    "\n",
    "\n",
    "kFold = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "val_map = next(kFold.split(shuffled_data_train, shuffled_labels_train))[1]\n",
    "\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', start_from_epoch=50,\n",
    "                                                           patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(shuffled_data_train[~val_map], shuffled_labels_train[~val_map], epochs=1000,\n",
    "                    batch_size=128, verbose=1,\n",
    "                    validation_data=(shuffled_data_train[val_map], shuffled_labels_train[val_map]),\n",
    "                   class_weight={0:0.5, 1:7.5},\n",
    "                   callbacks=[early_stopping_callback])\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "867ea14a-75bd-4998-b556-9969d573a227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:43:43.691115Z",
     "iopub.status.busy": "2023-02-25T11:43:43.690859Z",
     "iopub.status.idle": "2023-02-25T11:43:44.477082Z",
     "shell.execute_reply": "2023-02-25T11:43:44.476026Z",
     "shell.execute_reply.started": "2023-02-25T11:43:43.691094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 6ms/step\n",
      "[[0.09207973]\n",
      " [0.5232819 ]\n",
      " [0.08365646]\n",
      " [0.08683106]\n",
      " [0.12108651]\n",
      " [0.07708427]\n",
      " [0.07476983]\n",
      " [0.7291325 ]\n",
      " [0.0855781 ]\n",
      " [0.08702652]\n",
      " [0.07304312]\n",
      " [0.08535288]\n",
      " [0.08667742]\n",
      " [0.11896305]\n",
      " [0.08573274]\n",
      " [0.15506221]\n",
      " [0.08270247]\n",
      " [0.09155235]\n",
      " [0.07711575]\n",
      " [0.07357205]\n",
      " [0.09524757]\n",
      " [0.07166538]\n",
      " [0.07558786]\n",
      " [0.08878329]\n",
      " [0.08314909]\n",
      " [0.08850615]\n",
      " [0.08921529]\n",
      " [0.07728183]\n",
      " [0.07922476]\n",
      " [0.07512298]\n",
      " [0.07940032]\n",
      " [0.08559871]\n",
      " [0.08524824]\n",
      " [0.1133465 ]\n",
      " [0.1357782 ]\n",
      " [0.08941016]\n",
      " [0.15860921]\n",
      " [0.1396105 ]\n",
      " [0.0761445 ]\n",
      " [0.07421026]\n",
      " [0.10257125]\n",
      " [0.07929078]\n",
      " [0.4610611 ]\n",
      " [0.07235672]\n",
      " [0.07514978]\n",
      " [0.08391813]\n",
      " [0.07514004]\n",
      " [0.09033588]\n",
      " [0.07987241]\n",
      " [0.11202065]\n",
      " [0.23636572]\n",
      " [0.07345033]\n",
      " [0.12501204]\n",
      " [0.08178551]\n",
      " [0.08477883]\n",
      " [0.08429185]\n",
      " [0.08418581]\n",
      " [0.07809337]\n",
      " [0.07477432]\n",
      " [0.3509178 ]\n",
      " [0.39172423]\n",
      " [0.30344263]\n",
      " [0.11504294]\n",
      " [0.08689372]\n",
      " [0.08249546]\n",
      " [0.07520381]\n",
      " [0.08604702]\n",
      " [0.08681551]\n",
      " [0.07805096]\n",
      " [0.17212482]\n",
      " [0.344696  ]\n",
      " [0.5922229 ]\n",
      " [0.07164546]\n",
      " [0.08715085]\n",
      " [0.1832402 ]\n",
      " [0.11353639]\n",
      " [0.09117658]\n",
      " [0.07034514]\n",
      " [0.08108944]\n",
      " [0.07234188]\n",
      " [0.07255397]\n",
      " [0.08496115]\n",
      " [0.08473953]\n",
      " [0.16228181]\n",
      " [0.07451769]\n",
      " [0.07672802]\n",
      " [0.10222346]\n",
      " [0.07179036]\n",
      " [0.07944874]\n",
      " [0.0743145 ]\n",
      " [0.13610607]\n",
      " [0.34629887]\n",
      " [0.10055979]\n",
      " [0.0968606 ]\n",
      " [0.09384285]\n",
      " [0.11221087]\n",
      " [0.07038088]\n",
      " [0.13391249]\n",
      " [0.08557207]\n",
      " [0.08072499]\n",
      " [0.13551855]\n",
      " [0.07352447]\n",
      " [0.07687794]\n",
      " [0.16900046]\n",
      " [0.15242541]\n",
      " [0.07479952]\n",
      " [0.08798613]\n",
      " [0.10589299]\n",
      " [0.08134835]\n",
      " [0.5958011 ]\n",
      " [0.1612175 ]\n",
      " [0.1297153 ]\n",
      " [0.07794053]\n",
      " [0.48953706]\n",
      " [0.14894874]\n",
      " [0.07918716]\n",
      " [0.20044142]\n",
      " [0.19467263]\n",
      " [0.07720019]\n",
      " [0.10730179]\n",
      " [0.18435913]\n",
      " [0.14823535]\n",
      " [0.08584499]\n",
      " [0.09011853]\n",
      " [0.21349975]\n",
      " [0.08133671]\n",
      " [0.12631777]\n",
      " [0.0856821 ]\n",
      " [0.41348428]\n",
      " [0.12411389]\n",
      " [0.07471348]\n",
      " [0.08069635]\n",
      " [0.6690489 ]\n",
      " [0.10645289]\n",
      " [0.07505737]\n",
      " [0.07499723]\n",
      " [0.28616652]\n",
      " [0.08110464]\n",
      " [0.09511647]\n",
      " [0.080212  ]\n",
      " [0.07686941]\n",
      " [0.08986478]\n",
      " [0.09348802]\n",
      " [0.08361933]\n",
      " [0.08099447]\n",
      " [0.10984697]\n",
      " [0.11805457]\n",
      " [0.11392513]\n",
      " [0.09816922]\n",
      " [0.10352913]\n",
      " [0.3081862 ]\n",
      " [0.14693424]\n",
      " [0.10769421]\n",
      " [0.4232001 ]\n",
      " [0.09681903]\n",
      " [0.14567846]\n",
      " [0.07796909]\n",
      " [0.2575849 ]\n",
      " [0.07429496]\n",
      " [0.07859679]\n",
      " [0.07847014]\n",
      " [0.10024089]\n",
      " [0.07907687]\n",
      " [0.1950649 ]\n",
      " [0.2710251 ]\n",
      " [0.07814134]\n",
      " [0.09042086]\n",
      " [0.07374663]\n",
      " [0.0774555 ]\n",
      " [0.14133875]\n",
      " [0.14740983]\n",
      " [0.0993302 ]\n",
      " [0.16011849]\n",
      " [0.08064295]\n",
      " [0.07388295]\n",
      " [0.19960894]\n",
      " [0.37549138]\n",
      " [0.07528584]\n",
      " [0.44110617]\n",
      " [0.09203377]\n",
      " [0.09234585]\n",
      " [0.10653395]\n",
      " [0.07551309]\n",
      " [0.08716916]\n",
      " [0.0963508 ]\n",
      " [0.07371559]\n",
      " [0.09339268]\n",
      " [0.11275903]\n",
      " [0.0910804 ]\n",
      " [0.10650144]\n",
      " [0.07700587]\n",
      " [0.0785254 ]\n",
      " [0.11694243]\n",
      " [0.09251397]\n",
      " [0.07190911]\n",
      " [0.08015905]\n",
      " [0.07526171]\n",
      " [0.07653437]\n",
      " [0.0732829 ]\n",
      " [0.08741666]\n",
      " [0.07619733]\n",
      " [0.08350539]\n",
      " [0.09026392]\n",
      " [0.08433045]\n",
      " [0.09021573]\n",
      " [0.08310328]\n",
      " [0.07463793]\n",
      " [0.14448462]\n",
      " [0.12792437]\n",
      " [0.16372012]\n",
      " [0.20852128]\n",
      " [0.23983388]\n",
      " [0.14797923]\n",
      " [0.10171742]\n",
      " [0.09494436]\n",
      " [0.45098686]\n",
      " [0.15388145]\n",
      " [0.11931428]\n",
      " [0.10227766]\n",
      " [0.07520264]\n",
      " [0.08291188]\n",
      " [0.09760258]\n",
      " [0.10567513]\n",
      " [0.07698479]\n",
      " [0.07384067]\n",
      " [0.1304301 ]\n",
      " [0.09820243]\n",
      " [0.20317046]\n",
      " [0.14822137]\n",
      " [0.55166787]\n",
      " [0.07665413]\n",
      " [0.11377532]\n",
      " [0.08887569]\n",
      " [0.19348848]\n",
      " [0.12053128]\n",
      " [0.0775886 ]\n",
      " [0.31198797]\n",
      " [0.08742568]\n",
      " [0.5840129 ]\n",
      " [0.08785607]\n",
      " [0.44109496]\n",
      " [0.21288025]\n",
      " [0.07751065]\n",
      " [0.08650637]\n",
      " [0.09869067]\n",
      " [0.08698355]\n",
      " [0.07747369]\n",
      " [0.09072421]\n",
      " [0.09080274]\n",
      " [0.10533325]\n",
      " [0.09777325]\n",
      " [0.13452218]\n",
      " [0.10033821]\n",
      " [0.09788835]\n",
      " [0.16115882]\n",
      " [0.0721934 ]\n",
      " [0.08088003]\n",
      " [0.11553281]\n",
      " [0.09754068]\n",
      " [0.07612813]\n",
      " [0.07542203]\n",
      " [0.10791986]\n",
      " [0.07576466]\n",
      " [0.0780104 ]\n",
      " [0.07288328]\n",
      " [0.08263967]\n",
      " [0.09556622]\n",
      " [0.07052217]\n",
      " [0.09413403]\n",
      " [0.08132478]\n",
      " [0.10373516]\n",
      " [0.08854635]\n",
      " [0.07368946]\n",
      " [0.10305184]\n",
      " [0.07376274]\n",
      " [0.07190223]\n",
      " [0.09555612]\n",
      " [0.25131935]\n",
      " [0.4205964 ]\n",
      " [0.3287273 ]\n",
      " [0.12896298]\n",
      " [0.07006985]\n",
      " [0.10699169]\n",
      " [0.1839918 ]\n",
      " [0.13311748]\n",
      " [0.07792962]\n",
      " [0.166944  ]\n",
      " [0.09677534]\n",
      " [0.3834457 ]\n",
      " [0.07574134]\n",
      " [0.0745217 ]\n",
      " [0.15496927]\n",
      " [0.08059958]\n",
      " [0.11727294]\n",
      " [0.12263622]\n",
      " [0.11102705]\n",
      " [0.08631063]\n",
      " [0.7124919 ]\n",
      " [0.08160137]\n",
      " [0.08568744]\n",
      " [0.0757532 ]\n",
      " [0.3381739 ]\n",
      " [0.20724791]\n",
      " [0.12348394]\n",
      " [0.15552837]\n",
      " [0.10768463]\n",
      " [0.1246441 ]\n",
      " [0.15373285]\n",
      " [0.10387214]\n",
      " [0.10665259]\n",
      " [0.22291395]\n",
      " [0.08188718]\n",
      " [0.07419176]\n",
      " [0.07130661]\n",
      " [0.07840621]\n",
      " [0.14929633]\n",
      " [0.08555842]\n",
      " [0.09387702]\n",
      " [0.07216395]\n",
      " [0.37270993]\n",
      " [0.34109518]\n",
      " [0.07501327]\n",
      " [0.07561778]\n",
      " [0.4073811 ]\n",
      " [0.07384413]\n",
      " [0.12209699]\n",
      " [0.15673861]\n",
      " [0.42597383]\n",
      " [0.07624394]\n",
      " [0.1800784 ]\n",
      " [0.07268519]\n",
      " [0.10826935]\n",
      " [0.08848302]\n",
      " [0.11373305]\n",
      " [0.08918475]\n",
      " [0.08492935]\n",
      " [0.07438242]\n",
      " [0.07588096]\n",
      " [0.08678374]\n",
      " [0.08574032]\n",
      " [0.09864725]\n",
      " [0.10779513]\n",
      " [0.16463976]\n",
      " [0.07634655]\n",
      " [0.07453193]\n",
      " [0.07356654]\n",
      " [0.08499248]\n",
      " [0.08815997]\n",
      " [0.09220719]\n",
      " [0.08415427]\n",
      " [0.10994222]\n",
      " [0.08032597]\n",
      " [0.1227105 ]\n",
      " [0.08363809]\n",
      " [0.07857023]\n",
      " [0.16250907]\n",
      " [0.36482507]\n",
      " [0.07541691]\n",
      " [0.40029123]\n",
      " [0.10246532]\n",
      " [0.11059806]\n",
      " [0.07795729]\n",
      " [0.07810375]\n",
      " [0.16830167]\n",
      " [0.07344911]\n",
      " [0.08691221]\n",
      " [0.45680967]\n",
      " [0.13532849]\n",
      " [0.09749018]\n",
      " [0.07306837]\n",
      " [0.07724563]\n",
      " [0.111136  ]\n",
      " [0.09620111]\n",
      " [0.07820167]\n",
      " [0.10332131]\n",
      " [0.10438726]\n",
      " [0.7434301 ]\n",
      " [0.08958157]\n",
      " [0.08791763]\n",
      " [0.07215248]\n",
      " [0.29517928]\n",
      " [0.11059238]\n",
      " [0.09355158]\n",
      " [0.07279282]\n",
      " [0.44251055]\n",
      " [0.07223283]\n",
      " [0.08319735]\n",
      " [0.07955962]\n",
      " [0.07678938]\n",
      " [0.07813915]\n",
      " [0.06862852]\n",
      " [0.09074386]\n",
      " [0.08653563]\n",
      " [0.08509208]\n",
      " [0.09420488]\n",
      " [0.11231856]\n",
      " [0.6287103 ]\n",
      " [0.08043523]\n",
      " [0.07320075]\n",
      " [0.07414013]\n",
      " [0.08137463]\n",
      " [0.46286592]\n",
      " [0.07624876]\n",
      " [0.08786312]\n",
      " [0.1731409 ]\n",
      " [0.07724375]\n",
      " [0.08179391]\n",
      " [0.08759333]\n",
      " [0.15866525]\n",
      " [0.1357375 ]\n",
      " [0.07415912]\n",
      " [0.0941143 ]\n",
      " [0.19233795]\n",
      " [0.07788994]\n",
      " [0.1044759 ]\n",
      " [0.07289258]\n",
      " [0.0927064 ]\n",
      " [0.15757987]\n",
      " [0.0750315 ]\n",
      " [0.23742148]\n",
      " [0.08448324]\n",
      " [0.09521404]\n",
      " [0.23152515]\n",
      " [0.16053376]\n",
      " [0.10019007]\n",
      " [0.0695316 ]\n",
      " [0.09980888]\n",
      " [0.08674217]\n",
      " [0.08555215]\n",
      " [0.08603591]\n",
      " [0.07881226]\n",
      " [0.61612755]\n",
      " [0.08227155]\n",
      " [0.72858876]\n",
      " [0.08192122]\n",
      " [0.07638645]\n",
      " [0.0842761 ]\n",
      " [0.18506868]\n",
      " [0.07803702]\n",
      " [0.13898745]\n",
      " [0.07599147]\n",
      " [0.11932246]\n",
      " [0.0853123 ]\n",
      " [0.09503269]\n",
      " [0.12306872]\n",
      " [0.07822705]\n",
      " [0.08178773]\n",
      " [0.07619338]\n",
      " [0.44446734]\n",
      " [0.11261475]\n",
      " [0.4477611 ]\n",
      " [0.0792313 ]\n",
      " [0.07335018]\n",
      " [0.08409069]\n",
      " [0.36686134]\n",
      " [0.09189933]\n",
      " [0.08493198]\n",
      " [0.07319425]\n",
      " [0.11666504]\n",
      " [0.5213005 ]\n",
      " [0.07641022]\n",
      " [0.18864784]\n",
      " [0.07534065]\n",
      " [0.07791185]\n",
      " [0.07257775]\n",
      " [0.1525258 ]\n",
      " [0.08288406]\n",
      " [0.0806647 ]\n",
      " [0.0826598 ]\n",
      " [0.15773554]\n",
      " [0.5217251 ]\n",
      " [0.07768209]\n",
      " [0.13059872]\n",
      " [0.071767  ]\n",
      " [0.07442591]\n",
      " [0.07463314]\n",
      " [0.4056076 ]\n",
      " [0.36519507]\n",
      " [0.46016523]\n",
      " [0.6812371 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(shuffled_data_train[val_map]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a95d4335-2dd7-498e-8bee-794081cfeebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:43:44.479135Z",
     "iopub.status.busy": "2023-02-25T11:43:44.478746Z",
     "iopub.status.idle": "2023-02-25T11:43:45.477269Z",
     "shell.execute_reply": "2023-02-25T11:43:45.476047Z",
     "shell.execute_reply.started": "2023-02-25T11:43:44.479100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8007590132827325\n",
      "0.9331029502785229\n",
      "0.8715094339622642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9778801843317972"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_scores = model.predict(scaler.transform(x_data[_test_map])).squeeze()\n",
    "\n",
    "#labels_scores = model.predict(shuffled_data_train[~val_map]).squeeze()\n",
    "labels_pred = (labels_scores >= 0.5).astype(int)\n",
    "\n",
    "labels_true = (y_user.argmax(axis=1) == auth_user)[_test_map].astype(int)\n",
    "#labels_true = (shuffled_labels_train[~val_map]).astype(int)\n",
    "\n",
    "\n",
    "plt.hist(labels_scores, bins=100)\n",
    "plt.show()\n",
    "\n",
    "fmeasure = f1_score(labels_true, labels_pred, average = \"macro\", labels = np.unique(labels_pred)) \n",
    "auroc = sklearn.metrics.roc_auc_score(labels_true, labels_scores, labels = np.unique(labels_pred))\n",
    "\n",
    "print(fmeasure)\n",
    "print(auroc)\n",
    "print(sklearn.metrics.precision_score(labels_true, labels_pred, average = \"macro\", labels = np.unique(labels_pred)) )\n",
    "\n",
    "sum(labels_true == labels_pred) / len(labels_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c23777c2-e144-46bc-8a3b-bf137cdbb565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:43:45.479431Z",
     "iopub.status.busy": "2023-02-25T11:43:45.479145Z",
     "iopub.status.idle": "2023-02-25T11:43:46.717176Z",
     "shell.execute_reply": "2023-02-25T11:43:46.716236Z",
     "shell.execute_reply.started": "2023-02-25T11:43:45.479407Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_train = feature_x_data[_map] #[train_gesture_map == 1]\n",
    "labels_train = (y_user.argmax(axis=1) == auth_user)[_map].astype(int)\n",
    "\n",
    "shuffled_data_train, shuffled_labels_train = shuffle(data_train, labels_train, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "shuffled_data_train = scaler.fit_transform(shuffled_data_train)\n",
    "\n",
    "\n",
    "kFold = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "val_map = next(kFold.split(shuffled_data_train, shuffled_labels_train))[1]\n",
    "\n",
    "wa_model = RandomForestClassifier(n_estimators = 100, random_state = 0, class_weight=\"balanced\").fit(shuffled_data_train, shuffled_labels_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "83900649-0b49-450e-89af-174db62fdddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T12:01:33.694052Z",
     "iopub.status.busy": "2023-02-25T12:01:33.693040Z",
     "iopub.status.idle": "2023-02-25T12:01:33.970402Z",
     "shell.execute_reply": "2023-02-25T12:01:33.969522Z",
     "shell.execute_reply.started": "2023-02-25T12:01:33.694008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587249373433584\n",
      "0.9554492469568805\n",
      "0.5535769548174128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9686635944700461"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_test_map = (test_gesture_map == 1)\n",
    "\n",
    "labels_pred = wa_model.predict(scaler.transform(feature_x_data[_test_map]))\n",
    "labels_scores = wa_model.predict_proba(scaler.transform(feature_x_data[_test_map]))[:, 1]\n",
    "\n",
    "labels_true = (y_user.argmax(axis=1) == auth_user)[_test_map].astype(int)\n",
    "\n",
    "np.histogram(labels_scores)\n",
    "plt.hist(labels_scores, bins=100)\n",
    "\n",
    "fmeasure = f1_score(labels_true, labels_pred, average = \"macro\", labels = np.unique(labels_pred)) \n",
    "auroc = sklearn.metrics.roc_auc_score(labels_true, labels_scores, labels = np.unique(labels_pred))\n",
    "\n",
    "print(fmeasure)\n",
    "print(auroc)\n",
    "print(sklearn.metrics.recall_score(labels_true, labels_pred, average = \"macro\", labels = np.unique(labels_pred)) )\n",
    "\n",
    "sum(labels_true == labels_pred) / len(labels_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b0c5a0a0-ca99-42fe-8f66-4db64d582583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T11:43:46.897387Z",
     "iopub.status.busy": "2023-02-25T11:43:46.897082Z",
     "iopub.status.idle": "2023-02-25T11:43:46.905384Z",
     "shell.execute_reply": "2023-02-25T11:43:46.904192Z",
     "shell.execute_reply.started": "2023-02-25T11:43:46.897365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1080,    5]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418acd88-046f-436e-8992-56a3403f0537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T08:46:11.366550Z",
     "iopub.status.busy": "2023-02-20T08:46:11.365750Z",
     "iopub.status.idle": "2023-02-20T08:47:19.348422Z",
     "shell.execute_reply": "2023-02-20T08:47:19.346927Z",
     "shell.execute_reply.started": "2023-02-20T08:46:11.366471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_ranking\n",
      "  Downloading tensorflow_ranking-0.5.2-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.4/150.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow_ranking) (1.1.0)\n",
      "Collecting numpy==1.23.2\n",
      "  Downloading numpy-1.23.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
      "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow_ranking) (1.14.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.47.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.19.4)\n",
      "Collecting tensorflow<3,>=2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (63.1.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.26.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.14.1)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (21.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.0)\n",
      "Installing collected packages: flatbuffers, tensorflow-estimator, numpy, keras, tensorboard, tensorflow, tensorflow-serving-api, tensorflow_ranking\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.1\n",
      "    Uninstalling tensorflow-2.9.1:\n",
      "      Successfully uninstalled tensorflow-2.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.3.8+cuda11.cudnn82 requires flatbuffers<3.0,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-23.1.21 keras-2.11.0 numpy-1.23.2 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-serving-api-2.11.0 tensorflow_ranking-0.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478056-1c93-4c36-bdea-e1b0b300ce54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
