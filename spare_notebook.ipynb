{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf182c00-203b-485e-b8e8-72b056250048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T08:22:32.197555Z",
     "iopub.status.busy": "2023-02-25T08:22:32.197145Z",
     "iopub.status.idle": "2023-02-25T08:23:21.973252Z",
     "shell.execute_reply": "2023-02-25T08:23:21.972021Z",
     "shell.execute_reply.started": "2023-02-25T08:22:32.197479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_ranking\n",
      "  Downloading tensorflow_ranking-0.5.2-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.4/150.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow_probability\n",
      "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
      "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting numpy==1.23.2\n",
      "  Downloading numpy-1.23.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow_ranking) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow_ranking) (1.14.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability) (0.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability) (2.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability) (5.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (21.3)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting tensorflow<3,>=2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.19.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.47.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (63.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.26.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (14.0.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.1.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.3.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2019.11.28)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.0)\n",
      "Installing collected packages: flatbuffers, dm-tree, typeguard, tensorflow-estimator, numpy, keras, tensorflow_probability, tensorflow_addons, tensorboard, tensorflow, tensorflow-serving-api, tensorflow_ranking\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.1\n",
      "    Uninstalling tensorflow-2.9.1:\n",
      "      Successfully uninstalled tensorflow-2.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.3.8+cuda11.cudnn82 requires flatbuffers<3.0,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dm-tree-0.1.8 flatbuffers-23.1.21 keras-2.11.0 numpy-1.23.2 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-serving-api-2.11.0 tensorflow_addons-0.19.0 tensorflow_probability-0.19.0 tensorflow_ranking-0.5.2 typeguard-2.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_ranking tensorflow_probability tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04af61e7-bb9f-4a53-a553-69b370414568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T09:01:27.081783Z",
     "iopub.status.busy": "2023-02-25T09:01:27.081398Z",
     "iopub.status.idle": "2023-02-25T09:01:27.138635Z",
     "shell.execute_reply": "2023-02-25T09:01:27.137861Z",
     "shell.execute_reply.started": "2023-02-25T09:01:27.081755Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Permute, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Reshape, SpatialDropout1D, Dropout, TimeDistributed\n",
    "\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pickle\n",
    "\n",
    "import VAE\n",
    "from VAE import Sampling, VAE, get_auth_model_from_latent_space, get_decoder, get_encoder, get_auth_model\n",
    "from VAE_stats import VAE_stats\n",
    "from scaler import CustomScaler\n",
    "\n",
    "\n",
    "import datetime, csv, os, re, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, f1_score, precision_score, precision_recall_curve, recall_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scaler import CustomScaler\n",
    "\n",
    "from featurize import filter, featurize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#configs\n",
    "maxprewindowsize = 4\n",
    "classifier = 'rfc'\n",
    "\n",
    "folds = 10\n",
    "\n",
    "\n",
    "def get_average(l):\n",
    "\treturn 0 if 0 == len(l) else sum(l) / len(l)\n",
    "\n",
    "def get_eer(scores_legit, scores_adv):\n",
    "\tscores_legit = sorted(scores_legit)\n",
    "\tscores_adv = sorted(scores_adv)\n",
    "\t\n",
    "\t#treat each legitimate sample distance as a possible threshold, determine the point where FRR crosses FAR\n",
    "\tfor c, threshold in enumerate(scores_legit):\n",
    "\t\tfrr = c * 1.0 / len(scores_legit)\n",
    "\t\tadv_index = next((x[0] for x in enumerate(scores_adv) if x[1] > threshold), len(scores_adv))\n",
    "\t\tfar = 1 - (adv_index * 1.0 / len(scores_adv))\n",
    "\t\tif frr >= far:\n",
    "\t\t\treturn threshold, far\n",
    "\tprint(\"Failure\")\n",
    "\n",
    "def get_eer_recogblind(scores_legit, scores_adv_typed, total_w, total_b, total_i):\n",
    "\tscores_legit = sorted(scores_legit)\n",
    "\tscores_adv_typed = sorted(scores_adv_typed, key = lambda x:x[0])\n",
    "\t\n",
    "\t#treat each legitimate sample distance as a possible threshold, determine the point where FRR crosses FAR\n",
    "\tfor c, threshold in enumerate(scores_legit):\n",
    "\t\tfrr = c * 1.0 / len(scores_legit)\n",
    "\t\tadv_index = next((x[0] for x in enumerate(scores_adv_typed) if x[1][0] > threshold), len(scores_adv_typed))\n",
    "\t\tfar = 1 - (adv_index * 1.0 / len(scores_adv))\n",
    "\t\tif frr >= far:\n",
    "\t\t\trejectrate_w = 0 if 0 == total_w else len([i for i in scores_adv_typed if 'W' == i[1] and i[0] >= threshold]) / total_w\n",
    "\t\t\trejectrate_b = 0 if 0 == total_b else len([i for i in scores_adv_typed if 'B' == i[1] and i[0] >= threshold]) / total_b\n",
    "\t\t\trejectrate_i = 0 if 0 == total_i else len([i for i in scores_adv_typed if 'I' == i[1] and i[0] >= threshold]) / total_i\n",
    "\t\t\treturn threshold, far, rejectrate_w, rejectrate_b, rejectrate_i\n",
    "\n",
    "def get_far_when_zero_frr(scores_legit, scores_adv):\n",
    "\tscores_legit = sorted(scores_legit)\n",
    "\tscores_adv = sorted(scores_adv)\n",
    "\t\n",
    "\t#treat each legitimate sample distance as a possible threshold, determine the point with the lowest FAR that satisfies the condition that FRR = 0\n",
    "\tfor c, threshold in enumerate(scores_legit):\n",
    "\t\tfrr = c * 1.0 / len(scores_legit)\n",
    "\t\tadv_index = next((x[0] for x in enumerate(scores_adv) if x[1] > threshold), len(scores_adv))\n",
    "\t\tfar = 1 - (adv_index * 1.0 / len(scores_adv))\n",
    "\t\tif frr > 0.001:\n",
    "\t\t\treturn threshold, far\n",
    "\n",
    "def plot_threshold_by_far_frr(scores_legit, scores_adv, far_theta):\n",
    "\tscores_legit = sorted(scores_legit)\n",
    "\tscores_adv = sorted(scores_adv)\n",
    "\t\n",
    "\tfrr = []\n",
    "\tfar = []\n",
    "\tthresholds = []\n",
    "\tfor c, threshold in enumerate(scores_legit):\n",
    "\t\tfrr.append((c * 1.0 / len(scores_legit)) * 100)\n",
    "\t\tadv_index = next((x[0] for x in enumerate(scores_adv) if x[1] > threshold), len(scores_adv))\n",
    "\t\tfar.append((1 - (adv_index * 1.0 / len(scores_adv))) * 100)\n",
    "\t\tthresholds.append(threshold)\n",
    "\tplt.figure(figsize = (6, 6))\n",
    "\t#plt.rcParams.update({'font.size': fontsize_legends})\n",
    "\tplt.plot(thresholds, far, 'tab:blue', label = 'FAR')\n",
    "\tplt.plot(thresholds, frr, 'tab:orange', label = 'FRR')\n",
    "\tplt.ylabel('error rate (%)')\n",
    "\tplt.xlabel(r'decision threshold, $\\theta$')\n",
    "\tplt.axvline(x = far_theta, c = 'red')\n",
    "\tplt.legend(loc = 'best')\n",
    "\tplt.tight_layout(pad = 0.05)\n",
    "\tplt.show()\n",
    "\n",
    "def plot_threshold_by_precision_recall(labels_test, labels_scores):\n",
    "\tp, r, thresholds = precision_recall_curve(labels_test, labels_scores)\n",
    "\tplt.figure(figsize = (6, 6))\n",
    "\t#plt.rcParams.update({'font.size': fontsize_legends})\n",
    "\tplt.title('Precision and Recall Scores as a Function of the Decision Threshold', fontsize = 12)\n",
    "\tplt.plot(thresholds, p[:-1], 'tab:blue', label = 'precision')\n",
    "\tplt.plot(thresholds, r[:-1], 'tab:orange', label = 'recall')\n",
    "\tplt.ylabel('score')\n",
    "\tplt.xlabel(r'decision threshold, $\\theta$')\n",
    "\tplt.legend(loc = 'best')\n",
    "\tplt.tight_layout(pad = 0.05)\n",
    "\tplt.show()\n",
    "\n",
    "def plot_roc_curve(labels_test, labels_scores):\n",
    "\tfpr, tpr, auc_thresholds = roc_curve(labels_test, labels_scores)\n",
    "\tprint('AUC of ROC = ' + str(auc(fpr, tpr)))\n",
    "\tplt.figure(figsize = (6, 6))\n",
    "\t#plt.rcParams.update({'font.size': fontsize_legends})\n",
    "\tplt.title('ROC Curve', fontsize = 12)\n",
    "\tplt.plot(fpr, tpr, 'tab:orange', label = 'recall optimized')\n",
    "\tplt.plot([0, 1], [0, 1], 'k--')\n",
    "\tplt.axis([-0.005, 1, 0, 1.005])\n",
    "\tplt.xticks(np.arange(0, 1, 0.05), rotation = 90)\n",
    "\tplt.xlabel('false positive rate')\n",
    "\tplt.ylabel('true positive rate (recall)')\n",
    "\tplt.legend(loc = 'best')\n",
    "\tplt.tight_layout(pad = 0.05)\n",
    "\tplt.show()\n",
    "\n",
    "def get_ascending_userID_list_string():\n",
    "\tfor u in userIDs:\n",
    "\t\tif not 'user' in u and len(u) != 7:\n",
    "\t\t\tsys.exit('ERROR: userID not valid: ' + str(u))\n",
    "\tIDs = [int(u[4:]) for u in userIDs]\n",
    "\tIDs.sort(reverse = False)\n",
    "\treturn ','.join([f'{i:03}' for i in IDs])\n",
    "\n",
    "def get_descending_feature_list_string(weights, labels, truncate = 0):\n",
    "\tindicies = [i for i in range(len(weights))]\n",
    "\tfor i in range(len(indicies)):\n",
    "\t\tfor j in range(len(indicies)):\n",
    "\t\t\tif i != j and weights[indicies[i]] > weights[indicies[j]]:\n",
    "\t\t\t\ttemp = indicies[i]\n",
    "\t\t\t\tindicies[i] = indicies[j]\n",
    "\t\t\t\tindicies[j] = temp\n",
    "\tif truncate != 0:\n",
    "\t\tdel indicies[truncate:]\n",
    "\treturn '\\n'.join([str('%.6f' % weights[i]) + ' (' + labels[i] + ')' for i in indicies])\n",
    "\n",
    "def write_verbose(f, s):\n",
    "\toutfilename = f + '-verbose.txt'\n",
    "\toutfile = open(outfilename, 'a')\n",
    "\toutfile.write(s + '\\n')\n",
    "\toutfile.close()\n",
    "    \n",
    "\n",
    "class SplitLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super(SplitLayer, self).__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(inputs,indices=self.layers,axis=-1)\n",
    "\n",
    "\n",
    "def get_new_auth_model(input_dim=(200,16)):\n",
    "    #input_dim = (200,16)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_dim)\n",
    "    x = inputs\n",
    "\n",
    "    xs = []\n",
    "    \n",
    "    for i in range(16):\n",
    "        \n",
    "        x = SplitLayer(i)(inputs)\n",
    "        reshaped = Reshape((200, 1))(x)\n",
    "        \n",
    "        x = Conv1D(100, 10, strides=2, padding=\"same\")(reshaped)\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "        #x = SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        x = Conv1D(100, 3, strides=1, padding=\"same\")(x)\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "        #x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "        xs.append(x)\n",
    "    \n",
    "    x = layers.Concatenate()(xs)\n",
    "\n",
    "    \n",
    "    x = LSTM(50)(x)\n",
    "\n",
    "    x = Dense(25, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(10, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    #x = layers.Lambda(lambda x: x / 2)\n",
    "    out = x\n",
    "\n",
    "    model = keras.Model(inputs, out, name=\"LSTM_classifier_v2\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def run(models=[\"WatchAuth\"], forbidden_stat = 0, users = range(16), limit_data_first_x = None, median_filtering= False, terminal_types = [3,4,5,6,7,8,9], score_mode= \"macro\", extra_data_handle=None, repetitions=1, save_name = \"recent_results\"):\n",
    "    \n",
    "    file_name = \"raw_with_maps\" # or offsets_2\n",
    "    vae_stats = VAE_stats(10)\n",
    "    \n",
    "    if median_filtering:\n",
    "        x_data = np.load(f\"data/processed/x_{file_name}_median_filtered.npy\")\n",
    "        feature_x_data = np.load(f\"data/processed/x_{file_name}_median_filtered_features.npy\")\n",
    "    else:\n",
    "        x_data = np.load(f\"data/processed/x_{file_name}_filtered.npy\")  # _filtered\n",
    "        feature_x_data = np.load(f\"data/processed/x_{file_name}_features.npy\")\n",
    "\n",
    "    x_data = x_data[:, :, [0,1,2,3,4,5,6,7]]\n",
    "    feature_x_data = vae_stats.vae_featurize(x_data)\n",
    "        \n",
    "    y_user = np.load(f\"data/processed/y_user_{file_name}.npy\")\n",
    "    y_intent = np.load(f\"data/processed/y_intent_{file_name}.npy\")\n",
    "    y_gesture = np.load(f\"data/processed/y_gesture_type_{file_name}.npy\")\n",
    "\n",
    "    train_gesture_map = np.load(f\"data/processed/train_gesture_map_{file_name}.npy\")\n",
    "    test_gesture_map = np.load(f\"data/processed/test_gesture_map_{file_name}.npy\")\n",
    "    \n",
    "    feature_array = feature_x_data\n",
    "\n",
    "    \n",
    "\n",
    "    train_data_map = train_gesture_map.astype(bool)\n",
    "    test_data_map = test_gesture_map.astype(bool)\n",
    "\n",
    "    output = {}\n",
    "    \n",
    "    all_scores_legit, all_scores_adv = [], []\n",
    "    \n",
    "    for auth_user in users:\n",
    "        print(f\"auth_user is {auth_user}\")\n",
    "           \n",
    "        \n",
    "        zero_x_data = np.load(f\"data/generated_samples/user={auth_user}_model=vae_gan_specific_50_gestures=2_non_user.npy\")\n",
    "        zero_x_data = zero_x_data[:, :, [0,1,2,3,4,5,6,7]]\n",
    "        \n",
    "        zero_x_data[:,:,3] = (zero_x_data[:,:,0]**2 + zero_x_data[:,:,1]**2 + zero_x_data[:,:,2]**2)**0.5\n",
    "        zero_x_data[:,:,7] = (zero_x_data[:,:,4]**2 + zero_x_data[:,:,5]**2 + zero_x_data[:,:,6]**2)**0.5\n",
    "            \n",
    "        one_x_data = np.load(extra_data_handle(auth_user))\n",
    "        one_x_data = one_x_data[:, :, [0,1,2,3,4,5,6,7]]\n",
    "        \n",
    "        one_x_data[:,:,3] = (one_x_data[:,:,0]**2 + one_x_data[:,:,1]**2 + one_x_data[:,:,2]**2)**0.5\n",
    "        one_x_data[:,:,7] = (one_x_data[:,:,4]**2 + one_x_data[:,:,5]**2 + one_x_data[:,:,6]**2)**0.5\n",
    "          \n",
    "\n",
    "        if median_filtering:\n",
    "            one_x_feature_data = np.zeros((len(one_x_data), feature_x_data.shape[1]))\n",
    "\n",
    "            for i in range(0, len(one_x_data), 128):\n",
    "                median_filtered_one_data = tfa.image.median_filter2d(one_x_data[i:i+128],filter_shape=(1,5), padding=\"constant\")\n",
    "                one_x_feature_data[i:i+128] = vae_stats.vae_featurize(median_filtered_one_data)\n",
    "            \n",
    "            zero_x_feature_data = np.zeros((len(zero_x_data), feature_x_data.shape[1]))\n",
    "\n",
    "            for i in range(0, len(zero_x_data), 128):\n",
    "                median_filtered_zero_data = tfa.image.median_filter2d(zero_x_data[i:i+128],filter_shape=(1,5), padding=\"constant\")\n",
    "                zero_x_feature_data[i:i+128] = vae_stats.vae_featurize(median_filtered_zero_data)\n",
    "                \n",
    "\n",
    "        data_train = np.concatenate([zero_x_data, one_x_data])\n",
    "        feature_data_train = np.concatenate([zero_x_feature_data, one_x_feature_data])\n",
    "        labels_train = np.concatenate([np.zeros(len(zero_x_data)), np.ones(len(one_x_data))])\n",
    "        \n",
    "        feature_data_test = feature_array[test_data_map]\n",
    "        data_test = x_data[test_data_map]\n",
    "        labels_test = (y_user.argmax(axis=1) == auth_user)[test_data_map].astype(int)\n",
    "        \n",
    "        #print(np.unique(labels_train, return_counts=True))\n",
    "        \n",
    "        for model_name in models:  # \"DL\", \"MLP\", \"SVM\", \n",
    "\n",
    "            key = str(auth_user) + \"_\" + model_name\n",
    "            output[key] = {\"fm\":[], \"prec\":[], \"rec\":[], \"eer\":[], \"far\":[]}\n",
    "\n",
    "\n",
    "            a_precisions = []\n",
    "            a_recalls = []\n",
    "            a_fmeasures = []\n",
    "            a_pr_stdev = []\n",
    "            a_re_stdev = []\n",
    "            a_fm_stdev = []\n",
    "            a_eers = []\n",
    "            a_eer_thetas = []\n",
    "            a_fars = []\n",
    "            a_far_thetas = []\n",
    "            a_ee_stdev = []\n",
    "            a_ee_th_stdev = []\n",
    "            a_fa_stdev = []\n",
    "            a_fa_th_stdev = []\n",
    "            a_aurocs = []\n",
    "            a_losses = []\n",
    "\n",
    "\n",
    "            \n",
    "            if model_name in [\"DL\", \"SVM\"]:\n",
    "                reps = 1\n",
    "            else:\n",
    "                reps = repetitions\n",
    "\n",
    "\n",
    "            for repetition in range(reps):\n",
    "                if model_name == \"DL\":\n",
    "\n",
    "                    shuffled_data_train, shuffled_labels_train = shuffle(data_train, labels_train, random_state=0)\n",
    "\n",
    "                    #ORDER = 6\n",
    "                    #CUTOFF = 3.667\n",
    "                    #shuffled_data_train = filter(shuffled_data_train, list(range(16)), ORDER, CUTOFF)\n",
    "\n",
    "                    scaler = CustomScaler()\n",
    "                    shuffled_data_train = scaler.fit_and_transform(shuffled_data_train)\n",
    "\n",
    "\n",
    "                    kFold = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "                    val_map = next(kFold.split(shuffled_data_train, shuffled_labels_train))[1]\n",
    "\n",
    "\n",
    "                    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', start_from_epoch=50,\n",
    "                                                                               patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "                    model = None\n",
    "                    model = get_new_auth_model((200,16))\n",
    "                    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=\"binary_crossentropy\")\n",
    "                    history = model.fit(shuffled_data_train[~val_map], shuffled_labels_train[~val_map], epochs=1000,\n",
    "                                        batch_size=128, verbose=1,\n",
    "                                        validation_data=(shuffled_data_train[val_map], shuffled_labels_train[val_map]),\n",
    "                                       class_weight={0:0.5, 1:30},\n",
    "                                       callbacks=[early_stopping_callback])\n",
    "\n",
    "                    plt.plot(history.history['loss'])\n",
    "                    plt.plot(history.history['val_loss'])\n",
    "                    plt.title('model loss')\n",
    "                    plt.ylabel('loss')\n",
    "                    plt.xlabel('epoch')\n",
    "                    plt.legend(['train', 'val'], loc='upper left')\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.clf()\n",
    "\n",
    "                    labels_scores = model.predict(scaler.transform(data_test)).squeeze()\n",
    "                    labels_pred = (labels_scores >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "                elif model_name == \"WatchAuth\":\n",
    "                    model = RandomForestClassifier(n_estimators = 100, random_state = repetition, class_weight=\"balanced\").fit(feature_data_train, labels_train)\n",
    "\n",
    "\n",
    "                    labels_pred = model.predict(feature_data_test)\n",
    "                    labels_scores = model.predict_proba(feature_data_test)[:, 1]\n",
    "\n",
    "                elif model_name == \"SVM\":\n",
    "\n",
    "                    clf = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True, class_weight=\"balanced\"))\n",
    "                    clf.fit(feature_data_train, labels_train)\n",
    "\n",
    "                    labels_pred = clf.predict(feature_data_test)\n",
    "                    labels_scores = clf.predict_proba(feature_data_test)[:, 1]\n",
    "\n",
    "                elif model_name == \"MLP\":\n",
    "\n",
    "                    clf = make_pipeline(StandardScaler(), MLPClassifier(solver='lbfgs', alpha=1e-3,\n",
    "                             hidden_layer_sizes=(50,20, 2), random_state=repetition)) # ,class_weight=\"balanced\")) \n",
    "                    clf.fit(feature_data_train, labels_train)\n",
    "\n",
    "                    labels_pred = clf.predict(feature_data_test)\n",
    "                    labels_scores = clf.predict_proba(feature_data_test)[:, 1]\n",
    "\n",
    "                #get precision, recall, and F-measure scores\n",
    "                precision = precision_score(labels_test, labels_pred, average = score_mode, labels = np.unique(labels_pred))\n",
    "                recall = recall_score(labels_test, labels_pred, average = score_mode, labels = np.unique(labels_pred))\n",
    "                fmeasure = f1_score(labels_test, labels_pred, average = score_mode, labels = np.unique(labels_pred)) # macro vs binary\n",
    "                auroc = sklearn.metrics.roc_auc_score(labels_test, labels_scores, average = \"macro\", labels = np.unique(labels_pred))\n",
    "                a_precisions.append(precision)\n",
    "                a_recalls.append(recall)\n",
    "                a_fmeasures.append(fmeasure)\n",
    "                a_aurocs.append(auroc)\n",
    "                a_losses.append(sklearn.metrics.log_loss(labels_test, labels_scores))\n",
    "                #print(fmeasure)\n",
    "\n",
    "                #print(f\"Loss is {sklearn.metrics.log_loss(labels_test, labels_scores)}\")\n",
    "                #print(precision)\n",
    "                #print(recall)\n",
    "\n",
    "\n",
    "                scores_legit = [labels_scores[i] for i in range(len(labels_test)) if 1 == labels_test[i]]\n",
    "                scores_adv = [labels_scores[i] for i in range(len(labels_test)) if 0 == labels_test[i]]\n",
    "                eer_theta, eer = get_eer(scores_legit, scores_adv)\n",
    "                far_theta, far = get_far_when_zero_frr(scores_legit, scores_adv)\n",
    "\n",
    "                all_scores_legit += scores_legit\n",
    "                all_scores_adv += scores_adv\n",
    "                #plot_roc_curve(labels_test, labels_scores)\n",
    "                #plot_threshold_by_precision_recall(labels_test, labels_scores)\n",
    "                #plot_threshold_by_far_frr(scores_legit, scores_adv, far_theta)\n",
    "\n",
    "                #print(f\"eer is {eer}\")\n",
    "                a_eers.append(eer)\n",
    "                a_eer_thetas.append(eer_theta)\n",
    "                a_fars.append(far)\n",
    "                a_far_thetas.append(far_theta)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            a_pr_stdev = np.std(a_precisions, ddof = 1)\n",
    "            a_re_stdev = np.std(a_recalls, ddof = 1)\n",
    "            a_fm_stdev = np.std(a_fmeasures, ddof = 1)\n",
    "            a_ee_stdev = np.std(a_eers, ddof = 1)\n",
    "            a_ee_th_stdev = np.std(a_eer_thetas, ddof = 1)\n",
    "            a_fa_stdev = np.std(a_fars, ddof = 1)\n",
    "            a_fa_th_stdev = np.std(a_far_thetas, ddof = 1)\n",
    "            \n",
    "            a_auroc_stdev = np.std(a_aurocs, ddof = 1)\n",
    "            a_losses_stdev = np.std(a_losses, ddof = 1)\n",
    "\n",
    "            output[key][\"fm\"] = (get_average(a_fmeasures), a_fm_stdev)\n",
    "            output[key][\"prec\"] = (get_average(a_precisions), a_pr_stdev)\n",
    "            output[key][\"rec\"] = (get_average(a_recalls), a_re_stdev)\n",
    "            output[key][\"eer\"] = (get_average(a_eers), a_ee_stdev)\n",
    "            output[key][\"far\"] = (get_average(a_fars), a_fa_stdev)\n",
    "            output[key][\"auroc\"] = (get_average(a_aurocs), a_auroc_stdev)\n",
    "            output[key][\"loss\"] = (get_average(a_losses), a_losses_stdev)\n",
    "\n",
    "\n",
    "            with open(f'data/stats/{save_name}.pickle', 'wb') as handle:\n",
    "                pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "    # multiple_models_scaled_v2.pickle was a good summary of different types of model\n",
    "\n",
    "    return output, (all_scores_legit, all_scores_adv)\n",
    "\n",
    "def summarise_by_model(output, models=[\"WatchAuth\"]):\n",
    "    \n",
    "    for stat in [\"fm\", \"prec\", \"rec\", \"eer\", \"far\", \"auroc\", \"loss\"]:\n",
    "        for model_name in models:\n",
    "            l = []\n",
    "            for auth_user in range(16):\n",
    "                l.append(output[f\"{auth_user}_{model_name}\"][stat][0])\n",
    "            print(model_name, stat, sum(l) /16)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53c88ad9-e0b0-4c3b-80cd-f5554b74f764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T09:22:52.352577Z",
     "iopub.status.busy": "2023-02-25T09:22:52.351801Z",
     "iopub.status.idle": "2023-02-25T09:23:24.919617Z",
     "shell.execute_reply": "2023-02-25T09:23:24.918531Z",
     "shell.execute_reply.started": "2023-02-25T09:22:52.352542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "auth_user is 0\n",
      "(array([0, 1]), array([2254,    5]))\n",
      "auth_user is 1\n",
      "(array([0, 1]), array([2262,    5]))\n",
      "auth_user is 2\n",
      "(array([0, 1]), array([2248,    5]))\n",
      "auth_user is 3\n",
      "(array([0, 1]), array([2279,    5]))\n",
      "auth_user is 4\n",
      "(array([0, 1]), array([2180,    5]))\n",
      "auth_user is 5\n",
      "(array([0, 1]), array([2248,    5]))\n",
      "auth_user is 6\n",
      "(array([0, 1]), array([2253,    5]))\n",
      "auth_user is 7\n",
      "(array([0, 1]), array([2245,    5]))\n",
      "auth_user is 8\n",
      "(array([0, 1]), array([2311,    5]))\n",
      "auth_user is 9\n",
      "(array([0, 1]), array([2314,    5]))\n",
      "auth_user is 10\n",
      "(array([0, 1]), array([2245,    5]))\n",
      "auth_user is 11\n",
      "(array([0, 1]), array([2211,    5]))\n",
      "auth_user is 12\n",
      "(array([0, 1]), array([2239,    5]))\n",
      "auth_user is 13\n",
      "(array([0, 1]), array([2236,    5]))\n",
      "auth_user is 14\n",
      "(array([0, 1]), array([2230,    5]))\n",
      "auth_user is 15\n",
      "(array([0, 1]), array([2230,    5]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutputs = {\"original_restricted\":original_restricted}\\nfor model in models:\\n    for method in [ \"just_sampled\",  \"adversarially_mixed\", \"self_mixed\", \"same_user_other_gestures_terminal=345\"]: #, ]:   #[\"just_sampled\", \"adversarially_mixed\", \"self_mixed\", \"same_user_other_gestures_terminal=345\"]: # \\n\\n        handle = lambda x: f\"data/generated_samples/user={x}_model={model}_method={method}.npy\"\\n\\n        output = run(models=[\"WatchAuth\"], median_filtering=True, score_mode=\"macro\", extra_data_handle = handle, repetitions=3, save_name=f\"{model}-{method}-macro-synth\")\\n\\n        outputs[f\"{model}-{method}-macro\"] = output  \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testing_results import run as run_normal\n",
    "\n",
    "models = [\"vae_gan_specific_50_gestures=2\"]\n",
    "model = models[0]\n",
    "\n",
    "\n",
    "original_restricted = run_normal(models=[\"WatchAuth\"], terminal_types = [3], median_filtering=True, score_mode=\"macro\", limit_data_first_x=5, repetitions=3)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "outputs = {\"original_restricted\":original_restricted}\n",
    "for model in models:\n",
    "    for method in [ \"just_sampled\",  \"adversarially_mixed\", \"self_mixed\", \"same_user_other_gestures_terminal=345\"]: #, ]:   #[\"just_sampled\", \"adversarially_mixed\", \"self_mixed\", \"same_user_other_gestures_terminal=345\"]: # \n",
    "\n",
    "        handle = lambda x: f\"data/generated_samples/user={x}_model={model}_method={method}.npy\"\n",
    "\n",
    "        output = run(models=[\"WatchAuth\"], median_filtering=True, score_mode=\"macro\", extra_data_handle = handle, repetitions=3, save_name=f\"{model}-{method}-macro-synth\")\n",
    "\n",
    "        outputs[f\"{model}-{method}-macro\"] = output  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5e297d8-ef62-4acc-9876-550d4dc1839c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T09:23:24.921282Z",
     "iopub.status.busy": "2023-02-25T09:23:24.921037Z",
     "iopub.status.idle": "2023-02-25T09:23:24.945810Z",
     "shell.execute_reply": "2023-02-25T09:23:24.944353Z",
     "shell.execute_reply.started": "2023-02-25T09:23:24.921262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43msummarise_by_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_restricted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_restricted\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(key)\n",
      "File \u001b[0;32m/notebooks/testing_results.py:502\u001b[0m, in \u001b[0;36msummarise_by_model\u001b[0;34m(output, models)\u001b[0m\n\u001b[1;32m    500\u001b[0m     l \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m auth_user \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m--> 502\u001b[0m         l\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mauth_user\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[stat][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_name, stat, \u001b[38;5;28msum\u001b[39m(l) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from testing_results import summarise_by_model\n",
    "\n",
    "model_type = [\"WatchAuth\"]\n",
    "\n",
    "for key in outputs:\n",
    "    print(\"original\")\n",
    "\n",
    "    summarise_by_model(original_restricted, model_type)\n",
    "    if key != \"original_restricted\":\n",
    "        print(key)\n",
    "        summarise_by_model(outputs[key][0],model_type) #s[key]\n",
    "\n",
    "    print(\"_\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1fba9c8-f28b-4cdb-971b-8b56ebe77708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T09:14:15.391049Z",
     "iopub.status.busy": "2023-02-25T09:14:15.390064Z",
     "iopub.status.idle": "2023-02-25T09:14:15.406884Z",
     "shell.execute_reply": "2023-02-25T09:14:15.406181Z",
     "shell.execute_reply.started": "2023-02-25T09:14:15.391019Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.23,\n",
       "  0.23,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.1,\n",
       "  0.28,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.15,\n",
       "  0.12,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.12,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.18,\n",
       "  0.55,\n",
       "  0.24,\n",
       "  0.17,\n",
       "  0.28,\n",
       "  0.27,\n",
       "  0.28,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.45,\n",
       "  0.24,\n",
       "  0.24,\n",
       "  0.19,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.21,\n",
       "  0.18,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.17,\n",
       "  0.25,\n",
       "  0.16,\n",
       "  0.29,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.26,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.19,\n",
       "  0.41,\n",
       "  0.13,\n",
       "  0.2,\n",
       "  0.27,\n",
       "  0.11,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.17,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.06,\n",
       "  0.35,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.58,\n",
       "  0.26,\n",
       "  0.15,\n",
       "  0.24,\n",
       "  0.25,\n",
       "  0.22,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.54,\n",
       "  0.31,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.27,\n",
       "  0.24,\n",
       "  0.24,\n",
       "  0.18,\n",
       "  0.23,\n",
       "  0.29,\n",
       "  0.19,\n",
       "  0.1,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.12,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.07,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.14,\n",
       "  0.24,\n",
       "  0.24,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.24,\n",
       "  0.25,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.45,\n",
       "  0.2,\n",
       "  0.33,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.2,\n",
       "  0.06,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.17,\n",
       "  0.19,\n",
       "  0.64,\n",
       "  0.27,\n",
       "  0.15,\n",
       "  0.25,\n",
       "  0.24,\n",
       "  0.29,\n",
       "  0.18,\n",
       "  0.23,\n",
       "  0.39,\n",
       "  0.26,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.27,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.08,\n",
       "  0.21,\n",
       "  0.14,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.26,\n",
       "  0.26,\n",
       "  0.23,\n",
       "  0.19,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.36,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.27,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.06,\n",
       "  0.18,\n",
       "  0.06,\n",
       "  0.17,\n",
       "  0.27,\n",
       "  0.14,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.08,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.03,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.05,\n",
       "  0.1,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.15,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.12,\n",
       "  0.06,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.22,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.22,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.21,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.14,\n",
       "  0.24,\n",
       "  0.01,\n",
       "  0.08,\n",
       "  0.18,\n",
       "  0.05,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.18,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.24,\n",
       "  0.08,\n",
       "  0.18,\n",
       "  0.09,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.03,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.03,\n",
       "  0.07,\n",
       "  0.06,\n",
       "  0.02,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.18,\n",
       "  0.1,\n",
       "  0.25,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.27,\n",
       "  0.13,\n",
       "  0.07,\n",
       "  0.13,\n",
       "  0.16,\n",
       "  0.01,\n",
       "  0.1,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.06,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.06,\n",
       "  0.21,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.24,\n",
       "  0.08,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.27,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.02,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.04,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.08,\n",
       "  0.13,\n",
       "  0.08,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.18,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.22,\n",
       "  0.09,\n",
       "  0.23,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.19,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.06,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.14,\n",
       "  0.16,\n",
       "  0.05,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.07,\n",
       "  0.02,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.15,\n",
       "  0.02,\n",
       "  0.04,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.09,\n",
       "  0.22,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.01,\n",
       "  0.24,\n",
       "  0.03,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.11,\n",
       "  0.19,\n",
       "  0.1,\n",
       "  0.14,\n",
       "  0.16,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.17,\n",
       "  0.22,\n",
       "  0.33,\n",
       "  0.15,\n",
       "  0.2,\n",
       "  0.35,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.49,\n",
       "  0.13,\n",
       "  0.32,\n",
       "  0.16,\n",
       "  0.23,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.29,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.16,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.09,\n",
       "  0.17,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.29,\n",
       "  0.26,\n",
       "  0.11,\n",
       "  0.21,\n",
       "  0.07,\n",
       "  0.16,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.12,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.2,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.12,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.13,\n",
       "  0.19,\n",
       "  0.14,\n",
       "  0.11,\n",
       "  0.16,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.21,\n",
       "  0.39,\n",
       "  0.07,\n",
       "  0.21,\n",
       "  0.31,\n",
       "  0.27,\n",
       "  0.26,\n",
       "  0.24,\n",
       "  0.48,\n",
       "  0.13,\n",
       "  0.36,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.3,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.3,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.13,\n",
       "  0.24,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.05,\n",
       "  0.06,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.25,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.25,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.26,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.12,\n",
       "  0.19,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.2,\n",
       "  0.04,\n",
       "  0.24,\n",
       "  0.07,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.14,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.39,\n",
       "  0.11,\n",
       "  0.19,\n",
       "  0.39,\n",
       "  0.37,\n",
       "  0.27,\n",
       "  0.2,\n",
       "  0.52,\n",
       "  0.07,\n",
       "  0.4,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.3,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.22,\n",
       "  0.07,\n",
       "  0.16,\n",
       "  0.11,\n",
       "  0.13,\n",
       "  0.19,\n",
       "  0.13,\n",
       "  0.19,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.19,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.07,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.17,\n",
       "  0.28,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.04,\n",
       "  0.15,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.25,\n",
       "  0.13,\n",
       "  0.32,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.09,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.19,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.15,\n",
       "  0.01,\n",
       "  0.24,\n",
       "  0.06,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.09,\n",
       "  0.16,\n",
       "  0.23,\n",
       "  0.05,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.04,\n",
       "  0.15,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.24,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.12,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.05,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.29,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.17,\n",
       "  0.05,\n",
       "  0.13,\n",
       "  0.02,\n",
       "  0.2,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.15,\n",
       "  0.23,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.2,\n",
       "  0.09,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.27,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.24,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.05,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.29,\n",
       "  0.06,\n",
       "  0.13,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.03,\n",
       "  0.18,\n",
       "  0.07,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.14,\n",
       "  0.24,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.04,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.04,\n",
       "  0.05,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.21,\n",
       "  0.27,\n",
       "  0.2,\n",
       "  0.29,\n",
       "  0.29,\n",
       "  0.17,\n",
       "  0.24,\n",
       "  0.16,\n",
       "  0.35,\n",
       "  0.24,\n",
       "  0.25,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.1,\n",
       "  0.23,\n",
       "  0.28,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.2,\n",
       "  0.09,\n",
       "  0.23,\n",
       "  0.07,\n",
       "  0.11,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.27,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.22,\n",
       "  0.08,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.25,\n",
       "  0.31,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.19,\n",
       "  0.16,\n",
       "  0.21,\n",
       "  0.27,\n",
       "  0.21,\n",
       "  0.32,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.21,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.2,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.14,\n",
       "  0.2,\n",
       "  0.27,\n",
       "  0.15,\n",
       "  0.2,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.19,\n",
       "  0.37,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.12,\n",
       "  0.22,\n",
       "  0.26,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.1,\n",
       "  0.23,\n",
       "  0.21,\n",
       "  0.34,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.03,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.08,\n",
       "  0.01,\n",
       "  0.1,\n",
       "  0.26,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.33,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.12,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.27,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.19,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.16,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.26,\n",
       "  0.26,\n",
       "  0.2,\n",
       "  0.12,\n",
       "  0.19,\n",
       "  0.16,\n",
       "  0.23,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.23,\n",
       "  0.25,\n",
       "  0.29,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.25,\n",
       "  0.17,\n",
       "  0.19,\n",
       "  0.25,\n",
       "  0.18,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.28,\n",
       "  0.18,\n",
       "  0.14,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.29,\n",
       "  0.27,\n",
       "  0.21,\n",
       "  0.12,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.23,\n",
       "  0.28,\n",
       "  0.13,\n",
       "  0.21,\n",
       "  0.24,\n",
       "  0.28,\n",
       "  0.17,\n",
       "  0.35,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.17,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.19,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.17,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.14,\n",
       "  0.05,\n",
       "  0.1,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.06,\n",
       "  0.03,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.23,\n",
       "  0.15,\n",
       "  0.25,\n",
       "  0.22,\n",
       "  0.3,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.1,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.29,\n",
       "  0.19,\n",
       "  0.24,\n",
       "  0.1,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.29,\n",
       "  0.18,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.19,\n",
       "  0.12,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.26,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.27,\n",
       "  0.27,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.17,\n",
       "  0.24,\n",
       "  0.25,\n",
       "  0.14,\n",
       "  ...],\n",
       " [0.21,\n",
       "  0.03,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.23,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.09,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.01,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.19,\n",
       "  0.16,\n",
       "  0.57,\n",
       "  0.05,\n",
       "  0.08,\n",
       "  0.24,\n",
       "  0.14,\n",
       "  0.04,\n",
       "  0.08,\n",
       "  0.24,\n",
       "  0.08,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.01,\n",
       "  0.04,\n",
       "  0.06,\n",
       "  0.04,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.02,\n",
       "  0.0,\n",
       "  0.01,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.04,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.23,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.03,\n",
       "  0.18,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.3,\n",
       "  0.14,\n",
       "  0.07,\n",
       "  0.11,\n",
       "  0.23,\n",
       "  0.18,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.03,\n",
       "  0.2,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.09,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.02,\n",
       "  0.42,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.11,\n",
       "  0.01,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.08,\n",
       "  0.17,\n",
       "  0.09,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.09,\n",
       "  0.18,\n",
       "  0.23,\n",
       "  0.05,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.11,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.25,\n",
       "  0.11,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.19,\n",
       "  0.14,\n",
       "  0.2,\n",
       "  0.06,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.1,\n",
       "  0.19,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.36,\n",
       "  0.13,\n",
       "  0.2,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.14,\n",
       "  0.27,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.21,\n",
       "  0.42,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.04,\n",
       "  0.24,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.23,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.22,\n",
       "  0.04,\n",
       "  0.06,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.23,\n",
       "  0.16,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.22,\n",
       "  0.15,\n",
       "  0.05,\n",
       "  0.04,\n",
       "  0.11,\n",
       "  0.02,\n",
       "  0.19,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.04,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.07,\n",
       "  0.02,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.26,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.08,\n",
       "  0.27,\n",
       "  0.15,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.01,\n",
       "  0.24,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.06,\n",
       "  0.22,\n",
       "  0.0,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.02,\n",
       "  0.02,\n",
       "  0.09,\n",
       "  0.04,\n",
       "  0.16,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.08,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.14,\n",
       "  0.06,\n",
       "  0.31,\n",
       "  0.15,\n",
       "  0.12,\n",
       "  0.19,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.04,\n",
       "  0.02,\n",
       "  0.01,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.02,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.09,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.04,\n",
       "  0.02,\n",
       "  0.06,\n",
       "  0.51,\n",
       "  0.32,\n",
       "  0.34,\n",
       "  0.48,\n",
       "  0.43,\n",
       "  0.44,\n",
       "  0.45,\n",
       "  0.39,\n",
       "  0.34,\n",
       "  0.31,\n",
       "  0.16,\n",
       "  0.28,\n",
       "  0.35,\n",
       "  0.18,\n",
       "  0.33,\n",
       "  0.25,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.18,\n",
       "  0.23,\n",
       "  0.33,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.35,\n",
       "  0.33,\n",
       "  0.19,\n",
       "  0.24,\n",
       "  0.16,\n",
       "  0.25,\n",
       "  0.15,\n",
       "  0.23,\n",
       "  0.31,\n",
       "  0.27,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.31,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.31,\n",
       "  0.19,\n",
       "  0.35,\n",
       "  0.23,\n",
       "  0.11,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.14,\n",
       "  0.09,\n",
       "  0.32,\n",
       "  0.43,\n",
       "  0.26,\n",
       "  0.24,\n",
       "  0.13,\n",
       "  0.24,\n",
       "  0.26,\n",
       "  0.18,\n",
       "  0.3,\n",
       "  0.17,\n",
       "  0.5,\n",
       "  0.26,\n",
       "  0.36,\n",
       "  0.13,\n",
       "  0.3,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.05,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.27,\n",
       "  0.23,\n",
       "  0.3,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.13,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.16,\n",
       "  0.08,\n",
       "  0.05,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.0,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.22,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.14,\n",
       "  0.19,\n",
       "  0.07,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.14,\n",
       "  0.07,\n",
       "  0.14,\n",
       "  0.11,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.12,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.06,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.15,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.14,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.1,\n",
       "  0.16,\n",
       "  0.12,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.01,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.07,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.25,\n",
       "  0.19,\n",
       "  0.23,\n",
       "  0.55,\n",
       "  0.25,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.34,\n",
       "  0.16,\n",
       "  0.06,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.04,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.27,\n",
       "  0.1,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.12,\n",
       "  0.07,\n",
       "  0.19,\n",
       "  0.25,\n",
       "  0.16,\n",
       "  0.31,\n",
       "  0.37,\n",
       "  0.17,\n",
       "  0.02,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.24,\n",
       "  0.2,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.26,\n",
       "  0.13,\n",
       "  0.06,\n",
       "  0.1,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.18,\n",
       "  0.02,\n",
       "  0.0,\n",
       "  0.01,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.08,\n",
       "  0.22,\n",
       "  0.33,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.06,\n",
       "  0.18,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.09,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.02,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.4,\n",
       "  0.06,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.09,\n",
       "  0.27,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.2,\n",
       "  0.03,\n",
       "  0.06,\n",
       "  0.22,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.07,\n",
       "  0.34,\n",
       "  0.31,\n",
       "  0.06,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.18,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.03,\n",
       "  0.09,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.0,\n",
       "  0.03,\n",
       "  0.03,\n",
       "  0.08,\n",
       "  0.03,\n",
       "  0.2,\n",
       "  0.01,\n",
       "  0.1,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.06,\n",
       "  0.02,\n",
       "  0.32,\n",
       "  0.38,\n",
       "  0.16,\n",
       "  0.26,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.06,\n",
       "  0.19,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.11,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.02,\n",
       "  0.1,\n",
       "  0.05,\n",
       "  0.23,\n",
       "  0.06,\n",
       "  0.23,\n",
       "  0.04,\n",
       "  0.13,\n",
       "  0.08,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.14,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.29,\n",
       "  0.13,\n",
       "  0.18,\n",
       "  0.29,\n",
       "  0.12,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.23,\n",
       "  0.06,\n",
       "  0.2,\n",
       "  0.16,\n",
       "  0.18,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.06,\n",
       "  0.26,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.18,\n",
       "  0.13,\n",
       "  0.1,\n",
       "  0.06,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.03,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.03,\n",
       "  0.14,\n",
       "  0.03,\n",
       "  0.52,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.08,\n",
       "  0.22,\n",
       "  0.32,\n",
       "  0.19,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.17,\n",
       "  0.09,\n",
       "  0.04,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.1,\n",
       "  0.06,\n",
       "  0.19,\n",
       "  0.11,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.22,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.29,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.05,\n",
       "  0.18,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.12,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.13,\n",
       "  0.09,\n",
       "  0.02,\n",
       "  0.06,\n",
       "  0.07,\n",
       "  0.1,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.05,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.17,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.09,\n",
       "  0.04,\n",
       "  0.22,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.25,\n",
       "  0.13,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.04,\n",
       "  0.08,\n",
       "  0.18,\n",
       "  0.07,\n",
       "  0.04,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.13,\n",
       "  0.03,\n",
       "  0.0,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.06,\n",
       "  0.14,\n",
       "  0.23,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.2,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.09,\n",
       "  0.02,\n",
       "  0.04,\n",
       "  0.17,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.1,\n",
       "  0.01,\n",
       "  0.07,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.11,\n",
       "  0.04,\n",
       "  0.16,\n",
       "  0.09,\n",
       "  0.17,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.06,\n",
       "  0.09,\n",
       "  0.06,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.03,\n",
       "  0.01,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.0,\n",
       "  0.06,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.05,\n",
       "  0.0,\n",
       "  0.01,\n",
       "  0.05,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.04,\n",
       "  0.01,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.03,\n",
       "  0.39,\n",
       "  0.28,\n",
       "  0.15,\n",
       "  0.25,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.24,\n",
       "  0.3,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.27,\n",
       "  0.14,\n",
       "  0.09,\n",
       "  0.21,\n",
       "  0.17,\n",
       "  0.22,\n",
       "  0.19,\n",
       "  0.08,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.29,\n",
       "  0.42,\n",
       "  0.16,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.27,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.36,\n",
       "  0.28,\n",
       "  0.14,\n",
       "  0.08,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.1,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.23,\n",
       "  0.13,\n",
       "  0.23,\n",
       "  0.32,\n",
       "  0.31,\n",
       "  0.29,\n",
       "  0.4,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.27,\n",
       "  0.2,\n",
       "  0.26,\n",
       "  0.03,\n",
       "  0.02,\n",
       "  0.03,\n",
       "  0.12,\n",
       "  0.08,\n",
       "  0.01,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.27,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.14,\n",
       "  0.32,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.18,\n",
       "  0.1,\n",
       "  0.26,\n",
       "  0.28,\n",
       "  0.48,\n",
       "  0.12,\n",
       "  0.17,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.3,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.06,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.21,\n",
       "  0.08,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.12,\n",
       "  0.29,\n",
       "  0.1,\n",
       "  0.24,\n",
       "  0.2,\n",
       "  0.33,\n",
       "  0.36,\n",
       "  0.24,\n",
       "  0.22,\n",
       "  0.1,\n",
       "  0.28,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.21,\n",
       "  0.36,\n",
       "  0.19,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.27,\n",
       "  0.21,\n",
       "  0.45,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.12,\n",
       "  0.15,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.15,\n",
       "  0.21,\n",
       "  0.35,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  ...])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a9eff-1680-45f9-b801-8de87e1b41eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T09:35:59.993812Z",
     "iopub.status.busy": "2023-02-25T09:35:59.993197Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_legit, scores_adv = original_restricted[1] #outputs[\"vae_gan_specific_50_gestures=2-adversarially_mixed-macro\"][1]\n",
    "eer = get_eer(scores_legit, scores_adv)\n",
    "far = get_far_when_zero_frr(scores_legit, scores_adv)\n",
    "display(sum(1 if e == 0 else 0 for e in scores_adv) / len(scores_adv))\n",
    "\n",
    "frr = sum(1 if e == 0 else 0 for e in scores_legit) / len(scores_legit)\n",
    "\n",
    "display(eer)\n",
    "display(far)\n",
    "display(frr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d65113-7ca1-49dd-9a47-33ce969f05d5",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521d596-e873-4793-9c29-8cf4409d3cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T13:18:04.564865Z",
     "iopub.status.busy": "2023-02-12T13:18:04.564541Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 13:18:05.633139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 13:18:06.612902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-12 13:18:06.613069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-12 13:18:06.613069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Permute, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from featurize import featurize\n",
    "from visualise import *\n",
    "\n",
    "from scaler import CustomScaler\n",
    "\n",
    "from VAE_stats import VAE_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c7aa83-7e0a-4b32-9933-dbd93fd26103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T13:18:01.421539Z",
     "iopub.status.busy": "2023-02-12T13:18:01.421105Z",
     "iopub.status.idle": "2023-02-12T13:18:01.890483Z",
     "shell.execute_reply": "2023-02-12T13:18:01.889365Z",
     "shell.execute_reply.started": "2023-02-12T13:18:01.421501Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39msetLevel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m device_name \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mgpu_device_name()\n\u001b[1;32m      5\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_with_maps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# or offsets_2\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "file_name = \"raw_with_maps\" # or offsets_2\n",
    "\n",
    "x_data = np.load(f\"data/processed/x_{file_name}_median_filtered.npy\")\n",
    "\n",
    "#from featurize import filter as _filter, ORDER, CUTOFF, FILTER_INDICES\n",
    "#x_data = _filter(x_data, FILTER_INDICES, ORDER, CUTOFF)\n",
    "\n",
    "y_user = np.load(f\"data/processed/y_user_{file_name}.npy\")\n",
    "y_intent = np.load(f\"data/processed/y_intent_{file_name}.npy\").astype(int)\n",
    "y_gesture = np.load(f\"data/processed/y_gesture_type_{file_name}.npy\")\n",
    "\n",
    "train_gesture_map = np.load(f\"data/processed/train_gesture_map_{file_name}.npy\").astype(int)\n",
    "test_gesture_map = np.load(f\"data/processed/test_gesture_map_{file_name}.npy\").astype(int)\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "_map = (train_gesture_map == 1) #& ((y_user.argmax(axis=1) >= 11))\n",
    "\n",
    "user_x = x_data[_map]\n",
    "user_y = y_user.argmax(axis=1)[_map]\n",
    "\n",
    "def show_visuals(vae):\n",
    "    \n",
    "    \n",
    "    latent_space_means, latent_space_stds, latent_space = vae.encoder(vae.scaler.transform(x_data[:10]))\n",
    "\n",
    "    plt.scatter(latent_space_means[:,0], latent_space_means[:,1], c = range(10), label=\"means\")\n",
    "    plt.scatter(latent_space[:,0], latent_space[:,1], label=\"sampled\", c = range(10), s=10)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    plot_label_clusters(vae, vae.scaler.transform(user_x), user_y)\n",
    "    visualise(vae, vae.scaler.transform(user_x), user_y, 10)\n",
    "    plot_reconstructed_curves(vae, vae.scaler.transform(user_x), channel=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7537fabc-4cbe-4748-8200-e2995a0ed4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T00:07:16.579015Z",
     "iopub.status.busy": "2023-02-12T00:07:16.578645Z",
     "iopub.status.idle": "2023-02-12T00:07:44.238301Z",
     "shell.execute_reply": "2023-02-12T00:07:44.237135Z",
     "shell.execute_reply.started": "2023-02-12T00:07:16.578989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144,)\n",
      "0\n",
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n",
      "2176\n",
      "2304\n",
      "2432\n",
      "2560\n",
      "2688\n",
      "2816\n",
      "2944\n",
      "3072\n",
      "3200\n",
      "3328\n",
      "3456\n",
      "3584\n",
      "3712\n",
      "3840\n",
      "3968\n",
      "4096\n",
      "4224\n",
      "4352\n",
      "4480\n",
      "4608\n",
      "4736\n",
      "4864\n",
      "4992\n",
      "5120\n",
      "5248\n",
      "5376\n",
      "5504\n",
      "5632\n",
      "5760\n",
      "5888\n",
      "6016\n",
      "6144\n",
      "6272\n",
      "6400\n",
      "6528\n",
      "6656\n",
      "6784\n",
      "6912\n",
      "7040\n",
      "7168\n",
      "7296\n",
      "7424\n",
      "7552\n",
      "7680\n",
      "7808\n",
      "7936\n",
      "8064\n",
      "8192\n",
      "8320\n",
      "8448\n",
      "8576\n",
      "8704\n",
      "8832\n",
      "8960\n",
      "9088\n",
      "9216\n",
      "9344\n",
      "9472\n",
      "9600\n",
      "9728\n",
      "9856\n",
      "9984\n",
      "10112\n",
      "10240\n",
      "10368\n",
      "10496\n",
      "10624\n",
      "10752\n",
      "10880\n",
      "11008\n",
      "11136\n",
      "11264\n",
      "11392\n",
      "11520\n",
      "11648\n",
      "11776\n",
      "11904\n",
      "12032\n",
      "12160\n",
      "12288\n",
      "12416\n",
      "12544\n",
      "12672\n",
      "12800\n",
      "12928\n",
      "13056\n",
      "13184\n",
      "13312\n",
      "13440\n",
      "13568\n",
      "13696\n",
      "13824\n",
      "13952\n",
      "14080\n",
      "14208\n",
      "14336\n",
      "14464\n",
      "14592\n",
      "14720\n",
      "14848\n",
      "14976\n",
      "15104\n",
      "15232\n",
      "15360\n",
      "15488\n",
      "15616\n",
      "15744\n",
      "15872\n",
      "16000\n",
      "16128\n",
      "16256\n",
      "16384\n",
      "16512\n",
      "16640\n",
      "16768\n",
      "16896\n",
      "17024\n",
      "17152\n",
      "17280\n",
      "17408\n",
      "17536\n",
      "17664\n",
      "17792\n",
      "17920\n",
      "18048\n",
      "18176\n",
      "18304\n",
      "18432\n",
      "18560\n",
      "18688\n",
      "18816\n",
      "18944\n",
      "19072\n",
      "19200\n",
      "19328\n",
      "19456\n",
      "19584\n",
      "19712\n",
      "19840\n",
      "19968\n",
      "20096\n",
      "20224\n",
      "20352\n",
      "20480\n",
      "20608\n",
      "20736\n",
      "20864\n",
      "20992\n",
      "21120\n",
      "21248\n",
      "21376\n",
      "21504\n",
      "21632\n",
      "21760\n",
      "21888\n",
      "22016\n",
      "22144\n",
      "22272\n",
      "22400\n",
      "22528\n",
      "22656\n",
      "22784\n",
      "22912\n",
      "23040\n",
      "23168\n",
      "23296\n",
      "23424\n",
      "23552\n",
      "23680\n",
      "23808\n",
      "23936\n",
      "24064\n",
      "24192\n",
      "24320\n",
      "24448\n",
      "24576\n",
      "24704\n",
      "24832\n",
      "24960\n",
      "25088\n",
      "25216\n",
      "25344\n",
      "25472\n",
      "25600\n",
      "25728\n",
      "25856\n",
      "25984\n",
      "26112\n",
      "26240\n",
      "26368\n",
      "26496\n",
      "26624\n",
      "26752\n",
      "26880\n",
      "27008\n",
      "27136\n",
      "27264\n",
      "27392\n",
      "27520\n",
      "27648\n",
      "27776\n",
      "27904\n",
      "28032\n",
      "28160\n",
      "28288\n",
      "28416\n",
      "28544\n",
      "28672\n",
      "28800\n",
      "28928\n",
      "29056\n",
      "29184\n",
      "29312\n",
      "29440\n",
      "29568\n",
      "29696\n",
      "29824\n",
      "29952\n",
      "30080\n",
      "30208\n",
      "30336\n",
      "30464\n",
      "30592\n",
      "30720\n",
      "30848\n",
      "30976\n",
      "31104\n",
      "31232\n",
      "31360\n",
      "31488\n",
      "31616\n",
      "31744\n",
      "31872\n",
      "32000\n",
      "32128\n",
      "32256\n",
      "32384\n",
      "32512\n",
      "32640\n",
      "32768\n",
      "32896\n",
      "33024\n",
      "33152\n",
      "33280\n",
      "33408\n",
      "33536\n",
      "33664\n",
      "33792\n",
      "33920\n",
      "34048\n",
      "34176\n"
     ]
    }
   ],
   "source": [
    "vae = VAE_stats(50)\n",
    "\n",
    "x_feature_array = []\n",
    "print(vae.vae_featurize(x_data[0:1])[0].shape)\n",
    "\n",
    "for i in range(0, len(x_data), 128):\n",
    "    print(i)\n",
    "    x_feature_array.append(vae.vae_featurize(x_data[i:i+128]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5677b9f2-55b4-4e4c-9235-dbcb88db0340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T00:08:49.664121Z",
     "iopub.status.busy": "2023-02-12T00:08:49.663139Z",
     "iopub.status.idle": "2023-02-12T00:08:49.842942Z",
     "shell.execute_reply": "2023-02-12T00:08:49.842043Z",
     "shell.execute_reply.started": "2023-02-12T00:08:49.664088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 144])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34255, 144)\n"
     ]
    }
   ],
   "source": [
    "display(x_feature_array[0].shape)\n",
    "x_features = np.concatenate(x_feature_array, axis=0)\n",
    "print(x_features.shape)\n",
    "np.save(f\"data/processed/x_{file_name}_median_filtered_features.npy\", x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd90de0-75c8-491f-bbe4-336658de9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for latent_dim in [50]: # , 30, 10, 100]: # 500, 50, 40, 30, 20, 10\n",
    "\n",
    "#latent_dim = 100\n",
    "    \"\"\"\n",
    "    vae = VAE_stats(latent_dim)\n",
    "    vae.fit_scaler(x_data[(y_intent==0)])\n",
    "    vae.auth_on = False\n",
    "    \n",
    "    show_visuals(vae)\n",
    "\n",
    "    initial_map = (train_gesture_map==1) | (y_intent == 0)\n",
    "    train_data = vae.scaler.transform(x_data[initial_map])\n",
    "\n",
    "    print(\"Fitting basic\")\n",
    "\n",
    "    vae.beta = 1e-4\n",
    "    vae.compile(Adam())\n",
    "    history_1 = vae.fit(train_data, y_user[initial_map], epochs=15, batch_size=128, verbose=1)\n",
    "    vae.save_model(f\"all_users\", f\"vae_stats_basic_reconstruction_{latent_dim}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    specific_map = (train_gesture_map==1) \n",
    "    vae = VAE_stats(latent_dim)\n",
    "    vae.load_model(f\"all_users\", f\"vae_stats_basic_reconstruction_{latent_dim}\")\n",
    "\n",
    "    show_visuals(vae)\n",
    "    \n",
    "    beta = 1e-4\n",
    "    vae.beta = beta\n",
    "    vae.alpha = 0.01\n",
    "    vae.delta = 0.11\n",
    "    vae.auth_on = True\n",
    "\n",
    "    vae.compile(Adam())\n",
    "\n",
    "    epochs = 5\n",
    "    train_data = vae.scaler.transform(x_data[specific_map])\n",
    "\n",
    "    for i in range(10):\n",
    "        print(\"Fitting specific\")\n",
    "\n",
    "        vae.fit(train_data, y_user[specific_map], epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "        show_visuals(vae)\n",
    "    \"\"\"\n",
    "    beta = 1e-2\n",
    "    vae.beta = beta\n",
    "    vae.alpha = 0.5\n",
    "    vae.auth_on = True\n",
    "    print(\"Fitting specific - higher auth\")\n",
    "    \n",
    "    vae.fit(train_data, y_user[specific_map], epochs=1000, batch_size=128, verbose=1)\n",
    "    \n",
    "    show_visuals(vae)\n",
    "    \"\"\"\n",
    "    \n",
    "    vae.save_model(f\"all_users\", f\"vae_stats_specific_v2_{latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6216e3-8b87-4861-bc0e-9acc3e769f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_max(g_data):\n",
    "\treturn tf.math.reduce_max(g_data, axis=1)\n",
    " \n",
    "def feature_min(g_data):\n",
    "\treturn tf.math.reduce_min(g_data, axis=1)\n",
    " \n",
    "\n",
    "def feature_mean(g_data):\n",
    "\treturn tf.math.reduce_mean(g_data, axis=1)\n",
    "    \n",
    "def feature_stdev(g_data):\n",
    "\treturn tf.math.reduce_std(g_data, axis=1)\n",
    "\n",
    "def feature_var(g_data):\n",
    "\treturn tf.math.reduce_variance(g_data, axis=1)\n",
    "\n",
    "def feature_median(g_data):\n",
    "    return tfp.stats.percentile(g_data, 50.0, interpolation='midpoint', axis=1)\n",
    "\n",
    "def feature_iqr(g_data):\n",
    "    return tfp.stats.percentile(g_data, 75.0, interpolation='midpoint', axis=1) - tfp.stats.percentile(g_data, 25.0, interpolation='midpoint', axis=1)\n",
    "\n",
    "def feature_kurt(g_data):\n",
    "    std = tf.math.reduce_std(g_data, axis=1)\n",
    "    diff = tf.math.reduce_mean((g_data - tf.math.reduce_mean(g_data, axis=1, keepdims = True))**4, axis=1)\n",
    "    return diff / std**4\n",
    "\n",
    "def feature_skew(g_data):\n",
    "    std = tf.math.reduce_std(g_data, axis=1)\n",
    "    diff = tf.math.reduce_mean((g_data - tf.math.reduce_mean(g_data, axis=1, keepdims = True))**3, axis=1)\n",
    "    return diff / std**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd78d7-a567-4434-8155-d5fd8bab9c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d5758ec-ac81-4234-8bdb-d26e5c1fae20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T11:39:56.834332Z",
     "iopub.status.busy": "2023-02-11T11:39:56.833418Z",
     "iopub.status.idle": "2023-02-11T11:39:57.330472Z",
     "shell.execute_reply": "2023-02-11T11:39:57.329757Z",
     "shell.execute_reply.started": "2023-02-11T11:39:56.834263Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"raw_with_maps\" # or offsets_2\n",
    "\n",
    "x_data = np.load(f\"data/processed/x_{file_name}_filtered.npy\")\n",
    "y_user = np.load(f\"data/processed/y_user_{file_name}.npy\")\n",
    "y_intent = np.load(f\"data/processed/y_intent_{file_name}.npy\").astype(int)\n",
    "y_gesture = np.load(f\"data/processed/y_gesture_type_{file_name}.npy\")\n",
    "\n",
    "train_gesture_map = np.load(f\"data/processed/train_gesture_map_{file_name}.npy\").astype(int)\n",
    "\n",
    "test_gesture_map = np.load(f\"data/processed/test_gesture_map_{file_name}.npy\").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c69fa09-68e5-4b67-95ab-347dd0ab2766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T11:39:57.332324Z",
     "iopub.status.busy": "2023-02-11T11:39:57.331541Z",
     "iopub.status.idle": "2023-02-11T11:39:57.335233Z",
     "shell.execute_reply": "2023-02-11T11:39:57.334708Z",
     "shell.execute_reply.started": "2023-02-11T11:39:57.332297Z"
    }
   },
   "outputs": [],
   "source": [
    "trial_data = x_data[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44726c5f-bd19-4d04-8e76-8a55d9e294f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T11:39:58.201934Z",
     "iopub.status.busy": "2023-02-11T11:39:58.201038Z",
     "iopub.status.idle": "2023-02-11T11:39:58.207615Z",
     "shell.execute_reply": "2023-02-11T11:39:58.206842Z",
     "shell.execute_reply.started": "2023-02-11T11:39:58.201902Z"
    }
   },
   "outputs": [],
   "source": [
    "data = tf.convert_to_tensor(trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72c34f10-1a50-40c2-a7b7-c7087bb5b075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T11:39:58.814629Z",
     "iopub.status.busy": "2023-02-11T11:39:58.813643Z",
     "iopub.status.idle": "2023-02-11T11:39:58.861878Z",
     "shell.execute_reply": "2023-02-11T11:39:58.861304Z",
     "shell.execute_reply.started": "2023-02-11T11:39:58.814600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 144), dtype=float64, numpy=\n",
       "array([[ 1.17380573, -2.23082167,  9.58853584, ..., -0.381646  ,\n",
       "         0.28685247,  0.36725875],\n",
       "       [ 0.22855253, -2.97880558,  9.11028331, ..., -0.34530999,\n",
       "        -0.44720132,  0.3219769 ],\n",
       "       [ 0.71983189, -2.99275854,  9.2993504 , ...,  0.83732114,\n",
       "        -1.17741003,  0.83818612],\n",
       "       ...,\n",
       "       [ 3.18385887, -2.67086618, 10.53634914, ..., -1.32756023,\n",
       "        -0.24308826,  1.43226517],\n",
       "       [ 0.35267441, -4.88235892,  8.34485913, ..., -1.86707896,\n",
       "        -0.75049372,  0.55233989],\n",
       "       [ 1.19715888, -0.99358881,  9.36356918, ...,  0.35623664,\n",
       "        -1.63732195,  0.62811479]])>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_max = feature_max(data)\n",
    "_min = feature_min(data)\n",
    "_mean = feature_mean(data)\n",
    "_std = feature_stdev(data)\n",
    "_var = feature_var(data)\n",
    "_median = feature_median(data)\n",
    "_iqr = feature_iqr(data)\n",
    "_kurt = feature_kurt(data)\n",
    "_skew = feature_skew(data)\n",
    "\n",
    "features = tf.concat([_max, _min, _mean, _std, _var, _median, _iqr, _kurt, _skew], axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c163add1-3351-4f44-8e02-a8b8b59a19b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T11:29:13.416424Z",
     "iopub.status.busy": "2023-02-11T11:29:13.415657Z",
     "iopub.status.idle": "2023-02-11T11:29:13.457685Z",
     "shell.execute_reply": "2023-02-11T11:29:13.456891Z",
     "shell.execute_reply.started": "2023-02-11T11:29:13.416393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 200, 16), dtype=float64, numpy=\n",
       "array([[[ 1.17380573e+00, -2.58720799e+00,  9.58853584e+00, ...,\n",
       "         -6.96151705e+00,  2.23089887e+00,  8.14994100e+00],\n",
       "        [ 1.13823141e+00, -2.53876070e+00,  9.56722722e+00, ...,\n",
       "         -6.83049185e+00,  2.55111246e+00,  8.10702500e+00],\n",
       "        [ 1.10481621e+00, -2.49456617e+00,  9.54612329e+00, ...,\n",
       "         -6.69454243e+00,  2.84177267e+00,  8.31343100e+00],\n",
       "        ...,\n",
       "        [-1.01573015e+00, -3.91179250e+00, -8.92585399e+00, ...,\n",
       "          1.97838667e+00, -1.64349376e+00,  3.28856500e+00],\n",
       "        [-1.01526334e+00, -3.92784447e+00, -8.91247754e+00, ...,\n",
       "          1.93335272e+00, -1.58150781e+00,  3.27801900e+00],\n",
       "        [-1.01411311e+00, -3.94542381e+00, -8.89980296e+00, ...,\n",
       "          1.88489797e+00, -1.52064250e+00,  2.98492300e+00]],\n",
       "\n",
       "       [[ 2.25204930e-02, -6.63771436e+00,  9.11028331e+00, ...,\n",
       "         -3.09413823e+00, -3.00602563e-02,  3.10295400e+00],\n",
       "        [ 4.74700950e-02, -6.27461778e+00,  8.87753813e+00, ...,\n",
       "         -2.47424009e+00, -1.46404460e-01,  2.28055800e+00],\n",
       "        [ 7.19401913e-02, -5.93234289e+00,  8.66005963e+00, ...,\n",
       "         -1.88979127e+00, -2.53946643e-01,  1.32082400e+00],\n",
       "        ...,\n",
       "        [-6.12575933e-01, -3.33136459e+00, -9.27924750e+00, ...,\n",
       "          2.05792805e+00, -1.33893971e+00,  2.59747600e+00],\n",
       "        [-6.32011076e-01, -3.19243222e+00, -9.31138777e+00, ...,\n",
       "          2.17956412e+00, -1.34619934e+00,  2.52910500e+00],\n",
       "        [-6.49769215e-01, -3.04329079e+00, -9.34146060e+00, ...,\n",
       "          2.31069137e+00, -1.35151557e+00,  2.94157600e+00]],\n",
       "\n",
       "       [[ 7.19831892e-01, -4.45346835e+00,  8.71433278e+00, ...,\n",
       "          3.89427832e-04,  1.28939757e-02,  7.28690000e-02],\n",
       "        [ 7.18327504e-01, -4.48169703e+00,  8.74563834e+00, ...,\n",
       "         -2.67822023e-02,  4.43767758e-02,  2.80411000e-01],\n",
       "        [ 7.15243066e-01, -4.50705953e+00,  8.77414530e+00, ...,\n",
       "         -5.22551378e-02,  7.25625677e-02,  2.98288000e-01],\n",
       "        ...,\n",
       "        [-1.21830119e+00, -4.36274150e+00, -8.74722645e+00, ...,\n",
       "          3.23262372e+00, -3.00462592e+00,  5.63621400e+00],\n",
       "        [-1.20992019e+00, -4.37963849e+00, -8.75265804e+00, ...,\n",
       "          3.22062468e+00, -3.06755027e+00,  5.73768300e+00],\n",
       "        [-1.20064566e+00, -4.39590030e+00, -8.75890523e+00, ...,\n",
       "          3.20960715e+00, -3.13302828e+00,  5.87600900e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.66061221e+00, -6.77443774e+00,  6.97712052e+00, ...,\n",
       "         -8.40112554e-01, -7.00590377e-01,  1.37792600e+00],\n",
       "        [-1.68620514e+00, -6.75037293e+00,  7.00365696e+00, ...,\n",
       "         -7.11723056e-01, -6.06527029e-01,  1.27447200e+00],\n",
       "        [-1.70946726e+00, -6.72971868e+00,  7.02977322e+00, ...,\n",
       "         -5.92145951e-01, -5.17538182e-01,  1.02835700e+00],\n",
       "        ...,\n",
       "        [ 4.96429561e-02, -5.65578082e+00, -8.11362505e+00, ...,\n",
       "         -8.16132845e+00, -1.59077307e+00,  1.08246310e+01],\n",
       "        [ 6.84508753e-02, -5.65705175e+00, -8.10117287e+00, ...,\n",
       "         -7.89379789e+00, -1.38373360e+00,  1.05790630e+01],\n",
       "        [ 8.91829164e-02, -5.65658727e+00, -8.08346804e+00, ...,\n",
       "         -7.57737620e+00, -1.15256806e+00,  1.01256830e+01]],\n",
       "\n",
       "       [[-1.36524716e+00, -6.51520324e+00,  6.94554643e+00, ...,\n",
       "          1.31779993e+00,  2.72489056e+00,  3.83097200e+00],\n",
       "        [-1.34712336e+00, -6.44681506e+00,  7.05752419e+00, ...,\n",
       "          1.66473747e+00,  2.67475065e+00,  4.14460000e+00],\n",
       "        [-1.32679680e+00, -6.36624757e+00,  7.16689086e+00, ...,\n",
       "          1.99388850e+00,  2.63029818e+00,  4.33473800e+00],\n",
       "        ...,\n",
       "        [-1.82850541e-01, -6.13511008e+00, -8.01350804e+00, ...,\n",
       "         -6.37891371e+00,  1.38905594e-01,  8.74921200e+00],\n",
       "        [-1.97595467e-01, -6.37431879e+00, -7.97669779e+00, ...,\n",
       "         -6.63082356e+00,  1.28592424e-01,  8.97896900e+00],\n",
       "        [-2.12382672e-01, -6.62752480e+00, -7.93396025e+00, ...,\n",
       "         -6.88873576e+00,  1.29507341e-01,  8.91073200e+00]],\n",
       "\n",
       "       [[-7.61617794e-01, -3.25622822e+00,  8.94263260e+00, ...,\n",
       "         -7.46379202e-01, -4.32549456e-01,  1.01318300e+00],\n",
       "        [-7.48652445e-01, -3.49484867e+00,  8.99724583e+00, ...,\n",
       "         -8.51340197e-01, -3.55070421e-01,  1.29035400e+00],\n",
       "        [-7.34281668e-01, -3.74045117e+00,  9.02861518e+00, ...,\n",
       "         -9.51875027e-01, -2.99087635e-01,  1.46632000e+00],\n",
       "        ...,\n",
       "        [ 1.63829499e-01, -5.97735991e+00, -7.89611680e+00, ...,\n",
       "         -4.82061502e+00,  1.79151482e+00,  5.25323000e+00],\n",
       "        [ 1.67591258e-01, -5.97318293e+00, -7.94196965e+00, ...,\n",
       "         -4.83082236e+00,  1.75956814e+00,  5.05133600e+00],\n",
       "        [ 1.69665615e-01, -5.97085577e+00, -7.98918209e+00, ...,\n",
       "         -4.84465558e+00,  1.72611923e+00,  5.22683100e+00]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 1, 16), dtype=float64, numpy=\n",
       "array([[[-0.10715186, -4.0124292 ,  1.65174318, ..., -0.36785356,\n",
       "         -1.12441811,  4.17918539]],\n",
       "\n",
       "       [[-0.49439555, -4.85515248,  0.13258025, ...,  0.79979367,\n",
       "         -1.6168396 ,  3.31411813]],\n",
       "\n",
       "       [[-0.53934345, -4.90105357,  3.9796861 , ...,  0.77064798,\n",
       "         -1.05054327,  2.01857928]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.94539079, -6.5275634 ,  3.96524293, ..., -0.81100671,\n",
       "         -0.32052866,  3.02705698]],\n",
       "\n",
       "       [[-0.95769577, -6.32157963,  3.84019919, ...,  0.17634873,\n",
       "          0.41917022,  2.97261469]],\n",
       "\n",
       "       [[-0.07504546, -8.18444418, -0.8867777 , ..., -0.96772822,\n",
       "          0.32706953,  3.49886675]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 200, 16), dtype=float64, numpy=\n",
       "array([[[  1.28095759,   1.4252212 ,   7.93679266, ...,  -6.59366349,\n",
       "           3.35531698,   3.97075561],\n",
       "        [  1.24538327,   1.4736685 ,   7.91548404, ...,  -6.46263829,\n",
       "           3.67553056,   3.92783961],\n",
       "        [  1.21196808,   1.51786303,   7.89438011, ...,  -6.32668886,\n",
       "           3.96619077,   4.13424561],\n",
       "        ...,\n",
       "        [ -0.90857829,   0.1006367 , -10.57759717, ...,   2.34624024,\n",
       "          -0.51907565,  -0.89062039],\n",
       "        [ -0.90811147,   0.08458473, -10.56422072, ...,   2.30120629,\n",
       "          -0.4570897 ,  -0.90116639],\n",
       "        [ -0.90696125,   0.06700538, -10.55154614, ...,   2.25275153,\n",
       "          -0.39622439,  -1.19426239]],\n",
       "\n",
       "       [[  0.51691604,  -1.78256188,   8.97770306, ...,  -3.89393189,\n",
       "           1.58677934,  -0.21116413],\n",
       "        [  0.54186564,  -1.4194653 ,   8.74495788, ...,  -3.27403376,\n",
       "           1.47043514,  -1.03356013],\n",
       "        [  0.56633574,  -1.07719041,   8.52747938, ...,  -2.68958493,\n",
       "           1.36289295,  -1.99329413],\n",
       "        ...,\n",
       "        [ -0.11818038,   1.52378789,  -9.41182774, ...,   1.25813438,\n",
       "           0.27789988,  -0.71664213],\n",
       "        [ -0.13761553,   1.66272026,  -9.44396802, ...,   1.37977046,\n",
       "           0.27064025,  -0.78501313],\n",
       "        [ -0.15537367,   1.81186168,  -9.47404085, ...,   1.5108977 ,\n",
       "           0.26532403,  -0.37254213]],\n",
       "\n",
       "       [[  1.25917534,   0.44758522,   4.73464668, ...,  -0.77025855,\n",
       "           1.06343724,  -1.94571028],\n",
       "        [  1.25767095,   0.41935654,   4.76595223, ...,  -0.79743018,\n",
       "           1.09492004,  -1.73816828],\n",
       "        [  1.25458651,   0.39399405,   4.7944592 , ...,  -0.82290312,\n",
       "           1.12310583,  -1.72029128],\n",
       "        ...,\n",
       "        [ -0.67895775,   0.53831208, -12.72691255, ...,   2.46197574,\n",
       "          -1.95408265,   3.61763472],\n",
       "        [ -0.67057674,   0.52141508, -12.73234414, ...,   2.4499767 ,\n",
       "          -2.01700701,   3.71910372],\n",
       "        [ -0.66130222,   0.50515328, -12.73859133, ...,   2.43895917,\n",
       "          -2.08248501,   3.85742972]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -0.71522143,  -0.24687434,   3.01187759, ...,  -0.02910585,\n",
       "          -0.38006172,  -1.64913098],\n",
       "        [ -0.74081436,  -0.22280953,   3.03841403, ...,   0.09928365,\n",
       "          -0.28599837,  -1.75258498],\n",
       "        [ -0.76407647,  -0.20215527,   3.06453029, ...,   0.21886075,\n",
       "          -0.19700952,  -1.99869998],\n",
       "        ...,\n",
       "        [  0.99503374,   0.87178259, -12.07886798, ...,  -7.35032175,\n",
       "          -1.27024441,   7.79757402],\n",
       "        [  1.01384166,   0.87051166, -12.0664158 , ...,  -7.08279118,\n",
       "          -1.06320494,   7.55200601],\n",
       "        [  1.0345737 ,   0.87097613, -12.04871096, ...,  -6.76636949,\n",
       "          -0.8320394 ,   7.09862602]],\n",
       "\n",
       "       [[ -0.40755139,  -0.1936236 ,   3.10534725, ...,   1.14145121,\n",
       "           2.30572034,   0.85835731],\n",
       "        [ -0.3894276 ,  -0.12523542,   3.217325  , ...,   1.48838874,\n",
       "           2.25558043,   1.1719853 ],\n",
       "        [ -0.36910103,  -0.04466794,   3.32669167, ...,   1.81753978,\n",
       "           2.21112795,   1.3621233 ],\n",
       "        ...,\n",
       "        [  0.77484522,   0.18646955, -11.85370723, ...,  -6.55526244,\n",
       "          -0.28026463,   5.77659731],\n",
       "        [  0.7601003 ,  -0.05273916, -11.81689698, ...,  -6.80717228,\n",
       "          -0.2905778 ,   6.0063543 ],\n",
       "        [  0.74531309,  -0.30594517, -11.77415944, ...,  -7.06508449,\n",
       "          -0.28966288,   5.9381173 ]],\n",
       "\n",
       "       [[ -0.68657233,   4.92821596,   9.8294103 , ...,   0.22134901,\n",
       "          -0.75961899,  -2.48568375],\n",
       "        [ -0.67360698,   4.68959551,   9.88402352, ...,   0.11638802,\n",
       "          -0.68213995,  -2.20851275],\n",
       "        [ -0.65923621,   4.44399302,   9.91539288, ...,   0.01585319,\n",
       "          -0.62615717,  -2.03254675],\n",
       "        ...,\n",
       "        [  0.23887496,   2.20708427,  -7.0093391 , ...,  -3.8528868 ,\n",
       "           1.46444529,   1.75436325],\n",
       "        [  0.24263672,   2.21126125,  -7.05519195, ...,  -3.86309414,\n",
       "           1.4324986 ,   1.55246925],\n",
       "        [  0.24471108,   2.21358841,  -7.1024044 , ...,  -3.87692736,\n",
       "           1.3990497 ,   1.72796425]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 200, 16), dtype=float64, numpy=\n",
       "array([[[2.69239646e+00, 4.12599882e+00, 3.96807745e+03, ...,\n",
       "         1.89019720e+03, 1.26745987e+02, 2.48595140e+02],\n",
       "        [2.40553738e+00, 4.71627587e+00, 3.92563494e+03, ...,\n",
       "         1.74437317e+03, 1.82507264e+02, 2.38020840e+02],\n",
       "        [2.15756914e+00, 5.30799291e+00, 3.88393663e+03, ...,\n",
       "         1.60216009e+03, 2.47453955e+02, 2.92136013e+02],\n",
       "        ...,\n",
       "        [6.81474197e-01, 1.02571218e-04, 1.25183790e+04, ...,\n",
       "         3.03032991e+01, 7.25976608e-02, 6.29173649e-01],\n",
       "        [6.80074755e-01, 5.11879629e-05, 1.24551759e+04, ...,\n",
       "         2.80428538e+01, 4.36521619e-02, 6.59507796e-01],\n",
       "        [6.76635732e-01, 2.01575980e-05, 1.23955103e+04, ...,\n",
       "         2.57545031e+01, 2.46470435e-02, 2.03422513e+00]],\n",
       "\n",
       "       [[7.13970124e-02, 1.00966770e+01, 6.49622335e+03, ...,\n",
       "         2.29907641e+02, 6.33966264e+00, 1.98829376e-03],\n",
       "        [8.62117505e-02, 4.05974845e+00, 5.84831677e+03, ...,\n",
       "         1.14903329e+02, 4.67502014e+00, 1.14115065e+00],\n",
       "        [1.02871691e-01, 1.34638703e+00, 5.28789365e+03, ...,\n",
       "         5.23288333e+01, 3.45022155e+00, 1.57864888e+01],\n",
       "        ...,\n",
       "        [1.95066008e-04, 5.39135678e+00, 7.84685957e+03, ...,\n",
       "         2.50557915e+00, 5.96421741e-03, 2.63760241e-01],\n",
       "        [3.58649113e-04, 7.64322695e+00, 7.95459423e+03, ...,\n",
       "         3.62432694e+00, 5.36499814e-03, 3.79758647e-01],\n",
       "        [5.82786740e-04, 1.07770566e+01, 8.05639984e+03, ...,\n",
       "         5.21123004e+00, 4.95571517e-03, 1.92620076e-02]],\n",
       "\n",
       "       [[2.51388169e+00, 4.01331228e-02, 5.02516473e+02, ...,\n",
       "         3.52002797e-01, 1.27893202e+00, 1.43321947e+01],\n",
       "        [2.50188946e+00, 3.09267085e-02, 5.15939457e+02, ...,\n",
       "         4.04362317e-01, 1.43724107e+00, 9.12782459e+00],\n",
       "        [2.47743607e+00, 2.40967589e-02, 5.28394767e+02, ...,\n",
       "         4.58558587e-01, 1.59104598e+00, 8.75806082e+00],\n",
       "        ...,\n",
       "        [2.12505903e-01, 8.39723859e-02, 2.62356748e+04, ...,\n",
       "         3.67396555e+01, 1.45804765e+01, 1.71276922e+02],\n",
       "        [2.02205958e-01, 7.39153027e-02, 2.62804909e+04, ...,\n",
       "         3.60286359e+01, 1.65512054e+01, 1.91316822e+02],\n",
       "        [1.91249329e-01, 6.51167468e-02, 2.63321076e+04, ...,\n",
       "         3.53849118e+01, 1.88073470e+01, 2.21407377e+02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.61674904e-01, 3.71452939e-03, 8.22904183e+01, ...,\n",
       "         7.17663841e-07, 2.08649102e-02, 7.39640369e+00],\n",
       "        [3.01187929e-01, 2.46453535e-03, 8.52290823e+01, ...,\n",
       "         9.71652430e-05, 6.69043329e-03, 9.43444477e+00],\n",
       "        [3.40837454e-01, 1.67009167e-03, 8.81973779e+01, ...,\n",
       "         2.29441287e-03, 1.50642979e-03, 1.59584401e+01],\n",
       "        ...,\n",
       "        [9.80282458e-01, 5.77607415e-01, 2.12865333e+04, ...,\n",
       "         2.91894156e+03, 2.60344956e+00, 3.69690272e+03],\n",
       "        [1.05652684e+00, 5.74246507e-01, 2.11988912e+04, ...,\n",
       "         2.51662063e+03, 1.27781488e+00, 3.25273969e+03],\n",
       "        [1.14563359e+00, 5.75473082e-01, 2.10747458e+04, ...,\n",
       "         2.09615232e+03, 4.79264840e-01, 2.53920161e+03]],\n",
       "\n",
       "       [[2.75885919e-02, 1.40550760e-03, 9.29909499e+01, ...,\n",
       "         1.69757670e+00, 2.82635378e+01, 5.42840735e-01],\n",
       "        [2.29988908e-02, 2.45985073e-04, 1.07146930e+02, ...,\n",
       "         4.90755872e+00, 2.58841119e+01, 1.88663838e+00],\n",
       "        [1.85601315e-02, 3.98092188e-06, 1.22475777e+02, ...,\n",
       "         1.09127874e+01, 2.39031700e+01, 3.44243457e+00],\n",
       "        ...,\n",
       "        [3.60462294e-01, 1.20901511e-03, 1.97431655e+04, ...,\n",
       "         1.84654686e+03, 6.16982954e-03, 1.11349526e+03],\n",
       "        [3.33797910e-01, 7.73629027e-06, 1.94990655e+04, ...,\n",
       "         2.14717266e+03, 7.12934645e-03, 1.30149885e+03],\n",
       "        [3.08570926e-01, 8.76141834e-03, 1.92185072e+04, ...,\n",
       "         2.49154904e+03, 7.03997941e-03, 1.24335484e+03]],\n",
       "\n",
       "       [[2.22200584e-01, 5.89873550e+02, 9.33490399e+03, ...,\n",
       "         2.40054786e-03, 3.32953244e-01, 3.81753912e+01],\n",
       "        [2.05885768e-01, 4.83661524e+02, 9.54410220e+03, ...,\n",
       "         1.83498749e-04, 2.16517972e-01, 2.37902851e+01],\n",
       "        [1.88870534e-01, 3.90025921e+02, 9.66584237e+03, ...,\n",
       "         6.31635589e-08, 1.53721079e-01, 1.70671961e+01],\n",
       "        ...,\n",
       "        [3.25598583e-03, 2.37287936e+01, 2.41383891e+03, ...,\n",
       "         2.20366208e+02, 4.59930916e+00, 9.47279372e+00],\n",
       "        [3.46598036e-03, 2.39089344e+01, 2.47762364e+03, ...,\n",
       "         2.22710743e+02, 4.21091843e+00, 5.80887487e+00],\n",
       "        [3.58603482e-03, 2.40097417e+01, 2.54461209e+03, ...,\n",
       "         2.25917907e+02, 3.83118011e+00, 8.91536252e+00]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 16), dtype=float64, numpy=\n",
       "array([[7.21221660e-01, 4.91903634e+01, 6.59595874e+03, ...,\n",
       "        1.54698191e+02, 2.71016193e+01, 9.90385308e+01],\n",
       "       [9.56046101e-02, 2.29821287e+01, 5.44950296e+03, ...,\n",
       "        3.91537215e+01, 1.71859048e+01, 2.77437351e+01],\n",
       "       [4.58212441e-01, 2.03672813e+01, 6.58288794e+03, ...,\n",
       "        1.28716550e+01, 3.63123347e+01, 4.11491769e+01],\n",
       "       ...,\n",
       "       [1.23878519e+01, 1.18536572e+01, 3.98311363e+03, ...,\n",
       "        2.22072316e+02, 1.79348935e+01, 2.40220222e+02],\n",
       "       [2.63659476e-01, 6.92254580e+00, 4.28730026e+03, ...,\n",
       "        1.22905628e+02, 5.38968289e+01, 6.18389292e+01],\n",
       "       [7.68882895e-01, 1.63069153e+02, 1.47250589e+03, ...,\n",
       "        1.19841638e+02, 6.85062716e+01, 5.27896058e+01]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 16), dtype=float64, numpy=\n",
       "array([[1.201262  , 5.46334932, 1.1774653 , ..., 1.95322272, 2.56115095,\n",
       "        2.71780486],\n",
       "       [1.85331584, 5.72000602, 1.06268927, ..., 1.88840455, 2.59585253,\n",
       "        1.96282621],\n",
       "       [2.42153196, 9.26472397, 2.13121657, ..., 2.55454018, 3.22804511,\n",
       "        1.95613293],\n",
       "       ...,\n",
       "       [5.61528187, 5.90874765, 2.81029872, ..., 4.21202296, 2.4787322 ,\n",
       "        4.60330872],\n",
       "       [2.12859411, 8.41481702, 2.60689201, ..., 6.60891902, 3.54179118,\n",
       "        2.29145415],\n",
       "       [2.10639996, 3.53126772, 2.63754659, ..., 3.2928732 , 6.40018638,\n",
       "        3.09699744]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trial_data)\n",
    "display( tf.math.reduce_mean(trial_data, axis=1, keepdims = True))\n",
    "display(trial_data  - tf.math.reduce_mean(trial_data, axis=1, keepdims = True))\n",
    "display((trial_data  - tf.math.reduce_mean(trial_data, axis=1, keepdims = True))**4)\n",
    "display(tf.math.reduce_mean((trial_data  - tf.math.reduce_mean(trial_data, axis=1, keepdims = True))**4, axis=1))\n",
    "display(tf.math.reduce_mean((trial_data  - tf.math.reduce_mean(trial_data, axis=1, keepdims = True))**4, axis=1) / tf.math.reduce_std(trial_data, axis=1)**4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b818a3-0b50-4ebe-8013-1b162b74aa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
