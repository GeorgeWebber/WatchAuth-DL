{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd758b6-9ea2-4b5e-be76-95a25bdd8150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T17:10:30.262734Z",
     "iopub.status.busy": "2023-02-07T17:10:30.261814Z",
     "iopub.status.idle": "2023-02-07T17:10:35.009939Z",
     "shell.execute_reply": "2023-02-07T17:10:35.008627Z",
     "shell.execute_reply.started": "2023-02-07T17:10:30.262529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 17:10:31.302361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 17:10:32.299043: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 17:10:32.299140: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 17:10:32.299152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, ConvLSTM1D, Flatten, LSTM, Permute, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Reshape, SpatialDropout1D, Dropout, TimeDistributed\n",
    "\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from featurize import featurize\n",
    "\n",
    "from scaler import CustomScaler\n",
    "from VAE import get_auth_model, VAE\n",
    "\n",
    "class SplitLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super(SplitLayer, self).__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(inputs,indices=self.layers,axis=-1)\n",
    "\n",
    "\n",
    "def get_discriminator(input_dim=(200,16)):\n",
    "    #input_dim = (200,16)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_dim)\n",
    "    x = inputs\n",
    "    \n",
    "    xs = []\n",
    "    \n",
    "    for i in range(16):\n",
    "        \n",
    "        x = SplitLayer(i)(inputs)\n",
    "        reshaped = Reshape((200, 1))(x)\n",
    "        \n",
    "        x = Conv1D(100, 10, strides=2, padding=\"same\")(reshaped)   #, kernel_regularizer=l2(1e-5)\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "        \n",
    "        x = Conv1D(100, 3, strides=1,  padding=\"same\")(x)     #   kernel_regularizer=l2(1e-5),\n",
    "        x = MaxPooling1D(pool_size=2, strides=None, padding=\"same\")(x)\n",
    "\n",
    "        \n",
    "        xs.append(x)\n",
    "    \n",
    "    x = layers.Concatenate()(xs)\n",
    "    x = LSTM(50)(x)\n",
    "\n",
    "    x = Dense(25, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(10, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    #x = layers.Lambda(lambda x: x / 2)\n",
    "    out = x\n",
    "\n",
    "    model = keras.Model(inputs, out, name=\"differentiating_fake_gestures\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0080ba46-4227-4466-b8db-fa951e50f390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T17:10:35.011892Z",
     "iopub.status.busy": "2023-02-07T17:10:35.011318Z",
     "iopub.status.idle": "2023-02-07T17:10:35.511807Z",
     "shell.execute_reply": "2023-02-07T17:10:35.510527Z",
     "shell.execute_reply.started": "2023-02-07T17:10:35.011868Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"raw_with_maps\" # or offsets_2\n",
    "\n",
    "x_data = np.load(f\"data/processed/x_{file_name}_filtered.npy\") # pre-filtered data is \"windowed_filtered\"\n",
    "feature_x_data = np.load(f\"data/processed/x_{file_name}_features.npy\")\n",
    "y_user = np.load(f\"data/processed/y_user_{file_name}.npy\")\n",
    "y_intent = np.load(f\"data/processed/y_intent_{file_name}.npy\")\n",
    "y_gesture = np.load(f\"data/processed/y_gesture_type_{file_name}.npy\")\n",
    "\n",
    "train_gesture_map = np.load(f\"data/processed/train_gesture_map_{file_name}.npy\")\n",
    "test_gesture_map = np.load(f\"data/processed/test_gesture_map_{file_name}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7abea3e-07a2-4870-bcaa-145783f9dccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T17:10:35.513540Z",
     "iopub.status.busy": "2023-02-07T17:10:35.513297Z",
     "iopub.status.idle": "2023-02-07T17:10:37.494341Z",
     "shell.execute_reply": "2023-02-07T17:10:37.493465Z",
     "shell.execute_reply.started": "2023-02-07T17:10:35.513518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-07 13:44:36           64\n",
      "config.json                                    2023-02-07 13:44:36        38011\n",
      "variables.h5                                   2023-02-07 13:44:36      3551848\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......concatenate\n",
      ".........vars\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......input_layer\n",
      ".........vars\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "......max_pooling1d\n",
      ".........vars\n",
      "......max_pooling1d_1\n",
      ".........vars\n",
      "......max_pooling1d_10\n",
      ".........vars\n",
      "......max_pooling1d_11\n",
      ".........vars\n",
      "......max_pooling1d_12\n",
      ".........vars\n",
      "......max_pooling1d_13\n",
      ".........vars\n",
      "......max_pooling1d_14\n",
      ".........vars\n",
      "......max_pooling1d_15\n",
      ".........vars\n",
      "......max_pooling1d_16\n",
      ".........vars\n",
      "......max_pooling1d_17\n",
      ".........vars\n",
      "......max_pooling1d_18\n",
      ".........vars\n",
      "......max_pooling1d_19\n",
      ".........vars\n",
      "......max_pooling1d_2\n",
      ".........vars\n",
      "......max_pooling1d_20\n",
      ".........vars\n",
      "......max_pooling1d_21\n",
      ".........vars\n",
      "......max_pooling1d_22\n",
      ".........vars\n",
      "......max_pooling1d_23\n",
      ".........vars\n",
      "......max_pooling1d_24\n",
      ".........vars\n",
      "......max_pooling1d_25\n",
      ".........vars\n",
      "......max_pooling1d_26\n",
      ".........vars\n",
      "......max_pooling1d_27\n",
      ".........vars\n",
      "......max_pooling1d_28\n",
      ".........vars\n",
      "......max_pooling1d_29\n",
      ".........vars\n",
      "......max_pooling1d_3\n",
      ".........vars\n",
      "......max_pooling1d_30\n",
      ".........vars\n",
      "......max_pooling1d_31\n",
      ".........vars\n",
      "......max_pooling1d_4\n",
      ".........vars\n",
      "......max_pooling1d_5\n",
      ".........vars\n",
      "......max_pooling1d_6\n",
      ".........vars\n",
      "......max_pooling1d_7\n",
      ".........vars\n",
      "......max_pooling1d_8\n",
      ".........vars\n",
      "......max_pooling1d_9\n",
      ".........vars\n",
      "......reshape\n",
      ".........vars\n",
      "......reshape_1\n",
      ".........vars\n",
      "......reshape_10\n",
      ".........vars\n",
      "......reshape_11\n",
      ".........vars\n",
      "......reshape_12\n",
      ".........vars\n",
      "......reshape_13\n",
      ".........vars\n",
      "......reshape_14\n",
      ".........vars\n",
      "......reshape_15\n",
      ".........vars\n",
      "......reshape_2\n",
      ".........vars\n",
      "......reshape_3\n",
      ".........vars\n",
      "......reshape_4\n",
      ".........vars\n",
      "......reshape_5\n",
      ".........vars\n",
      "......reshape_6\n",
      ".........vars\n",
      "......reshape_7\n",
      ".........vars\n",
      "......reshape_8\n",
      ".........vars\n",
      "......reshape_9\n",
      ".........vars\n",
      "......sampling\n",
      ".........vars\n",
      "......split_layer\n",
      ".........vars\n",
      "......split_layer_1\n",
      ".........vars\n",
      "......split_layer_10\n",
      ".........vars\n",
      "......split_layer_11\n",
      ".........vars\n",
      "......split_layer_12\n",
      ".........vars\n",
      "......split_layer_13\n",
      ".........vars\n",
      "......split_layer_14\n",
      ".........vars\n",
      "......split_layer_15\n",
      ".........vars\n",
      "......split_layer_2\n",
      ".........vars\n",
      "......split_layer_3\n",
      ".........vars\n",
      "......split_layer_4\n",
      ".........vars\n",
      "......split_layer_5\n",
      ".........vars\n",
      "......split_layer_6\n",
      ".........vars\n",
      "......split_layer_7\n",
      ".........vars\n",
      "......split_layer_8\n",
      ".........vars\n",
      "......split_layer_9\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-07 13:44:36           64\n",
      "config.json                                    2023-02-07 13:44:36         5964\n",
      "variables.h5                                   2023-02-07 13:44:36       319024\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......input_layer\n",
      ".........vars\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "......repeat_vector\n",
      ".........vars\n",
      "......time_distributed\n",
      ".........layer\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      ".........vars\n",
      "......time_distributed_1\n",
      ".........layer\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      ".........vars\n",
      "......up_sampling1d\n",
      ".........vars\n",
      "......up_sampling1d_1\n",
      ".........vars\n",
      "......up_sampling1d_2\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-02-07 13:44:36           64\n",
      "config.json                                    2023-02-07 13:44:36         1630\n",
      "variables.h5                                   2023-02-07 13:44:36        15648\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......input_layer\n",
      ".........vars\n",
      "......tf_op_lambda\n",
      ".........vars\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(None, None, None, None)\n",
    "vae.load_model(\"all_users\", \"filtered_specifically_trained_v3_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9393f3c3-c11c-4238-97db-b29759277d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T17:11:49.506159Z",
     "iopub.status.busy": "2023-02-07T17:11:49.504728Z",
     "iopub.status.idle": "2023-02-07T17:14:06.426160Z",
     "shell.execute_reply": "2023-02-07T17:14:06.424943Z",
     "shell.execute_reply.started": "2023-02-07T17:11:49.506159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"differentiating_fake_gestures\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 16)]    0           []                               \n",
      "                                                                                                  \n",
      " split_layer (SplitLayer)       (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_1 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_2 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_3 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_4 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_5 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_6 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_7 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_8 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_9 (SplitLayer)     (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_10 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_11 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_12 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_13 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_14 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " split_layer_15 (SplitLayer)    (None, 200)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 200, 1)       0           ['split_layer[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 200, 1)       0           ['split_layer_1[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 200, 1)       0           ['split_layer_2[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 200, 1)       0           ['split_layer_3[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 200, 1)       0           ['split_layer_4[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 200, 1)       0           ['split_layer_5[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 200, 1)       0           ['split_layer_6[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 200, 1)       0           ['split_layer_7[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 200, 1)       0           ['split_layer_8[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 200, 1)       0           ['split_layer_9[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 200, 1)       0           ['split_layer_10[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 200, 1)       0           ['split_layer_11[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 200, 1)       0           ['split_layer_12[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_13 (Reshape)           (None, 200, 1)       0           ['split_layer_13[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)           (None, 200, 1)       0           ['split_layer_14[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 200, 1)       0           ['split_layer_15[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 100, 100)     1100        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 100, 100)     1100        ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 100, 100)     1100        ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 100, 100)     1100        ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 100, 100)     1100        ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 100, 100)     1100        ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 100, 100)     1100        ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 100, 100)     1100        ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 100, 100)     1100        ['reshape_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 100, 100)     1100        ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 100, 100)     1100        ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 100, 100)     1100        ['reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 100, 100)     1100        ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 100, 100)     1100        ['reshape_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 100, 100)     1100        ['reshape_14[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 100, 100)     1100        ['reshape_15[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 50, 100)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 50, 100)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 50, 100)     0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 50, 100)     0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 50, 100)     0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_12 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_12[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_14 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_14[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_16 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_16[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_18 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_18[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_20 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_20[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_22 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_22[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_24 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_24[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_26 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_26[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_28 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_28[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_30 (MaxPooling1D  (None, 50, 100)     0           ['conv1d_30[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 50, 100)      30100       ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 50, 100)      30100       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 50, 100)      30100       ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 50, 100)      30100       ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 50, 100)      30100       ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_10[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_12[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_14[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_16[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_18[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_20[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_22[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_24[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_26[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_28[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 50, 100)      30100       ['max_pooling1d_30[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 25, 100)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 25, 100)     0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 25, 100)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 25, 100)     0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 25, 100)     0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_13 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_13[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_15 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_15[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_17 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_17[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_19 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_19[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_21 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_21[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_23 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_23[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_25 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_25[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_27 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_27[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_29 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_29[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_31 (MaxPooling1D  (None, 25, 100)     0           ['conv1d_31[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 25, 1600)     0           ['max_pooling1d_1[0][0]',        \n",
      "                                                                  'max_pooling1d_3[0][0]',        \n",
      "                                                                  'max_pooling1d_5[0][0]',        \n",
      "                                                                  'max_pooling1d_7[0][0]',        \n",
      "                                                                  'max_pooling1d_9[0][0]',        \n",
      "                                                                  'max_pooling1d_11[0][0]',       \n",
      "                                                                  'max_pooling1d_13[0][0]',       \n",
      "                                                                  'max_pooling1d_15[0][0]',       \n",
      "                                                                  'max_pooling1d_17[0][0]',       \n",
      "                                                                  'max_pooling1d_19[0][0]',       \n",
      "                                                                  'max_pooling1d_21[0][0]',       \n",
      "                                                                  'max_pooling1d_23[0][0]',       \n",
      "                                                                  'max_pooling1d_25[0][0]',       \n",
      "                                                                  'max_pooling1d_27[0][0]',       \n",
      "                                                                  'max_pooling1d_29[0][0]',       \n",
      "                                                                  'max_pooling1d_31[0][0]']       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 50)           330200      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 25)           1275        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 25)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           260         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            11          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 830,946\n",
      "Trainable params: 830,946\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "8/8 [==============================] - 12s 259ms/step - loss: 3.8768 - val_loss: 2.0336\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.5586 - val_loss: 2.6878\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.5614 - val_loss: 2.3562\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.4542 - val_loss: 1.8641\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 1.4493 - val_loss: 1.8392\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.2778 - val_loss: 1.7351\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.9824 - val_loss: 1.0899\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.6451 - val_loss: 0.6102\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.3681 - val_loss: 0.2135\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1748 - val_loss: 0.1366\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0661 - val_loss: 0.0650\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0656 - val_loss: 0.0415\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0723 - val_loss: 0.0225\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0212 - val_loss: 0.0174\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0321 - val_loss: 0.0156\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0200 - val_loss: 0.0156\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0145 - val_loss: 0.0154\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0243 - val_loss: 0.0149\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0121 - val_loss: 0.0097\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0143 - val_loss: 0.0103\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0060 - val_loss: 0.0101\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0140 - val_loss: 0.0098\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0073 - val_loss: 0.0100\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0051 - val_loss: 0.0101\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0105 - val_loss: 0.0071\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0171 - val_loss: 0.0071\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0033 - val_loss: 0.0080\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0040 - val_loss: 0.0092\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0245 - val_loss: 0.0097\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0042 - val_loss: 0.0099\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0013 - val_loss: 0.0094\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0051 - val_loss: 0.0093\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0021 - val_loss: 0.0088\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0030 - val_loss: 0.0086\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0032 - val_loss: 0.0088\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0046 - val_loss: 0.0089\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0044 - val_loss: 0.0094\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0012 - val_loss: 0.0094\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0036 - val_loss: 0.0095\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0032 - val_loss: 0.0097\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0134 - val_loss: 0.0099\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0022 - val_loss: 0.0105\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0014 - val_loss: 0.0107\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0028 - val_loss: 0.0110\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.0062 - val_loss: 0.0106\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0028 - val_loss: 0.0107\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0021 - val_loss: 0.0112\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0013 - val_loss: 0.0115\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0011 - val_loss: 0.0114\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.0025 - val_loss: 0.0114\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0012 - val_loss: 0.0115\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.0996e-04 - val_loss: 0.0115\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0016 - val_loss: 0.0110\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 4.9517e-04 - val_loss: 0.0098\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.0045 - val_loss: 0.0095\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0028 - val_loss: 0.0095\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.2183e-04 - val_loss: 0.0095\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0026 - val_loss: 0.0097\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 9.6066e-04 - val_loss: 0.0099\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0019 - val_loss: 0.0104\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.1810e-04 - val_loss: 0.0110\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0024 - val_loss: 0.0111\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0013 - val_loss: 0.0113\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0028 - val_loss: 0.0105\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0013 - val_loss: 0.0103\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.9200e-04 - val_loss: 0.0106\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0092 - val_loss: 0.0108\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.1208e-04 - val_loss: 0.0109\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.5983e-04 - val_loss: 0.0112\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0082 - val_loss: 0.0116\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0020 - val_loss: 0.0121\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.9810e-04 - val_loss: 0.0124\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0053 - val_loss: 0.0132\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0022 - val_loss: 0.0137\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 5.3609e-04 - val_loss: 0.0138\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.9239e-04 - val_loss: 0.0137\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.2856e-04 - val_loss: 0.0137\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.4172e-04 - val_loss: 0.0135\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0016 - val_loss: 0.0133\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.4276e-04 - val_loss: 0.0134\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0023 - val_loss: 0.0132\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0045 - val_loss: 0.0132\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.8778e-04 - val_loss: 0.0134\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.6358e-04 - val_loss: 0.0135\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 8.1422e-04 - val_loss: 0.0136\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0030 - val_loss: 0.0139\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 7.0983e-04 - val_loss: 0.0156\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.3009e-04 - val_loss: 0.0156\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0019 - val_loss: 0.0152\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.8895e-04 - val_loss: 0.0152\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.8552e-04 - val_loss: 0.0151\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 5.3598e-04 - val_loss: 0.0147\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 6.0087e-04 - val_loss: 0.0144\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.1768e-04 - val_loss: 0.0137\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 7.5350e-04 - val_loss: 0.0132\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.9759e-04 - val_loss: 0.0131\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 9.7985e-04 - val_loss: 0.0127\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.1029e-04 - val_loss: 0.0125\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0011 - val_loss: 0.0125\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0018 - val_loss: 0.0128\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.5036e-04 - val_loss: 0.0131\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 3.1404e-04 - val_loss: 0.0132\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.2812e-04 - val_loss: 0.0132\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0014 - val_loss: 0.0131\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.1791e-04 - val_loss: 0.0128\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4986e-04 - val_loss: 0.0126\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0019 - val_loss: 0.0128\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.9448e-04 - val_loss: 0.0130\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.5421e-04 - val_loss: 0.0133\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.1192e-04 - val_loss: 0.0136\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.2175e-04 - val_loss: 0.0136\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.6511e-04 - val_loss: 0.0133\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.3288e-04 - val_loss: 0.0129\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.2339e-04 - val_loss: 0.0126\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.3515e-04 - val_loss: 0.0133\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0011 - val_loss: 0.0117\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.6338e-04 - val_loss: 0.0112\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0020 - val_loss: 0.0111\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.8012e-04 - val_loss: 0.0111\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.7892e-05 - val_loss: 0.0111\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.4298e-04 - val_loss: 0.0111\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.2140e-04 - val_loss: 0.0111\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.4645e-05 - val_loss: 0.0111\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.2356e-04 - val_loss: 0.0110\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.7941e-04 - val_loss: 0.0110\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.5333e-04 - val_loss: 0.0111\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.1006e-04 - val_loss: 0.0110\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.1906e-04 - val_loss: 0.0109\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.4237e-04 - val_loss: 0.0108\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.3891e-04 - val_loss: 0.0108\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.9987e-04 - val_loss: 0.0111\n",
      "Epoch 141/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.1687e-04 - val_loss: 0.0108\n",
      "Epoch 142/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.8493e-04 - val_loss: 0.0104\n",
      "Epoch 143/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0018 - val_loss: 0.0102\n",
      "Epoch 144/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0013 - val_loss: 0.0104\n",
      "Epoch 145/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.5907e-04 - val_loss: 0.0104\n",
      "Epoch 146/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.5517e-05 - val_loss: 0.0103\n",
      "Epoch 147/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.8161e-04 - val_loss: 0.0104\n",
      "Epoch 148/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.4059e-04 - val_loss: 0.0104\n",
      "Epoch 149/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.5857e-04 - val_loss: 0.0103\n",
      "Epoch 150/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0012 - val_loss: 0.0102\n",
      "Epoch 151/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 4.9674e-04 - val_loss: 0.0100\n",
      "Epoch 152/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 5.5207e-04 - val_loss: 0.0100\n",
      "Epoch 153/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.5021e-04 - val_loss: 0.0100\n",
      "Epoch 154/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.0679e-04 - val_loss: 0.0101\n",
      "Epoch 155/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.6253e-04 - val_loss: 0.0100\n",
      "Epoch 156/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.7460e-04 - val_loss: 0.0095\n",
      "Epoch 157/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 7.8042e-04 - val_loss: 0.0094\n",
      "Epoch 158/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.0626e-04 - val_loss: 0.0094\n",
      "Epoch 159/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.7157e-04 - val_loss: 0.0094\n",
      "Epoch 160/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.8532e-04 - val_loss: 0.0094\n",
      "Epoch 161/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0047 - val_loss: 0.0098\n",
      "Epoch 162/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.3333e-04 - val_loss: 0.0100\n",
      "Epoch 163/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 9.6789e-04 - val_loss: 0.0099\n",
      "Epoch 164/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 4.2786e-05 - val_loss: 0.0098\n",
      "Epoch 165/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 1.1983e-04 - val_loss: 0.0097\n",
      "Epoch 166/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.4923e-04 - val_loss: 0.0097\n",
      "Epoch 167/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.4379e-04 - val_loss: 0.0096\n",
      "Epoch 168/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.4141e-04 - val_loss: 0.0115\n",
      "Epoch 169/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 1.8449e-04 - val_loss: 0.0144\n",
      "Epoch 170/1000\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 2.2503e-04 - val_loss: 0.0172\n",
      "Epoch 171/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0052 - val_loss: 0.0186\n",
      "Epoch 172/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.5823e-04 - val_loss: 0.0193\n",
      "Epoch 173/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.6742e-05 - val_loss: 0.0195\n",
      "Epoch 174/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.1525e-05 - val_loss: 0.0193\n",
      "Epoch 175/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.8064e-04 - val_loss: 0.0190\n",
      "Epoch 176/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.5736e-05 - val_loss: 0.0183\n",
      "Epoch 177/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.0021e-05 - val_loss: 0.0178\n",
      "Epoch 178/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.0105e-04 - val_loss: 0.0176\n",
      "Epoch 179/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.7916e-04 - val_loss: 0.0181\n",
      "Epoch 180/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.6188e-04 - val_loss: 0.0182\n",
      "Epoch 181/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 4.2427e-05 - val_loss: 0.0182\n",
      "Epoch 182/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.6961e-05 - val_loss: 0.0178\n",
      "Epoch 183/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.6808e-04 - val_loss: 0.0170\n",
      "Epoch 184/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.6911e-04 - val_loss: 0.0164\n",
      "Epoch 185/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.3283e-04 - val_loss: 0.0163\n",
      "Epoch 186/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.0460e-04 - val_loss: 0.0164\n",
      "Epoch 187/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.7858e-05 - val_loss: 0.0152\n",
      "Epoch 188/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 8.6126e-05 - val_loss: 0.0146\n",
      "Epoch 189/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0037 - val_loss: 0.0145\n",
      "Epoch 190/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.1005e-05 - val_loss: 0.0146\n",
      "Epoch 191/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.1381e-05 - val_loss: 0.0146\n",
      "Epoch 192/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4366e-04 - val_loss: 0.0147\n",
      "Epoch 193/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.4552e-04 - val_loss: 0.0147\n",
      "Epoch 194/1000\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 4.2811e-04 - val_loss: 0.0146\n",
      "Epoch 195/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.9151e-04 - val_loss: 0.0146\n",
      "Epoch 196/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.2911e-04 - val_loss: 0.0145\n",
      "Epoch 197/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.4621e-04 - val_loss: 0.0139\n",
      "Epoch 198/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.2516e-05 - val_loss: 0.0136\n",
      "Epoch 199/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.6859e-04 - val_loss: 0.0136\n",
      "Epoch 200/1000\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 3.4352e-05 - val_loss: 0.0136\n",
      "Epoch 201/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.7808e-04 - val_loss: 0.0133\n",
      "Epoch 202/1000\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.3422e-05 - val_loss: 0.0130\n",
      "Epoch 203/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 7.6768e-04 - val_loss: 0.0128\n",
      "Epoch 204/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.2329e-04 - val_loss: 0.0128\n",
      "Epoch 205/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 8.5103e-04 - val_loss: 0.0128\n",
      "Epoch 206/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1412e-04 - val_loss: 0.0129\n",
      "Epoch 207/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.3796e-05 - val_loss: 0.0129\n",
      "Epoch 208/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 9.1472e-05 - val_loss: 0.0129\n",
      "Epoch 209/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.2152e-05 - val_loss: 0.0128\n",
      "Epoch 210/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.9964e-04 - val_loss: 0.0123\n",
      "Epoch 211/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.1637e-04 - val_loss: 0.0121\n",
      "Epoch 212/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.0760e-05 - val_loss: 0.0119\n",
      "Epoch 213/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.0656e-04 - val_loss: 0.0119\n",
      "Epoch 214/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.9635e-05 - val_loss: 0.0119\n",
      "Epoch 215/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.6168e-04 - val_loss: 0.0119\n",
      "Epoch 216/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 5.1811e-04 - val_loss: 0.0124\n",
      "Epoch 217/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.4540e-04 - val_loss: 0.0125\n",
      "Epoch 218/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.3383e-05 - val_loss: 0.0126\n",
      "Epoch 219/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 9.3017e-05 - val_loss: 0.0125\n",
      "Epoch 220/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.3480e-05 - val_loss: 0.0124\n",
      "Epoch 221/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.3075e-04 - val_loss: 0.0125\n",
      "Epoch 222/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.3357e-04 - val_loss: 0.0106\n",
      "Epoch 223/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.6752e-04 - val_loss: 0.0100\n",
      "Epoch 224/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.2883e-04 - val_loss: 0.0100\n",
      "Epoch 225/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 5.5733e-04 - val_loss: 0.0100\n",
      "Epoch 226/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.3177e-05 - val_loss: 0.0101\n",
      "Epoch 227/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.3983e-05 - val_loss: 0.0101\n",
      "Epoch 228/1000\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.2883e-04 - val_loss: 0.0101\n",
      "Epoch 229/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.6392e-04 - val_loss: 0.0101\n",
      "Epoch 230/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 4.8101e-05 - val_loss: 0.0104\n",
      "Epoch 231/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.4396e-04 - val_loss: 0.0103\n",
      "Epoch 232/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.1538e-05 - val_loss: 0.0104\n",
      "Epoch 233/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.4858e-04 - val_loss: 0.0103\n",
      "Epoch 234/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.3208e-04 - val_loss: 0.0103\n",
      "Epoch 235/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.4323e-04 - val_loss: 0.0104\n",
      "Epoch 236/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0300e-04 - val_loss: 0.0105\n",
      "Epoch 237/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.0800e-05 - val_loss: 0.0105\n",
      "Epoch 238/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.1070e-04 - val_loss: 0.0105\n",
      "Epoch 239/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.0190e-05 - val_loss: 0.0105\n",
      "Epoch 240/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.9822e-04 - val_loss: 0.0108\n",
      "Epoch 241/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.9255e-04 - val_loss: 0.0113\n",
      "Epoch 242/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.0941e-05 - val_loss: 0.0115\n",
      "Epoch 243/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3965e-04 - val_loss: 0.0107\n",
      "Epoch 244/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.3525e-04 - val_loss: 0.0109\n",
      "Epoch 245/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.5806e-04 - val_loss: 0.0110\n",
      "Epoch 246/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.5706e-04 - val_loss: 0.0110\n",
      "Epoch 247/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.9753e-05 - val_loss: 0.0108\n",
      "Epoch 248/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.0329e-04 - val_loss: 0.0105\n",
      "Epoch 249/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 6.1213e-05 - val_loss: 0.0104\n",
      "Epoch 250/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.6108e-05 - val_loss: 0.0104\n",
      "Epoch 251/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.2278e-04 - val_loss: 0.0106\n",
      "Epoch 252/1000\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.7351e-04 - val_loss: 0.0108\n",
      "Epoch 253/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.7725e-05 - val_loss: 0.0109\n",
      "Epoch 254/1000\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.8119e-04 - val_loss: 0.0110\n",
      "Epoch 255/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.9573e-04 - val_loss: 0.0113\n",
      "Epoch 256/1000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.2180e-05 - val_loss: 0.0112\n",
      "Epoch 257/1000\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 2.4761e-05 - val_loss: 0.0112\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = x_data[train_gesture_map == 1]\n",
    "\n",
    "reconstructed_data_train = vae.scaler.inverse_transform(vae.decoder(vae.encoder(vae.scaler.transform(data_train))[0]).numpy())\n",
    "\n",
    "data_train = np.concatenate([data_train, reconstructed_data_train])\n",
    "\n",
    "labels_train = np.concatenate([np.ones(len(data_train)//2), np.zeros(len(data_train)//2)])\n",
    "\n",
    "\n",
    "shuffled_data_train, shuffled_labels_train = shuffle(data_train, labels_train, random_state=0)\n",
    "\n",
    "scaler = CustomScaler()\n",
    "shuffled_data_train = scaler.fit_and_transform(shuffled_data_train)\n",
    "\n",
    "\n",
    "model = get_discriminator((200,16))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "\n",
    "\n",
    "kFold = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "val_map = next(kFold.split(shuffled_data_train, shuffled_labels_train))[1]\n",
    "\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', start_from_epoch=50,\n",
    "                                                           patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(shuffled_data_train[~val_map], shuffled_labels_train[~val_map], epochs=1000,\n",
    "                    batch_size=128, verbose=1,\n",
    "                    validation_data=(shuffled_data_train[val_map], shuffled_labels_train[val_map]),\n",
    "                   class_weight={0:0.5, 1:30},\n",
    "                   callbacks=[early_stopping_callback])\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23777c2-e144-46bc-8a3b-bf137cdbb565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83900649-0b49-450e-89af-174db62fdddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418acd88-046f-436e-8992-56a3403f0537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T23:17:28.730346Z",
     "iopub.status.busy": "2023-02-06T23:17:28.729980Z",
     "iopub.status.idle": "2023-02-06T23:18:28.577083Z",
     "shell.execute_reply": "2023-02-06T23:18:28.576017Z",
     "shell.execute_reply.started": "2023-02-06T23:17:28.730275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_ranking\n",
      "  Downloading tensorflow_ranking-0.5.2-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.4/150.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.23.2\n",
      "  Downloading numpy-1.23.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow_ranking) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow_ranking) (1.1.0)\n",
      "Collecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
      "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.47.0)\n",
      "Collecting tensorflow<3,>=2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.19.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.26.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.7.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.1.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (63.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2019.11.28)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.0)\n",
      "Installing collected packages: flatbuffers, tensorflow-estimator, numpy, keras, tensorboard, tensorflow, tensorflow-serving-api, tensorflow_ranking\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.1\n",
      "    Uninstalling tensorflow-2.9.1:\n",
      "      Successfully uninstalled tensorflow-2.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.3.8+cuda11.cudnn82 requires flatbuffers<3.0,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-23.1.21 keras-2.11.0 numpy-1.23.2 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-serving-api-2.11.0 tensorflow_ranking-0.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478056-1c93-4c36-bdea-e1b0b300ce54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
